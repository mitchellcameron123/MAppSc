<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.52">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Propensity Scores with Machine Learning – My project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./metalearners.html" rel="next">
<link href="./background.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./propensity.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propensity Scores with Machine Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">My project</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction DONT READ THIS YET. ITS STILL BLAAA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background: Causal Inference and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./propensity.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propensity Scores with Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metalearners.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Meta-Learners</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Causal Trees and Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-conventional-approach-propensity-scores-and-balance" id="toc-a-conventional-approach-propensity-scores-and-balance" class="nav-link active" data-scroll-target="#a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">3.1</span> A Conventional Approach: Propensity Scores and Balance</a>
  <ul class="collapse">
  <li><a href="#propensity-score-modelling-with-logistic-regression" id="toc-propensity-score-modelling-with-logistic-regression" class="nav-link" data-scroll-target="#propensity-score-modelling-with-logistic-regression"><span class="header-section-number">3.1.1</span> Propensity Score Modelling with Logistic Regression</a></li>
  </ul></li>
  <li><a href="#probability-machines-probability-theory-and-machine-learning" id="toc-probability-machines-probability-theory-and-machine-learning" class="nav-link" data-scroll-target="#probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">3.2</span> Probability Machines: Probability Theory and Machine Learning</a>
  <ul class="collapse">
  <li><a href="#choice-of-loss-function-and-probability-prediction" id="toc-choice-of-loss-function-and-probability-prediction" class="nav-link" data-scroll-target="#choice-of-loss-function-and-probability-prediction"><span class="header-section-number">3.2.1</span> Choice of Loss Function and Probability Prediction</a></li>
  <li><a href="#sec-bagg-rf-probmachines" id="toc-sec-bagg-rf-probmachines" class="nav-link" data-scroll-target="#sec-bagg-rf-probmachines"><span class="header-section-number">3.2.2</span> Bagging and Random Forest as Probability Machines</a></li>
  <li><a href="#gradient-boosting-machines-as-probability-machines" id="toc-gradient-boosting-machines-as-probability-machines" class="nav-link" data-scroll-target="#gradient-boosting-machines-as-probability-machines"><span class="header-section-number">3.2.3</span> Gradient Boosting Machines as Probability Machines</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting"><span class="header-section-number">3.2.4</span> Overfitting</a></li>
  <li><a href="#sec-mlps-sims" id="toc-sec-mlps-sims" class="nav-link" data-scroll-target="#sec-mlps-sims"><span class="header-section-number">3.2.5</span> Comparison of Machine Learning Algorithms: Simulation Results</a></li>
  </ul></li>
  <li><a href="#implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" id="toc-implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" class="nav-link" data-scroll-target="#implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r"><span class="header-section-number">3.3</span> Implimentation and Hyperparameter Tuning with <code>WeightIt</code> and<code>gbm</code> in R</a>
  <ul class="collapse">
  <li><a href="#sec-gbm-tune-workflow" id="toc-sec-gbm-tune-workflow" class="nav-link" data-scroll-target="#sec-gbm-tune-workflow"><span class="header-section-number">3.3.1</span> Hyperparameter Tuning and Workflow</a></li>
  </ul></li>
  <li><a href="#example-nsw-jobs-dataset-using-r" id="toc-example-nsw-jobs-dataset-using-r" class="nav-link" data-scroll-target="#example-nsw-jobs-dataset-using-r"><span class="header-section-number">3.4</span> Example: NSW Jobs Dataset Using R</a>
  <ul class="collapse">
  <li><a href="#step-1-6-model-fitting-and-tuning" id="toc-step-1-6-model-fitting-and-tuning" class="nav-link" data-scroll-target="#step-1-6-model-fitting-and-tuning"><span class="header-section-number">3.4.1</span> Step 1-6: Model Fitting and Tuning</a></li>
  <li><a href="#sec-nsw-balance" id="toc-sec-nsw-balance" class="nav-link" data-scroll-target="#sec-nsw-balance"><span class="header-section-number">3.4.2</span> Step 7 and 8: Assessing Balance</a></li>
  <li><a href="#step-9-results" id="toc-step-9-results" class="nav-link" data-scroll-target="#step-9-results"><span class="header-section-number">3.4.3</span> Step 9: Results</a></li>
  </ul></li>
  <li><a href="#replication-study-dont-read-this.-needs-an-honest-days-work" id="toc-replication-study-dont-read-this.-needs-an-honest-days-work" class="nav-link" data-scroll-target="#replication-study-dont-read-this.-needs-an-honest-days-work"><span class="header-section-number">3.5</span> Replication Study (Don’t read this. needs an honest days work)</a>
  <ul class="collapse">
  <li><a href="#replication-of-original-results" id="toc-replication-of-original-results" class="nav-link" data-scroll-target="#replication-of-original-results"><span class="header-section-number">3.5.1</span> Replication of Original Results</a></li>
  <li><a href="#further-modelling" id="toc-further-modelling" class="nav-link" data-scroll-target="#further-modelling"><span class="header-section-number">3.5.2</span> Further Modelling</a></li>
  <li><a href="#comparison-of-methods" id="toc-comparison-of-methods" class="nav-link" data-scroll-target="#comparison-of-methods"><span class="header-section-number">3.5.3</span> Comparison of Methods</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-propensity" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propensity Scores with Machine Learning</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- To-do:   -->
<!-- - Finish coffee data example -->
<!-- - rework the loss function theory stuff -->
<!-- - change code to quarto formatting -->
<!-- - perhaps remove the nsw example here -->
<!-- - change the tutorial code to be the replication study -->
<!-- - exposed vs treated (using treatment here). -->
<!-- make quarto and ggplot themes consistent. also changing colors of note callouts.  -->
<!-- make sure the link here says chapter not section -->
<!-- - better intro that explains a probability machine.  -->
<!-- - round off the structure and ensure header labels are consistent.  -->
<!-- - ml background to be transferred/written up to the background chapter. cite in this. -->
<!-- - state generally about how classification is binary and give examples of how ml is used for this. then transition it all a lot better.  -->
<!-- - more organised comparison of simulation results -->
<!-- - clarify the gini splitting vs accuracy loss function for rf/bag -->
<!-- add rf and bagging to reduce words. replace all.  -->
<!-- ensure that covariate balance measures are noted and that there is a clear flow down to the simulation settting where balance is discussed.  -->
<!-- make some notes in the application that say why im not comparing balance across polynomicals. perhaps addd these to an appendix somewhere.   -->
<section id="a-conventional-approach-propensity-scores-and-balance" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">3.1</span> A Conventional Approach: Propensity Scores and Balance</h2>
<p>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. For similar groups, the average treatment effect (ATT) is a contrast of means from <a href="background.html#eq-ate-estimate" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, <a href="background.html#eq-ate-estimate" class="quarto-xref">Equation&nbsp;<span>2.3</span></a> leads to a biased estimate of the treatment effect.</p>
<p>For example, consider the causal question: <em>“How much does obtaining a bachelors degree increase lifetime earnings?”</em>. Individuals who complete a bachelor’s degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the <em>cause</em> of higher incomes when the effect is confounded by the differences in covariates between groups. Recall that <a href="background.html#fig-dag-confounder" class="quarto-xref">Figure&nbsp;<span>2.1 (a)</span></a> shows a confounding relationship. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of <em>covariate balance</em> which is a key concept behind underlying propensity score methods.</p>
<div id="nte-balance-intution" class="callout callout-style-default callout-note callout-titled" title="What is Covariate Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.1: What is Covariate Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate are the same for each group, then those covariates are <em>balanced</em>. If covariates are similar across groups, then there should not be any confounding. Equally, similar covariates across groups implies exchangability between groups as the two groups should be similar (thus can be exchanged). There is a conceptual equivalence between covariate balance, unconfoundedness, and exchangeability meaning that <a href="background.html#eq-independence" class="quarto-xref">Equation&nbsp;<span>2.6</span></a> is satisfied when covariates are balanced.</p>
</div>
</div>
<p>In bachelor’s degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parent income, geographic residence etc. Comparing the balanced matched pairs should result in a robust estimate of a bachelor’s degree’s impact on earnings because the individuals are exchangeable. As pairs are exchangeable, <a href="background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> is satisfied. The covariates are said to be “conditioned on” by matching individuals on these covariates. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a <em>dimensionality</em> problem as the dimension of the number of covariates increases.</p>
<p><span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as <span class="math inline">\(e(X)\)</span> and be expressed as:</p>
<p><span id="eq-pscore"><span class="math display">\[
e(X)=P(T=1|X).
\tag{3.1}\]</span></span></p>
<p>A prediction of the probability of treatment based on the covariates is the best summary of the individual covariates. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects.</p>
<p>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in <a href="background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>2.7</span></a>. There are many sources that offer a comprehensive guide to propensity score methods such as <span class="citation" data-cites="C5Mixtape2021">(<a href="#ref-C5Mixtape2021" role="doc-biblioref">Cunningham 2021, chap. 4</a>)</span> who provides applications and coded examples in R, Python, and Stata.</p>
<div id="nte-balance-pscore" class="callout callout-style-default callout-note callout-titled" title="Balance and Propensity Scores">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.2: Balance and Propensity Scores
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that an RCT will satisfy <a href="background.html#eq-independence" class="quarto-xref">Equation&nbsp;<span>2.6</span></a> as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy <a href="background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> as the potential outcomes are independent of the treatment status conditioned on some covariates. Conditioning on the propensity score aims to replicate an RCT in the observational data by balancing covariates between groups. When units are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates. The variables used to predict the propensity score are said to be conditioned on.</p>
</div>
</div>
<!-- there is repetition here of eqs in the boxes and writing.  -->
<p>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</p>
<p><span class="citation" data-cites="King2019">King and Nielsen (<a href="#ref-King2019" role="doc-biblioref">2019</a>)</span> provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</p>
<section id="propensity-score-modelling-with-logistic-regression" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="propensity-score-modelling-with-logistic-regression"><span class="header-section-number">3.1.1</span> Propensity Score Modelling with Logistic Regression</h3>
<p>A conventional propensity score model uses logistic regression to predict a probability between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Models may be specified to include interaction terms and polynomial terms so the model captures complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is driven by heuristics <span class="citation" data-cites="Brookhart2006 Heinrich2010">(<a href="#ref-Brookhart2006" role="doc-biblioref">Brookhart et al. 2006</a>; <a href="#ref-Heinrich2010" role="doc-biblioref">Heinrich 2010</a>)</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are statistically significant. Many researchers do not discuss the specification of their propensity model in papers. <span class="citation" data-cites="Austin2008">Austin (<a href="#ref-Austin2008" role="doc-biblioref">2008</a>)</span> review 47 papers that use propensity scores and few assess balance, perform adequate model selection and diagnosis, or apply correct statistical tests.</p>
<p>It’s important to note that the true value of a propensity score is never observable. A propensity score that is close to the theoretical probability is well calibrated. Using poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. The calibration of propensity scores depends on correctly specifying the model used to estimate them. Covariates may be omitted by error, poorly measured, or be unobservable. If the true relationship is non-linear or involves complex interactions between covariates, logistic regression may not predict calibrated scores. Another important note is that the propensity model itself does not have an informative causal interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for a variable which is not informative of the desired estimand.</p>
<p>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</p>
</section>
</section>
<section id="probability-machines-probability-theory-and-machine-learning" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">3.2</span> Probability Machines: Probability Theory and Machine Learning</h2>
<p>Predicting probabilities is not a typical machine learning task. Supervised machine learning usually focuses on classifying observations into groups, or regression to predict continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability that an observation will belong to a certain class. In this context, these applications are sometimes called probability machines.</p>
<p>Probability machines are valuable in applications requiring calibrated probability predictions. Probability machines can predict loan defaults or other adverse events in finance. They estimate the likelihood of customer response to a campaign in marketing. In criminal justice, they help forecast recidivism or future arrests, informing parole decisions. Weather forecasting uses probability machines to predict events like the chance of rain. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</p>
<p>Probability machines offer many advantages over parametric methods like logistic regression:</p>
<ol type="1">
<li><p><strong>Improved Calibration</strong>: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</p></li>
<li><p><strong>Flexible Modelling</strong>: Unlike parametric methods like logistic regression, probability machines don’t rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</p></li>
<li><p><strong>Efficient Feature Selection</strong>: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</p></li>
<li><p><strong>Handling Missing Data</strong>: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</p></li>
<li><p><strong>Simplified Data Exploration</strong>: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</p></li>
</ol>
<p>In causal inference, probability machines can predict propensity scores to maximize covariate balance and better estimate treatment effects. The first use of machine learning in economics and social sciences was for predicting propensity scores, driven by strong theoretical and practical motivations. This discussion aims to clarify the use of probability machines in causal inference given the sometimes unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</p>
<p>Please note that this chapter assumes a reader is familiar with <em>CART (Classification and Regression Tree)</em>, <em>Boosting</em>, <em>Bagging (Bootstrap Aggregation)</em>, <em>Random Forests</em>, <em>LASSO (Least Absolute Shrinkage and Selection Operator</em>, and <em>Logistic Regression</em>. These methods are briefly discussed in <a href="background.html#sec-background-ml" class="quarto-xref"><span>Section 2.2</span></a>.</p>
<section id="choice-of-loss-function-and-probability-prediction" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="choice-of-loss-function-and-probability-prediction"><span class="header-section-number">3.2.1</span> Choice of Loss Function and Probability Prediction</h3>
<p>The loss function measures the difference between a model’s predictions and the actual target values, serving as an measure of the model’s performance. The model with the lowest error is found when the loss function is minimised. In standard least squares regression, the loss function is the residual sum of squares that can be stated as: <span class="math inline">\(\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span>. This loss function says that the model must reduce the squared differences between the observed and predicted values. Different loss functions influence the model’s behaviour and so the choice of loss function is important.</p>
<p>Classification models determine the category to which each observation belongs. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Another example is in email filtering, where classifiers are used to predict whether or not an email is spam. Given these binary classification objectives, many loss functions minimize classification errors and improve accuracy.</p>
<p>A probability machine might employ a classification approach suitable for binary outcomes. While a loss function like the Gini (introduced in <a href="background.html#sec-background-cart" class="quarto-xref"><span>Section 2.2.1</span></a>) index is effective for classification problems, its effectiveness in calculating class probabilities is uncertain. In other words, minimizing misclassification error may not lead to accurate probability predictions.</p>
<p>To classify an observation as either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, a model needs to determine if <span class="math inline">\(P(A)\)</span> is less than or greater than <span class="math inline">\(0.5\)</span>. Thus, it is trivial if the probability of that classification is <span class="math inline">\(0.51\)</span> or <span class="math inline">\(0.99\)</span> as this makes no difference to the classification. For a probability machine, the difference between <span class="math inline">\(\hat{P}(A) = 0.51\)</span> and <span class="math inline">\(\hat{P}(A) = 0.99\)</span> is extreme. Understanding that classification models are optimized for classification accuracy rather than probability prediction is important. This distinction affects the performance of ensemble methods like random forests or bagging ensembles which use classification trees.</p>
</section>
<section id="sec-bagg-rf-probmachines" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="sec-bagg-rf-probmachines"><span class="header-section-number">3.2.2</span> Bagging and Random Forest as Probability Machines</h3>
<!-- todo:  -->
<!-- - reference to appendix -->
<!-- - additional reserach for nsw inclding the cite ther.  -->
<!-- - section reference -->
<p>In a bagging or random forest ensemble, class probabilities are determined through a <em>vote count</em> method. Each tree in the ensemble makes a class prediction based on the majority class in a terminal node. For instance, if <span class="math inline">\(x_i\)</span> lies in a terminal node where <span class="math inline">\(80\%\)</span> of the observations are classified as <span class="math inline">\(A\)</span>, that <em>individual tree</em> will classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span>. The ensemble’s overall prediction for <span class="math inline">\(x_i\)</span> is derived from the proportion of trees that classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. Let <span class="math inline">\(T\)</span> be the total number of trees and <span class="math inline">\(b_t\)</span> be the <span class="math inline">\(t\)</span>-th tree in the ensemble. Let <span class="math inline">\(\mathbb{I}(b_t(x_i) = A)\)</span> be the indicator function that returns <span class="math inline">\(1\)</span> when <span class="math inline">\(b_t\)</span> predicts that observation <span class="math inline">\(x_i\)</span> belongs to class <span class="math inline">\(A\)</span>. The probability of class <span class="math inline">\(A\)</span> for observation <span class="math inline">\(x_i\)</span> is calculated as:</p>
<p><span id="eq-ensemble-vote-method"><span class="math display">\[
\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A).
\tag{3.2}\]</span></span></p>
<p>In discussing the theoretical properties of random forests and bagging ensembles for probability predictions, <span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="#ref-Olson2018" role="doc-biblioref">2018</a>)</span> notes a potential bias towards predictions of <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> when trees in an ensemble are highly correlated and a voting mechanism is used. When trees in an ensemble are highly correlated, a vote count method can bias predicted probabilities towards <span class="math inline">\(\hat{P}(x_i=A) \in \{0,1\}\)</span> because each individual tree gives an identical prediction for each <span class="math inline">\(x_i\)</span>. Across the whole ensemble, probability predictions will bias towards <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Although having an ensemble of identical trees is unrealistic, the notion illustrates that tree correlation can introduce a <em>divergence bias</em>. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</p>
<p>However, divergence bias is problematic in probability applications. If <span class="math inline">\(x_i\)</span> has a known membership of <span class="math inline">\(A\)</span>, and an unknown <span class="math inline">\(P_{\text{true}}(x_i=A) = 0.6\)</span>, the ensemble might classify <span class="math inline">\(x_i\)</span> correctly <span class="math inline">\(90\%\)</span> of the time leading to <span class="math inline">\(\hat{P}(x_i=A) = 0.9\)</span>. As a probability machine, the ensemble has overestimated the probability by <span class="math inline">\(0.3\)</span> even though <span class="math inline">\(90\%\)</span> accuracy is excellent. To predict <span class="math inline">\(P_{\text{true}}(x_i=A) = 0.6\)</span>, an ensemble would need to incorrectly classify <span class="math inline">\(x_i\)</span> in <span class="math inline">\(40\%\)</span> of its trees. However, random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</p>
<p>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected subset of the data. Random forests further reduce tree correlation by considering only a random number of variables at each split, referred to as <span class="math inline">\(mtry\)</span>. When <span class="math inline">\(mtry\)</span> is near to to number of predictors, the model considers more variables at each split, making the random forest closer to a bagging ensemble. A lower <span class="math inline">\(mtry\)</span> should reduce the correlation between trees and decrease divergence bias, but a lower <span class="math inline">\(mtry\)</span> also introduces other theoretical problems.</p>
<p>Consider the scenario where the binary outcome of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If <span class="math inline">\(mtry\)</span> is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. For example, each predictor has a chance of <span class="math inline">\(\frac{mtry}{\text{number of predictors}}\)</span> of selection at each split implying a lower <span class="math inline">\(mtry\)</span> decreases the chance of a split considering the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of <span class="math inline">\(x_i\)</span> in each tree. Additionally, consider there is a class imbalance and the majority of obvervations are classified as <span class="math inline">\(A\)</span> not <span class="math inline">\(B\)</span>. If sucessive noisy splits result in impure terminal nodes, then terminal nodes may be dominated by the majority class <span class="math inline">\(A\)</span>. Consequently, there is a <em>majority class</em> effect as each tree in the ensemble is more likely to misclassify an observation as an <span class="math inline">\(A\)</span> because the terminal nodes have a higher proportion of <span class="math inline">\(A\)</span> due to the higher proportion of <span class="math inline">\(A\)</span>’s in the data overall.</p>
<p>To exemplify this theoretical discussion, consider the National Supported Work (NSW) programme, which is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with <span class="math inline">\(445\)</span> total participants, <span class="math inline">\(185\)</span> in the program group, and <span class="math inline">\(260\)</span> in the control group, so the true probability of treatment for each individual can be calculated as <span class="math inline">\(185/445=0.42\)</span> or <span class="math inline">\(42\)</span>%. Further information about this data is found in <a href="appendix.html#sec-data-nsw-jobs" class="quarto-xref"><span>Section 7.1.1</span></a>.</p>
<p>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although <a href="#fig-rf-varimp" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> and <a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;<span>3.1</span></a> do suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by <span class="citation" data-cites="Smith2005">Smith and Todd (<a href="#ref-Smith2005" role="doc-biblioref">2005</a>)</span> who suggests that self-selection bias is prevalent in the NSW data.</p>
<p><a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows both divergence bias and majority class effect using <code>randomForest</code> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when <span class="math inline">\(mtry\)</span> is equal to the number of predictors and so specifying <code>mtry = 7</code> in the <code>randomForest</code> function will fit a bagging ensemble. Logistic regression using the <code>gbm()</code> function provides a meaningful comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nsw_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">as.factor</span>(treat) <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                          black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> marr)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>logit_preds <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="fu">binomial</span>())<span class="sc">$</span>fitted.values </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>rf_mtry1_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">randomForest</span>(nsw_formula, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">data =</span> nsw_data), </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                          <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(nsw_formula, <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> nsw_data)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>bagged_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagging_model, <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plot_pmachines <span class="ot">&lt;-</span> <span class="cf">function</span>(preds, title) {</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(nsw_data, <span class="fu">aes</span>(<span class="at">x =</span> preds, <span class="at">fill =</span> <span class="fu">factor</span>(treat))) <span class="sc">+</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Participants"</span>)) <span class="sc">+</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">subtitle =</span> title, <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>         <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    custom_ggplot_theme</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(logit_preds, <span class="st">"Logistic Regression"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span> </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(rf_mtry1_preds, <span class="st">"Random Forest (mtry = 1)"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(bagged_preds, <span class="st">"Bagging (Bootstrap Aggregation)"</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">/</span> p2 <span class="sc">/</span> p3 <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-theory-demo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-rf-theory-demo-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: This figure compares the kernel density estimates of propensity score of each observation in the National Supported Work programme. The random forest and bagging ensemle are fitted with the randomForest default value of 500 trees.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create the Plot</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"none"</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-varimp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-rf-varimp-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: The figure compares the variable importance assigned to each variable from a baggin ensemble. The data originates from the National Supported Work programme. The difference in relative important of some variables indicates that randomisation may not have created exchangability between the groups.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The logistic regression model has identified a central tendency and most probabilities are between <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span> which roughly aligns with the true probability. For the random forest with <span class="math inline">\(mtry=1\)</span>, a significant number of the treatment and control observations are centred near <span class="math inline">\(0\)</span> with a wide range of other predictions. Such behaviour is consistent with a model overly predicting the majority class and having unstable predictions otherwise. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span>. Compared to the theoretically true probability, both random forest and bagging ensembles have performed poorly.</p>
<p>The tuning of <span class="math inline">\(mtry\)</span> faces double jeopardy and is another important area of discussion in probability machines. The selection of <span class="math inline">\(mtry\)</span> is typically completed in with a classification loss function such as accuracy or out-of-bag error. <span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="#ref-Olson2018" role="doc-biblioref">2018</a>)</span> compares tuning <span class="math inline">\(mtry\)</span> measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of <span class="math inline">\(mtry\)</span> for classification differs greatly from for probability prediction.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In other words, if a grid search finds that <span class="math inline">\(mtry=3\)</span> is optimal for a classification task, this does not imply that <span class="math inline">\(mtry=3\)</span> is optimal for predicting probabilities.</p>
<p>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in <a href="#sec-mlps-sims" class="quarto-xref"><span>Section 3.2.5</span></a>. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</p>
<p>Anecdotally, divergence bias and majority class effects will most effect a probability machine when there is considerable overlap between groups. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common overlap may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low <span class="math inline">\(mtry\)</span>, the terminal nodes of each tree may be relatively impure leading to a majority class effect. Alternatively, if true probabilities exist near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and there is a clear separation of class, divergence effects may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low <span class="math inline">\(mtry\)</span> may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</p>
<!-- maybe need a little chat about cross entropy here and why its not as good as in the gbm case.  -->
</section>
<section id="gradient-boosting-machines-as-probability-machines" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="gradient-boosting-machines-as-probability-machines"><span class="header-section-number">3.2.3</span> Gradient Boosting Machines as Probability Machines</h3>
<!-- needs a little more comparison to rf and bagging. perhaps do some more reserach about why these are good. note and differentiate the different types of boosting. perhaps also clarify how the gradient descent works for my own learning (dont write it to be too technical).  -->
<p>Moving beyond classification trees in random forests or bagging ensembles, <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> introduced the <em>Gradient Boosting Machine</em> (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss until it reaches its minimum.</p>
<p>GBM’s can be be <em>generalised</em> to many different applications by providing different loss functions that can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple <em>boosted</em> regression trees and a logistic function to transform regression predictions into probabilities. This logistic function is the same as in logistic regression, and so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood function. The model is expected to be well-calibrated, as maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The GBM outputs probability predictions, avoiding the issues associated with vote count methods used by random forests and bagging ensembles.</p>
<p>Additionally, each split considers all variables and makes the most informative splits that descend the loss function most effectively. GBMs utilize many weak learners, where each learner is only slightly better than random guessing. These weak learners are often regression stumps, which are CART models with only a single split. However, additional splits enable the model to capture interactions between terms which may increase performance in complex or high-dimensional datasets.</p>
<p>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques as well as allowing a probability distribution based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. The implementation and workflow to fit a GBM for propensity scores, is discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref"><span>Section 3.3.1</span></a>.</p>
</section>
<section id="overfitting" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">3.2.4</span> Overfitting</h3>
<p>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting is used to prevent overfitting to ensure a model can generalize to unseen data.</p>
<p>However, the model is not required to generalise when predicting propensity scores, as a different propensity score model is fit for a other datasets. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</p>
<div id="nte-overfit-logistic" class="callout callout-style-default callout-note callout-titled" title="Overfitting in Logistic Regression">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.3: Overfitting in Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is <span class="citation" data-cites="Schuster2016">Schuster, Lowe, and Platt (<a href="#ref-Schuster2016" role="doc-biblioref">2016</a>)</span>, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> receive artificially inflated weighting.</p>
</div>
</div>
<p><span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</p>
<p>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</p>
</section>
<section id="sec-mlps-sims" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="sec-mlps-sims"><span class="header-section-number">3.2.5</span> Comparison of Machine Learning Algorithms: Simulation Results</h3>
<!-- todo:  -->
<!-- fix early cite -->
<!-- clarify that sim studies are propensity score based. maybe look for general probability machine example sims.  -->
<p>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="citation" data-cites="McCaffrey2004 Setoguchi2008 Lee2010 Cannas2019 Tu2019 Goller2020 Ferri2020">(see <a href="#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="#ref-Setoguchi2008" role="doc-biblioref">Setoguchi et al. 2008</a>; <a href="#ref-Lee2010" role="doc-biblioref">Lee, Lessler, and Stuart 2010</a>; <a href="#ref-Cannas2019" role="doc-biblioref">Cannas and Arpino 2019</a>; <a href="#ref-Tu2019" role="doc-biblioref">Tu 2019</a>; <a href="#ref-Goller2020" role="doc-biblioref">Goller et al. 2020</a>; <a href="#ref-Ferri2020" role="doc-biblioref">Ferri-García and Del Mar Rueda 2020</a>)</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</p>
<p><span class="citation" data-cites="Tu2019">Tu (<a href="#ref-Tu2019" role="doc-biblioref">2019</a>)</span> compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</p>
<p>Despite poor theoretical properties as a probability machine, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> find that bagging results in the lowest standard error across many datasets.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</p>
<p>Additionally, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best <span class="math inline">\(95\%\)</span> coverage in all simulation scenarios, with <span class="math inline">\(98.6\%\)</span> coverage.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</p>
<p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="#ref-Goller2020" role="doc-biblioref">2020</a>)</span> adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</p>
<p>Based on a review of the literature, the findings can be distilled into five important points:</p>
<ol type="1">
<li><p>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</p></li>
<li><p>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="citation" data-cites="Rosenbaum1983">(<a href="#ref-Rosenbaum1983" role="doc-biblioref">Rosenbaum and Rubin 1983</a>)</span>.</p></li>
<li><p>Bagging ensembles perform poorly, a finding replicated across multiple studies.</p></li>
<li><p>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</p></li>
<li><p>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</p></li>
<li><p>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</p></li>
<li><p>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</p></li>
</ol>
</section>
</section>
<section id="implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r"><span class="header-section-number">3.3</span> Implimentation and Hyperparameter Tuning with <code>WeightIt</code> and<code>gbm</code> in R</h2>
<p>Based on <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span>, the <code>gbm</code> package implements a <em>Generalized Boosting Machine</em>. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. <code>gbm</code> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <code>bag.fraction</code> parameter.</p>
<p>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <code>gbm</code> package does not support this type of tuning, a wrapper package like <code>WeightIt</code> is necessary. <code>WeightIt</code> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <code>WeightIt</code> supports hyperparameter turning of <code>shrinkage</code>, <code>interaction.depth</code>, and <code>n.trees</code>. Once the best model is identified, propensity scores are predicted inside <code>WeightIt</code>. These can be used inside <code>WeightIt</code> to perform IPW or extracted for other implementations. <code>WeightIt</code> also supports an offset meaning that logistic regression predictions are supplied to the <code>GBM</code> package.</p>
<p>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="citation" data-cites="McCaffrey2004 Ridgeway2024">(see <a href="#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="#ref-Ridgeway2024" role="doc-biblioref">Ridgeway et al. 2024</a>)</span>. A very low learning rate, such as <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.0005\)</span>, allows a smooth descent of the loss function. The model should include a high number of trees, with <span class="math inline">\(10,000\)</span> or <span class="math inline">\(20,000\)</span> being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for <span class="math inline">\(3\)</span> splits. Based on anecdotal experience, <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span> splits per tree is optimal, consistent with recommendations by <span class="citation" data-cites="McCaffrey2004">McCaffrey, Ridgeway, and Morral (<a href="#ref-McCaffrey2004" role="doc-biblioref">2004</a>)</span>.</p>
<p>Another package, <code>twang</code>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <code>caret</code> must be used. Although <code>twang</code> has other useful functionalities which users may wish to implement.</p>
<section id="sec-gbm-tune-workflow" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-gbm-tune-workflow"><span class="header-section-number">3.3.1</span> Hyperparameter Tuning and Workflow</h3>
<!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  -->
<p>The <code>WeigthtIt</code> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <code>cobalt</code>. The best information for this package can be found on this <a href="https://ngreifer.github.io/WeightIt/index.html">website</a> or accessed with <code>vignette("WeightIt")</code> inside R after installation using <code>install.packages("WeightIt")</code>.</p>
<p>A workflow for hyperparameter tuning in <code>WeightIt</code> may be completed as follows:</p>
<ol type="1">
<li><p>Specify the <code>criterion</code> option, which specifies the measure of the <em>”best model”</em>. The available options are the options that the <code>cobalt</code> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <code>sdm.mean</code> or the smallest maximum SDM across covariates called <code>sdm.max</code>.</p></li>
<li><p>Set the number of trees high. The package default is <code>n.trees = 10000</code> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <code>n.tree</code> if optimal.</p></li>
<li><p>Specify the grid search for the depth of the tree called <code>interaction.depth</code> and the learning rate called <code>shrinkage</code>. These values can be specified using <code>c()</code> such as <code>shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)</code> or as integers such as <code>interaction.depth = 1:5</code>. These particular values are heuristically selected <em>suggestions</em> of good starting values. Additionally, an offset can be considered by performing a grid search across <code>offset=c(TRUE,FALSE)</code>.</p></li>
<li><p>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <code>my_weightit_object$info$best.tune</code>.</p></li>
<li><p>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <code>interaction.depth = 1:5</code> and the best fit is <span class="math inline">\(5\)</span>, then a new search can consider <code>interaction.depth = 3:7</code> so that the local area around <span class="math inline">\(5\)</span> can be searched.</p></li>
<li><p>Experiment with <code>bag.fraction</code>, which means each tree will consider a drawn proportion of observations equal to <code>bag.fraction</code>. Iteratively changing <code>bag.fraction</code> and assessing balance at each value should be practical. Consider <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.67\)</span>, and <span class="math inline">\(1\)</span>.</p></li>
<li><p>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <code>love.plot()</code> such as <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>.</p></li>
<li><p>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</p></li>
<li><p>Estimation and reporting of treatment effect.</p></li>
</ol>
</section>
</section>
<section id="example-nsw-jobs-dataset-using-r" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="example-nsw-jobs-dataset-using-r"><span class="header-section-number">3.4</span> Example: NSW Jobs Dataset Using R</h2>
<p>For demonstration, propensity scores are estimated following the workflow discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref"><span>Section 3.3.1</span></a> to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in <a href="appendix.html#sec-data-nsw-jobs" class="quarto-xref"><span>Section 7.1.1</span></a>. Randomisation should eliminate structural differences between groups, but <span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <code>WeightIt</code> and a logistic regression model using <code>glm()</code>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</p>
<div id="nte-ipw" class="callout callout-style-default callout-note callout-titled" title="Inverse Probability of Treatment Weighting">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.4: Inverse Probability of Treatment Weighting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a <em>pseudo-population</em> where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</p>
</div>
</div>
<section id="step-1-6-model-fitting-and-tuning" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="step-1-6-model-fitting-and-tuning"><span class="header-section-number">3.4.1</span> Step 1-6: Model Fitting and Tuning</h3>
<p>The <code>glm()</code> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <code>binomial()</code>. Recall the <code>nsw_formula</code> is specified in <a href="#sec-bagg-rf-probmachines" class="quarto-xref"><span>Section 3.2.2</span></a></p>
<div class="sourceCode" id="annotated-cell-3"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-3-2" class="code-annotation-target"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>())</span>
<span id="annotated-cell-3-3"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-3-4" class="code-annotation-target"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="2" data-code-annotation="1">Fits a logistic regression model using the <code>glm()</code> function specified to be a logistic model with <code>family=binomial()</code> using the previously created <code>nsw_formula</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="4" data-code-annotation="2">Extracts the fitted values (propensity scores) from the model.</span>
</dd>
</dl>
<p>Using the propensity score column of <code>nsw_data</code>, the <code>WeightIt</code> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <code>nsw_logit_weight</code>.</p>
<div class="sourceCode" id="annotated-cell-4"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-4-1"><a href="#annotated-cell-4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-4-2" class="code-annotation-target"><a href="#annotated-cell-4-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-4-3" class="code-annotation-target"><a href="#annotated-cell-4-3" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-4-4" class="code-annotation-target"><a href="#annotated-cell-4-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-4" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="3" data-code-annotation="2">Provides <code>weightit()</code> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <code>weightit()</code> function with <code>method = "glm"</code>. The separate estimation of the propensity scores is for illustrative purposes.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="4" data-code-annotation="3">Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
</dd>
</dl>
<p>A GBM model for propensity scores can be specified using <code>method = "gbm"</code> inside the <code>weightit()</code> function. To ensure consistent results, running <code>set.seed(88)</code> will ensure each tree uses the same <code>seed</code> if <code>bag.fraction</code> less than <span class="math inline">\(1\)</span>. The model is fit using the heuristically suggested starting values. Note that this model may take approximately <span class="math inline">\(30\)</span> second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</p>
<div class="sourceCode" id="annotated-cell-5"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-5-1"><a href="#annotated-cell-5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-5-2" class="code-annotation-target"><a href="#annotated-cell-5-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-5-3" class="code-annotation-target"><a href="#annotated-cell-5-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="annotated-cell-5-4"><a href="#annotated-cell-5-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-5-5" class="code-annotation-target"><a href="#annotated-cell-5-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>),</span>
<span id="annotated-cell-5-6"><a href="#annotated-cell-5-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-5-7" class="code-annotation-target"><a href="#annotated-cell-5-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="annotated-cell-5-8"><a href="#annotated-cell-5-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-5-9" class="code-annotation-target"><a href="#annotated-cell-5-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>,</span>
<span id="annotated-cell-5-10"><a href="#annotated-cell-5-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="6" onclick="event.preventDefault();">6</a><span id="annotated-cell-5-11" class="code-annotation-target"><a href="#annotated-cell-5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-5" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="3,4" data-code-annotation="2">Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="5,6" data-code-annotation="3">Performs a grid search over these values of the learning rate and depth of tree.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="7,8" data-code-annotation="4">Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="9,10" data-code-annotation="5">Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be <span class="math inline">\(10000\)</span> which is the package default.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="11" data-code-annotation="6">Prints the tune details of the model with the best covariate balance.</span>
</dd>
</dl>
<!-- clarify the meaning of learning rate/shrinkage -->
<!-- change all the instructions to active speech not passive.  -->
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
6       0.3                 1    bernoulli      FALSE    0.02253485      2392</code></pre>
</div>
</div>
<!-- cite what the balance statistics should be in the intro when discussing propensity score and balance.  -->
<p>The best balance across all tuning combinations yields an average SMD of <span class="math inline">\(0.023\)</span> showing strong balance. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in <a href="#sec-nsw-balance" class="quarto-xref"><span>Section 3.4.2</span></a> accompanying a discussion of balance.</p>
<p>The best machine has a learning rate of <span class="math inline">\(0.3\)</span> and contains <span class="math inline">\(2392\)</span> decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to <span class="math inline">\(0.3\)</span>. A reduction in the depth of tree and number of trees will reduce computation time.</p>
<p>The new tune grid will consider <code>shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)</code> as this allows the GBM to consider values between <span class="math inline">\(0.2\)</span> and <span class="math inline">\(0.3\)</span> and above <span class="math inline">\(0.3\)</span> which were missing in the previous grid.</p>
<div class="cell">
<details class="code-fold">
<summary>PALCEHOLDER</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight2 <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method=</span><span class="st">"gbm"</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage=</span> <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.3</span>, <span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.45</span>, <span class="fl">0.5</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
11      0.45                 2    bernoulli      FALSE    0.01965492        95</code></pre>
</div>
</div>
<p>Comparing the two iterations, there is a reduction from <span class="math inline">\(0.022\)</span> to <span class="math inline">\(0.02\)</span>. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of <span class="math inline">\(0.45\)</span>, a tree depth of <span class="math inline">\(2\)</span>, and <span class="math inline">\(95\)</span> trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</p>
<p>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, <a href="#fig-balance-iterations" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> shows the optimal number of trees is highly variable. If the learning rate is set to <code>shrinkage = 0.05</code>, then the best balance is not achieved until near to <span class="math inline">\(20,000\)</span> trees.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Make the <span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span></summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span> </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-balance-iterations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-balance-iterations-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Average Standardised Mean Differernce (Covaraite Balance) and the number of interations. Please note the difference in horozontal scale between the two plots.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- showtext doesnt seem to be working here. Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font family 'Source Sans Pro' not found, will use 'sans' instead. perhaps need to define the font in side the plot_annotation()-->
<p>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</p>
</section>
<section id="sec-nsw-balance" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="sec-nsw-balance"><span class="header-section-number">3.4.2</span> Step 7 and 8: Assessing Balance</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="The Importance of Discussing Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Importance of Discussing Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study’s conclusions. <span class="citation" data-cites="King2019">King and Nielsen (<a href="#ref-King2019" role="doc-biblioref">2019</a>)</span> notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</p>
<p>A good resource of information for assessing balance is documentation from the <code>cobalt</code> package, which can be viewed by running <code>vignette(“cobalt”, package = “cobalt”)</code> in R.</p>
</div>
</div>
<p><code>cobalt</code> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <code>WeightIt</code> for IPW and <code>MatchIt</code> for propensity score matching. Balance tables are created using <code>bal.tab()</code>.</p>
<!-- make sure this comment about integration is not repeditive  -->
<div class="sourceCode" id="annotated-cell-8"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-8-1" class="code-annotation-target"><a href="#annotated-cell-8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-8-2" class="code-annotation-target"><a href="#annotated-cell-8-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight,</span>
<span id="annotated-cell-8-3"><a href="#annotated-cell-8-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-8-4" class="code-annotation-target"><a href="#annotated-cell-8-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="annotated-cell-8-5"><a href="#annotated-cell-8-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-8-6" class="code-annotation-target"><a href="#annotated-cell-8-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="annotated-cell-8-7"><a href="#annotated-cell-8-7" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-8-8" class="code-annotation-target"><a href="#annotated-cell-8-8" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="1" data-code-annotation="1">Loads the <code>cobalt</code> package. This assumes the package is already installed with <code>install.packages("cobalt")</code></span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="2,3" data-code-annotation="2">Uses the <code>bal.tab()</code> fucntion to create balance statistics for the previously created <code>nsw_logit_weight</code> model.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="4,5" data-code-annotation="3">Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="6" data-code-annotation="4">Sets a threshold of balance to be <span class="math inline">\(0.1\)</span> to determine if a covariate is balanced.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="8" data-code-annotation="5">Extracts the balance table of the <code>nsw_logit_btab</code> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
</dd>
</dl>
<p>Additionally, <code>bal.tab()</code> will create balance tables for the GBM method’s IPWs and the raw data. For presentation, <code>dplyr</code> combines each of the individual balance tables for presentation using <code>kable</code> and <code>kableExtra</code>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to See Creation of Balance Tables</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_boosted_weight, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> nsw_data,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_formula, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> nsw_data, </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> nsw_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> nsw_raw_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<details class="code-fold">
<summary>Show the Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T,<span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width=</span><span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-combined-btab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Balance Table for NSW Data
</figcaption>
<div aria-describedby="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell">
<div id="tbl-combined-btab" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Placeholder
</figcaption>
<div aria-describedby="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variable</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Type</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">SMD</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Balanced</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.11</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.03</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.13</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.55</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.08</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.08</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.04</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.20</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.28</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.09</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Logistic Regression</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.98</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.27</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.80</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Boosting</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.91</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.02</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.14</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.02</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.01</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.05</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.05</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
<!-- double check that the variables are in the righ tpalces in the table  -->
<p><a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;<span>3.1</span></a> shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</p>
<!-- perhaps find the threshold for variance ratios -->
</section>
<section id="step-9-results" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="step-9-results"><span class="header-section-number">3.4.3</span> Step 9: Results</h3>
<p>Finally, the treatment effect can be estimated using <code>lm_weightit()</code> from the <code>WeightIt</code> package and <code>avg_comparisons()</code> from the <code>marginaleffects</code> package. <code>lm_weightit()</code> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <code>avg_comparisons()</code> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</p>
<p>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="citation" data-cites="Naimi2017">(<a href="#ref-Naimi2017" role="doc-biblioref">Naimi, Cole, and Kennedy 2017</a>)</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <code>re78</code> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</p>
<div class="sourceCode" id="annotated-cell-10"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-10-1" class="code-annotation-target"><a href="#annotated-cell-10-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span></span>
<span id="annotated-cell-10-2"><a href="#annotated-cell-10-2" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-10-3" class="code-annotation-target"><a href="#annotated-cell-10-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights)</span>
<span id="annotated-cell-10-4"><a href="#annotated-cell-10-4" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-10-5" class="code-annotation-target"><a href="#annotated-cell-10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="annotated-cell-10-6"><a href="#annotated-cell-10-6" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-10" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="1,2" data-code-annotation="1">Uses <code>lm_weightit()</code> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <code>*</code> indicates multiplication in R.</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="3" data-code-annotation="2">Specifies the <code>weights</code> from the <code>nsw_boosted_weight</code> object created earlier by the <code>weightit()</code> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <code>nsw_boosted_weight$weights</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="5,6" data-code-annotation="3">Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
</dd>
</dl>
<p>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create the Table</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78<span class="sc">~</span>treat<span class="sc">*</span>(age <span class="sc">+</span> educ <span class="sc">+</span> </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                             re75 <span class="sc">+</span> black <span class="sc">+</span> hisp <span class="sc">+</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                             degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">weights =</span> nsw_logit_weight<span class="sc">$</span>weights)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>nsw_logit_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_logit_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>nsw_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">extract_comparison_results</span>(nsw_logit_result),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(nsw_boosted_result))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(nsw_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Logistic Regression"</span>, <span class="st">"GBM"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(nsw_comparisons_tab, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="tbl-nsw-comparisons" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Comparison of ATE Estimates
</figcaption>
<div aria-describedby="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell">
<div class="cell-output-display">
<table class="table caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: center;">1610.79</td>
<td style="text-align: center;">668.49</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">300.58</td>
<td style="text-align: center;">2921.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">Generalized Boosting Machine</td>
<td style="text-align: center;">1609.95</td>
<td style="text-align: center;">669.42</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">297.91</td>
<td style="text-align: center;">2921.99</td>
</tr>
</tbody>
</table>


</div>
</div>
</div>
</figure>
</div>
<p><a href="#tbl-nsw-comparisons" class="quarto-xref">Table&nbsp;<span>3.2</span></a> shows that both estimates of the treatment effect are nearly identical at <span class="math inline">\(\$1610\)</span> with logistic regression inferring a <span class="math inline">\(\$0.86\)</span> larger treatment effect. Additionally, these results are statistically significant at the <span class="math inline">\(5\%\)</span> level with nearly identical standard errors.</p>
</section>
</section>
<section id="replication-study-dont-read-this.-needs-an-honest-days-work" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="replication-study-dont-read-this.-needs-an-honest-days-work"><span class="header-section-number">3.5</span> Replication Study (Don’t read this. needs an honest days work)</h2>
<p><span class="citation" data-cites="coffecite">(<a href="#ref-coffecite" role="doc-biblioref"><strong>coffecite?</strong></a>)</span> aims to estimate the impact of the certification of coffee cooperatives on small-scale Ethiopian farmers’ livelihoods. Certification is seen as a potential tool for socioeconomic change and environmental sustainability and so it is important to understand the impact on small-scale farmers. Propensity scores are used to balance covariates between certified and non-certified farmers, isolating the certification’s effect on income.The paper did not assess the balance of propensity scores and it is difficult to replicate the results in the paper using best practice. However, this provides a good opportunity to assess covariate balance in the initial paper and the repeat the analysis using a machine learning propensity model.</p>
<section id="replication-of-original-results" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="replication-of-original-results"><span class="header-section-number">3.5.1</span> Replication of Original Results</h3>
<p><span class="citation" data-cites="coffeecite">(<a href="#ref-coffeecite" role="doc-biblioref"><strong>coffeecite?</strong></a>)</span> provides a replication package including Stata code that uses the psmatch2 function. Nearest neighbour matching with replacement and common support trimming is performed. Common support trimming means that any observations outside the commonly overlapping are are discarded. The results of the paper can be fully replicated using the <code>MatchIt</code> package inside R.</p>
<div class="cell">
<details class="code-fold">
<summary>Show Code to Replicate Results of <span class="citation" data-cites="coffeecite">(<a href="#ref-coffeecite" role="doc-biblioref"><strong>coffeecite?</strong></a>)</span></summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>coffee_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(certified <span class="sc">~</span> age_hh <span class="sc">+</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                  agesq <span class="sc">+</span> nonfarmincome_access <span class="sc">+</span> depratio <span class="sc">+</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                  logtotal_land <span class="sc">+</span> badweat <span class="sc">+</span> edu <span class="sc">+</span> gender <span class="sc">+</span> </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                  years_cofeproduction <span class="sc">+</span> access_credit)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MatchIt)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>coffee_rep_pmodel <span class="ot">&lt;-</span> <span class="fu">matchit</span>(coffee_formula, <span class="at">data=</span>coffee_data, <span class="at">distance=</span><span class="st">"glm"</span>, </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>                              <span class="at">method=</span><span class="st">"nearest"</span>, <span class="at">replace =</span> T, <span class="at">estimand=</span><span class="st">"ATT"</span>, </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                              <span class="at">discard=</span><span class="st">"both"</span>) </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>coffee_logit_md <span class="ot">&lt;-</span> <span class="fu">match.data</span>(coffee_rep_pmodel)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>coffee_rep_fit<span class="ot">&lt;-</span> <span class="fu">lm</span>(percapitaincome_day_maleeq <span class="sc">~</span> certified, <span class="at">data =</span> coffee_logit_md, <span class="at">weights=</span>weights)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>replicated_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_rep_fit, <span class="at">variables =</span> <span class="st">"certified"</span>,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                <span class="at">vcov =</span> <span class="cn">TRUE</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">newdata =</span> <span class="fu">subset</span>(coffee_logit_md, certified <span class="sc">==</span> <span class="dv">1</span>),</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                <span class="at">wts =</span> <span class="st">"weights"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show to Code to Make <a href="#tbl-replicated-result">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>replicated_result_tbl <span class="ot">&lt;-</span> <span class="fu">extract_comparison_results</span>(replicated_result)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(replicated_result_tbl) <span class="ot">&lt;-</span> <span class="st">"Replicated Result"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(replicated_result_tbl, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-replicated-result" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-replicated-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.3: Placeholder
</figcaption>
<div aria-describedby="tbl-replicated-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Replicated Result</td>
<td style="text-align: center;">-0.15</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">-1.6</td>
<td style="text-align: center;">1.29</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<!-- get the signifiance to match maybe another go 
doing mulitiple outcomes?-->
<p><a href="#tbl-replicated-result" class="quarto-xref">Table&nbsp;<span>3.3</span></a> shows the replicated result obtained by <span class="citation" data-cites="coffeecite">(<a href="#ref-coffeecite" role="doc-biblioref"><strong>coffeecite?</strong></a>)</span>. The intriguing finding of the paper is that the average treatment effect on the treated (ATT) of being certified on income is negative. That is, if a farmer becomes certified, this is predicted to decrease by <span class="math inline">\(\$0.15\)</span> per day. Intuition and proponents of certifications schemes suggest that certification leads to an increase of income. If certification negatively impacted well-being in this way, it would call into question a significant effort to engage in certification and fair trade practices.</p>
<p><span class="citation" data-cites="coffeecite">(<a href="#ref-coffeecite" role="doc-biblioref"><strong>coffeecite?</strong></a>)</span> does not perform any discussion or consideration of balance in their paper and so it is not clear if their propensity score matching process has resulted in balanced covariates. A balance table created by the <code>cobalt</code> package will provide the required information for balance assessment which will be aided by a graphical visualisation using <code>love.plot()</code>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-coffee-rep-kbl">Table #</a></summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_rep_pmodel, </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab_ss <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Observations</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Household Age"</span>, <span class="st">"Squared Household Age"</span>, <span class="st">"Non-farm Income Access"</span>, </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Log Total Land"</span>, <span class="st">"Dependency Ratio"</span>, <span class="st">"Bad Weather"</span>,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Education Level"</span>, <span class="st">"Gender"</span>, <span class="st">"Years of Coffee Production"</span>, </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Access to Credit"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>colnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Variable"</span>,<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balance Threshold"</span>, <span class="st">"Variance Ratio"</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_rep_btab) <span class="ot">&lt;-</span> rowlabels</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>          coffee_rep_btab[,<span class="dv">3</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-rep-kbl" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-rep-kbl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.4: PALCHOLDER
</figcaption>
<div aria-describedby="tbl-coffee-rep-kbl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Variable</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Type</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SMD</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Balance Threshold</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Household Age</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.272</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.073</td>
</tr>
<tr class="even">
<td style="text-align: left;">Squared Household Age</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.255</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.143</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Non-farm Income Access</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="even">
<td style="text-align: left;">Log Total Land</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.297</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dependency Ratio</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.400</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.979</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bad Weather</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Education Level</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.034</td>
</tr>
<tr class="even">
<td style="text-align: left;">Gender</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">-0.132</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Years of Coffee Production</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.340</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.911</td>
</tr>
<tr class="even">
<td style="text-align: left;">Access to Credit</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#fig-coffee-replication-lplot">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add render info for showtext to yaml. also chang legend to be more informative.  </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>, <span class="at">thresholds=</span><span class="fl">0.1</span>, <span class="at">s.d.denom=</span><span class="st">"treated"</span>) <span class="sc">+</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance"</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-coffee-replication-lplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coffee-replication-lplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-coffee-replication-lplot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coffee-replication-lplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: PLACEHOLDER
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#tbl-coffee-rep-kbl" class="quarto-xref">Table&nbsp;<span>3.4</span></a> and <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> show that the propensity score matching process has obtained very poor balance. Based on the 10% rule, not a single variable is balanced and so the estimate of the treatment effect is likely to be biased by the structural differences in the groups.</p>
<p>For key variables such as Age, Gender, or Education, balance is especially important. On a theoretical level, we expect that people who are more educated are more likely to become certified as they are better able to engage with the application process and also are expected to earn more as increased education should lead to greater productivity. There likely exists gender discrimination given the time period and geographic area which suggests woman are less likely to be certified than men while also earning less due to a wide gender pay gap. These variables are strong confounders in theory and so emphasising balance in these variables is critical to making a robust causal inference.</p>
<p><a href="#fig-replication-pscore" class="quarto-xref">Figure&nbsp;<span>3.5</span></a> shows trimmed regions that mostly impact the control group. <a href="#tbl-coffee-replication-ss" class="quarto-xref">Table&nbsp;<span>3.5</span></a> shows 34 observations are dropped of which 33 are treated and 1 are control being dropped. This increases balance as the trimmed observations are extreme data points. When observations are discarded, the ATT, ATC, or ATE cannot be claimed. Instead, this is refereed to as the average treatment effect on the matched or ATM. There is a significant reduction in the effective sample size as due to dropped obervations in the control group which has an effective sample size of <span class="math inline">\(21\)</span> obervations down from <span class="math inline">\(82\)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#fig-replication-pscore">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>discarded_scores <span class="ot">&lt;-</span> coffee_rep_pmodel<span class="sc">$</span>distance[coffee_rep_pmodel<span class="sc">$</span>discarded]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>discard_min <span class="ot">&lt;-</span> <span class="fu">min</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>discard_max <span class="ot">&lt;-</span> <span class="fu">max</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(coffee_data, <span class="fu">aes</span>(<span class="at">x =</span> coffee_rep_pmodel<span class="sc">$</span>distance, <span class="at">fill =</span> <span class="fu">factor</span>(certified))) <span class="sc">+</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Certified"</span>)) <span class="sc">+</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Distribution of Propensity Scores in @coffeecite"</span>, </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_min, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_max, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> <span class="dv">0</span>, <span class="at">xmax =</span> discard_min, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> discard_max, <span class="at">xmax =</span> <span class="dv">1</span>, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.02</span>, <span class="at">y =</span> <span class="fl">2.5</span>, </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>           <span class="at">label =</span> <span class="st">"Discarded Range"</span>, <span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">1.5</span>, <span class="at">size=</span><span class="dv">4</span>,<span class="at">fontface=</span><span class="st">"bold"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>) <span class="sc">+</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-replication-pscore" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-replication-pscore-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-replication-pscore-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-replication-pscore-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: PLACEHOLDER
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-coffee-replication-ss">Table #</a></summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab_ss, <span class="at">digits=</span><span class="dv">0</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span>F)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-replication-ss" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-replication-ss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.5: Placeholder
</figcaption>
<div aria-describedby="tbl-coffee-replication-ss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Control</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Treated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">All (ESS)</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">164</td>
</tr>
<tr class="even">
<td style="text-align: left;">All (Unweighted)</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">164</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Matched (ESS)</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">131</td>
</tr>
<tr class="even">
<td style="text-align: left;">Matched (Unweighted)</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">131</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Unmatched</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Discarded</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">33</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p>Overall, this model fit using logistic regression and propensity score matching has resulted in a poor model due to covariate imbalance and unidentifiable estimands. It is likely that improvement can be made using</p>
</section>
<section id="further-modelling" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="further-modelling"><span class="header-section-number">3.5.2</span> Further Modelling</h3>
<p>In the following model fitting process, I aim to obtain better results while preserving the estimand. To improve the poor balance achieved by the paper there are two strategies to obtain better balance. First, the propensity scores can be re-estimated using machine learning leading to better calibrated propensity scores. Second, inverse propensity weighting (IPW) can be used instead of propensity score matching (PSM). IPW should ensure that that the sample size remains the same as no observations are lost in a matching process. Additionally, IPW is generally more efficient as the pseudo-population is based on prescise weights compared to matching that is based on approximate similarity between treatment and control.</p>
<p>The machine learning propensity scores will be estimated using <code>WeightIt</code> in the same process as <span class="quarto-unresolved-ref">?sec-demo</span>. To select the criteria that defines the best model, consider <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> that shows there is a significant range of balance levels in the raw dataset. Knowing this, the model will be tuned using <code>criterion = "smd.max"</code> as reducing the extremely unbalanced variables is important to achieving balance even if this leads to a higher average standardised mean difference (SMD). Additionally, the model fitting process was completed using <code>criterion = "smd.max"</code> and is shown in <span class="quarto-unresolved-ref">?sec-appendix</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show to Code to Fit the GBM model using <code>WeightIt</code> and <code>cobalt</code></summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>coffee_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data=</span>coffee_data, </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                                <span class="at">method=</span><span class="st">"gbm"</span>, <span class="at">distribution=</span><span class="st">"bernoulli"</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                                <span class="at">use.offset=</span><span class="fu">c</span>(T),</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>                                <span class="at">shrinkage=</span><span class="fu">seq</span>(<span class="fl">0.15</span>, <span class="fl">0.4</span>,<span class="at">length.out=</span><span class="dv">5</span>),</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>                                <span class="at">bag.fraction=</span><span class="fl">0.67</span>, </span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>                                <span class="at">interaction.depth=</span><span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>                                <span class="at">n.trees=</span><span class="dv">500</span>,</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>                                <span class="at">criterion=</span><span class="st">"smd.mean"</span>, </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand=</span><span class="st">"ATT"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_boosted_weight, </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance tabltune# Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> coffee_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Discussion of Tuning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion of Tuning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initially, a tuning grid considering shrinkage values of <span class="math inline">\(0.001,0.005,.01,0.05,0.1,\text{ and }0.2\)</span> were considered using <span class="math inline">\(10000\)</span> trees with a depth between <span class="math inline">\(1\)</span> and <span class="math inline">\(5\)</span>. The best tuning performance was found with shrinkage of <span class="math inline">\(0.2\)</span> and <span class="math inline">\(9\)</span> trees which were three splits <span class="math inline">\(3\)</span> deep. As such, the tuning grid was redefined in a second iteration to use <span class="math inline">\(0.1, 0.15, 0.2, 0.25, 0.3,0.35,\text{ and } 0.4\)</span> with only <span class="math inline">\(1000\)</span> trees with between <span class="math inline">\(2\)</span> and <span class="math inline">\(5\)</span> depth. The second fit, suggested a learning rate of <span class="math inline">\(0.35\)</span> so the local area of <span class="math inline">\(0.3, 0.325, 0.350, 0.375, \text{ and }0.4\)</span> is searched in the final fit.</p>
</div>
</div>
<p>Of course there is no guarantee that the GBM model will perform the best and so a logisic model is also fitted. An interesting comparison is between the balance visible in the matched sample like in <span class="citation" data-cites="coffeecite">(<a href="#ref-coffeecite" role="doc-biblioref"><strong>coffeecite?</strong></a>)</span> and in the weighted sample. Any difference between these two samples relates to the difference between matching and weighting on the propensity score as the score is the same in both methods (generated by logistic regression).</p>
<div class="cell">
<details class="code-fold">
<summary>Show to Code to Fit the Logistic model using <code>WeightIt</code> and <code>cobalt</code></summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>coffee_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data=</span>coffee_data, <span class="at">method=</span><span class="st">"glm"</span>,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand=</span><span class="st">"ATT"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>coffee_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_logit_weight, </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                             <span class="at">formula =</span> coffee_formula,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>                             <span class="at">data =</span> coffee_data, </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>                             <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>                             <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                             <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                             <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>coffee_logit_btab <span class="ot">&lt;-</span> coffee_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Additionally, the balance present in the raw data is calculated.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_formula, </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> coffee_raw_btab<span class="sc">$</span>Balance[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="comparison-of-methods" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="comparison-of-methods"><span class="header-section-number">3.5.3</span> Comparison of Methods</h3>
<p>In each of the above code chunks, the <code>cobalt</code> table has computed balance tables which are combined together in <a href="#tbl-coffee-comparison" class="quarto-xref">Table&nbsp;<span>3.6</span></a> and <a href="#fig-coffee-love-all" class="quarto-xref">Figure&nbsp;<span>3.6</span></a>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code Prepairing the Balance Table for Presentation</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"data.table"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> <span class="fu">rbindlist</span>(<span class="fu">list</span>(coffee_raw_btab,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                                       coffee_logit_btab,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>                                       coffee_boosted_btab), <span class="at">use.names=</span><span class="cn">FALSE</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> coffee_combined_btab[,<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>          coffee_combined_btab[,<span class="dv">4</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-coffee-comparison">Table #</a>.</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_combined_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold=</span><span class="cn">TRUE</span>, <span class="at">width=</span><span class="st">"5cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold=</span><span class="cn">FALSE</span>, <span class="at">width=</span><span class="st">"1cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Raw Data"</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Logistic Regression and IPTW"</span>, <span class="dv">11</span>, <span class="dv">20</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Boosted Machine with IPTW"</span>, <span class="dv">21</span>, <span class="dv">30</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-comparison" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.6: Comparison of Balance for Coffee Data Using Different Propensity Models
</figcaption>
<div aria-describedby="tbl-coffee-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: center;" data-quarto-table-cell-role="th">Variable</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Type</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SMD</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Balance Threshold</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.563</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.865</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.491</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.007</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.393</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.405</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.551</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.049</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.237</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.250</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.002</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.727</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.275</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.456</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.362</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.597</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Logistic Regression and IPTW</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.245</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.927</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.228</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.072</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.170</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.092</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.856</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.114</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.388</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.194</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.047</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.922</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.046</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.061</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.112</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.029</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Boosted Machine with IPTW</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.067</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.269</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.099</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.491</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.058</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.028</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.876</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.062</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.766</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.191</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.138</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.073</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.082</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.006</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.970</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.123</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#fig-coffee-love-all">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Logit =</span> coffee_logit_weight,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Boosting=</span> coffee_boosted_weight),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,<span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>, <span class="st">"darkblue"</span>,<span class="st">"darkred"</span>), </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>, <span class="st">"triangle"</span>, <span class="st">"diamond"</span>),</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>,<span class="at">thresholds=</span><span class="fl">0.1</span>,<span class="at">s.d.denom=</span><span class="st">"treated"</span>,<span class="at">use.grid=</span>F)<span class="sc">+</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance Using Different Balance Methods"</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="sc">+</span> custom_ggplot_theme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-coffee-love-all" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coffee-love-all-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-coffee-love-all-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coffee-love-all-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Comparison of Balance for Coffee Data Using Different Methods
</figcaption>
</figure>
</div>
</div>
</div>
<p>Viewing results of our balance shows three notable findings:</p>
<ol type="1">
<li><p>PSM has performed very poorly relative to IPW even though matching dropped a significant number of observations.</p></li>
<li><p>A GBM model has resulted in better covariate balance than logistic regression for most variables. Using a <span class="math inline">\(10\%\)</span> threshold for determining balance, logistic regression leaves <span class="math inline">\(5\)</span> variables unbalanced and the GBM leaves <span class="math inline">\(3\)</span> variables unbalanced. Additionally, the margin above being unbalanced is also larger for logistic regression. Although logistic regression results in better balance for “access to credit”, “gender”, and “education”. Logistic regress has the average balance across all variables is 0.0768603 which is satisfactory. For boosting, The average standardised mean is 0.0498114 which is quite impressive compared to the methodology used in the paper.</p></li>
<li><p>The variable with the worst balance for each model is Age with a SMD of <span class="math inline">\(0.245\)</span> for logistic regression and “bad weather” with <span class="math inline">\(0.191\)</span> for the GBM.</p></li>
<li><p>Using a looser balance threshold of <span class="math inline">\(0.2\)</span>, the GBM achieves balance across all covariates while the logistic regression does not balance either of the Age variables.</p></li>
</ol>
<p>For the logistic regression model, the balance statistics are marginally balanced. Using a <span class="math inline">\(10\%\)</span> threshold, half of the variables are balanced. Using a relaxed <span class="math inline">\(20\%\)</span> threshold, only Age and Age Squared are unbalanced but balance with threshold should be interpreted with caution.</p>
<p>Now that satisfactory covariate balance is achieved, the treatment effect can be estimated under logistic regression, the GBM, and then compared to the result in the paper. Note that the estimand in the paper is intended to be the average treatment effect (ATT) but dropped observations mean the actual treatment effect is the average treatment effect on matched (ATM) individuals.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-comparison-coffee-results">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>coffee_att_formula <span class="ot">&lt;-</span> <span class="fu">update.formula</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, <span class="fu">paste</span>(<span class="fu">attr</span>(<span class="fu">terms</span>(coffee_formula), <span class="st">"term.labels"</span>), <span class="at">collapse =</span> <span class="st">" + "</span>))), percapitaincome_day_maleeq <span class="sc">~</span> certified <span class="sc">*</span> .)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>coffee_logit_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_logit_weight)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>coffee_boosted_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_boosted_weight)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>coffee_logit_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_logit_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>coffee_boosted_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_boosted_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>coffee_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(replicated_result_tbl, <span class="fu">extract_comparison_results</span>(coffee_logit_att),</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(coffee_boosted_att))</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Rep. Result (Logistic with PSM)"</span>,<span class="st">"Logistic Regression and IPW"</span>, <span class="st">"Generalized Boosting Machine and IPW"</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_comparisons_tab, <span class="at">digits=</span><span class="dv">4</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-comparison-coffee-results" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comparison-coffee-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.7: PLACEHOLDER
</figcaption>
<div aria-describedby="tbl-comparison-coffee-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Rep. Result (Logistic with PSM)</td>
<td style="text-align: center;">-0.1538</td>
<td style="text-align: center;">0.7384</td>
<td style="text-align: center;">0.8350</td>
<td style="text-align: center;">-1.6009</td>
<td style="text-align: center;">1.2934</td>
</tr>
<tr class="even">
<td style="text-align: left;">Logistic Regression and IPW</td>
<td style="text-align: center;">-1.5824</td>
<td style="text-align: center;">0.6072</td>
<td style="text-align: center;">0.0092</td>
<td style="text-align: center;">-2.7724</td>
<td style="text-align: center;">-0.3924</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Generalized Boosting Machine and IPW</td>
<td style="text-align: center;">-1.0187</td>
<td style="text-align: center;">0.5196</td>
<td style="text-align: center;">0.0499</td>
<td style="text-align: center;">-2.0372</td>
<td style="text-align: center;">-0.0003</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Austin2008" class="csl-entry" role="listitem">
Austin, Peter C. 2008. <span>“<span class="nocase">A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003</span>.”</span> <em>Statistics in Medicine</em> 27 (April): 2037–49. <a href="https://doi.org/10.1002/sim.3150">https://doi.org/10.1002/sim.3150</a>.
</div>
<div id="ref-Brookhart2006" class="csl-entry" role="listitem">
Brookhart, M. Alan, Sebastian Schneeweiss, Kenneth J. Rothman, Robert J. Glynn, Jerry Avorn, and Til Stürmer. 2006. <span>“<span class="nocase">Variable selection for propensity score models</span>.”</span> <em>American Journal of Epidemiology</em> 163 (12): 1149–56. <a href="https://doi.org/10.1093/aje/kwj149">https://doi.org/10.1093/aje/kwj149</a>.
</div>
<div id="ref-Cannas2019" class="csl-entry" role="listitem">
Cannas, Massimo, and Bruno Arpino. 2019. <span>“<span class="nocase">A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting</span>.”</span> <em>Biometrical Journal</em> 61 (4): 1049–72. <a href="https://doi.org/10.1002/bimj.201800132">https://doi.org/10.1002/bimj.201800132</a>.
</div>
<div id="ref-C5Mixtape2021" class="csl-entry" role="listitem">
Cunningham, Scott. 2021. <span>“<span class="nocase">Matching and Subclassification</span>.”</span> In <em>Causal Inference: The Mixtape</em>, 175–240. Yale University Press. <a href="https://doi.org/10.2307/j.ctv1c29t27.8">https://doi.org/10.2307/j.ctv1c29t27.8</a>.
</div>
<div id="ref-Ferri2020" class="csl-entry" role="listitem">
Ferri-García, Ramón, and María Del Mar Rueda. 2020. <span>“<span class="nocase">Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys</span>.”</span> <em>PLoS ONE</em> 15 (4): 1–19. <a href="https://doi.org/10.1371/journal.pone.0231500">https://doi.org/10.1371/journal.pone.0231500</a>.
</div>
<div id="ref-Friedman2001" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“<span>Greedy Function Approximation: A Gradient Boosting Machine</span>.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232. <a href="https://www.jstor.org/stable/2699986">https://www.jstor.org/stable/2699986</a>.
</div>
<div id="ref-Goller2020" class="csl-entry" role="listitem">
Goller, Daniel, Michael Lechner, Andreas Moczall, and Joachim Wolff. 2020. <span>“<span class="nocase">Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed</span>.”</span> <em>Labour Economics</em> 65 (March). <a href="https://doi.org/10.1016/j.labeco.2020.101855">https://doi.org/10.1016/j.labeco.2020.101855</a>.
</div>
<div id="ref-Heinrich2010" class="csl-entry" role="listitem">
Heinrich, Carolyn. 2010. <span>“<span class="nocase">A Primer for Applying Propensity-Score Matching</span>.”</span> <em>Development</em>, no. August: 59. <a href="http://www.iadb.org/document.cfm?id=35320229">http://www.iadb.org/document.cfm?id=35320229</a>.
</div>
<div id="ref-King2019" class="csl-entry" role="listitem">
King, Gary, and Richard Nielsen. 2019. <span>“<span class="nocase">Why Propensity Scores Should Not Be Used for Matching</span>.”</span> <em>Political Analysis</em> 27 (4): 435–54. <a href="https://doi.org/10.1017/pan.2019.11">https://doi.org/10.1017/pan.2019.11</a>.
</div>
<div id="ref-Lee2010" class="csl-entry" role="listitem">
Lee, Brian K., Justin Lessler, and Elizabeth A. Stuart. 2010. <span>“<span class="nocase">Improving propensity score weighting using machine learning</span>.”</span> <em>Statistics in Medicine</em> 29: 337–46. <a href="https://doi.org/10.1002/sim.3782">https://doi.org/10.1002/sim.3782</a>.
</div>
<div id="ref-McCaffrey2004" class="csl-entry" role="listitem">
McCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. <span>“<span class="nocase">Propensity score estimation with boosted regression for evaluating causal effects in observational studies</span>.”</span> <em>Psychological Methods</em> 9 (4): 403–25. <a href="https://doi.org/10.1037/1082-989X.9.4.403">https://doi.org/10.1037/1082-989X.9.4.403</a>.
</div>
<div id="ref-Naimi2017" class="csl-entry" role="listitem">
Naimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. 2017. <span>“<span class="nocase">An introduction to g methods</span>.”</span> <em>International Journal of Epidemiology</em> 46 (2): 756–62. <a href="https://doi.org/10.1093/ije/dyw323">https://doi.org/10.1093/ije/dyw323</a>.
</div>
<div id="ref-Olson2018" class="csl-entry" role="listitem">
Olson, Matthew A., and Abraham J. Wyner. 2018. <span>“<span class="nocase">Making Sense of Random Forest Probabilities: a Kernel Perspective</span>,”</span> 1–35. <a href="http://arxiv.org/abs/1812.05792">http://arxiv.org/abs/1812.05792</a>.
</div>
<div id="ref-Ridgeway2024" class="csl-entry" role="listitem">
Ridgeway, Greg, Dan Mccaffrey, Andrew Morral, Matthew Cefalu, Lane Burgette, and Beth Ann Griffin. 2024. <span>“<span class="nocase">Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package</span>.”</span> <a href="https://doi.org/10.7249/tl136.1">https://doi.org/10.7249/tl136.1</a>.
</div>
<div id="ref-Rosenbaum1983" class="csl-entry" role="listitem">
Rosenbaum, Paul R., and Donald B. Rubin. 1983. <span>“<span class="nocase">The central role of the propensity score in observational studies for causal effects</span>.”</span> <em>Biometrika</em> 70 (1): 41–55. <a href="https://doi.org/10.1017/CBO9780511810725.016">https://doi.org/10.1017/CBO9780511810725.016</a>.
</div>
<div id="ref-Schuster2016" class="csl-entry" role="listitem">
Schuster, Tibor, Wilfrid Kouokam Lowe, and Robert W. Platt. 2016. <span>“<span class="nocase">Propensity score model overfitting led to inflated variance of estimated odds ratios</span>.”</span> <em>Journal of Clinical Epidemiology</em> 80: 97–106. <a href="https://doi.org/10.1016/j.jclinepi.2016.05.017">https://doi.org/10.1016/j.jclinepi.2016.05.017</a>.
</div>
<div id="ref-Setoguchi2008" class="csl-entry" role="listitem">
Setoguchi, Soko, Sebastian Schneeweiss, Alan M. Brookhart, Robert J. Glynn, and Francis E. Cook. 2008. <span>“<span class="nocase">Evaluating uses of data mining techniques in propensity score estimation: a simulation study</span>.”</span> <em>Pharmacoepidemiology and Drug Safety</em> 17 (March): 546–55. <a href="https://doi.org/10.1002/pds">https://doi.org/10.1002/pds</a>.
</div>
<div id="ref-Smith2005" class="csl-entry" role="listitem">
Smith, Jeffrey A., and Petra E. Todd. 2005. <em><span class="nocase">Does matching overcome LaLonde’s critique of nonexperimental estimators?</span></em> Vol. 125. 1-2 SPEC. ISS. <a href="https://doi.org/10.1016/j.jeconom.2004.04.011">https://doi.org/10.1016/j.jeconom.2004.04.011</a>.
</div>
<div id="ref-Tu2019" class="csl-entry" role="listitem">
Tu, Chunhao. 2019. <span>“<span class="nocase">Comparison of various machine learning algorithms for estimating generalized propensity score</span>.”</span> <em>Journal of Statistical Computation and Simulation</em> 89 (4): 708–19. <a href="https://doi.org/10.1080/00949655.2019.1571059">https://doi.org/10.1080/00949655.2019.1571059</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note that tuning <span class="math inline">\(mtry\)</span> for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In this context, the coverage is the proportion of times that the true treatment effect is within the <span class="math inline">\(95\%\)</span> confidence interval across the number of simulations. This author implements <span class="math inline">\(1000\)</span> simulations of each scenario.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <code>twang</code> and <code>gbm</code> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benifits discussed in <span class="citation" data-cites="cross-reference">(<a href="#ref-cross-reference" role="doc-biblioref"><strong>cross-reference?</strong></a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="#ref-Goller2020" role="doc-biblioref">2020</a>)</span> calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the <em>true</em> treatment effect. Thus, the covariate balance table offers a clearer view of each method’s performance.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./background.html" class="pagination-link" aria-label="Background: Causal Inference and Machine Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background: Causal Inference and Machine Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./metalearners.html" class="pagination-link" aria-label="Meta-Learners">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Meta-Learners</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb23" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="in">load(file = "my_environment.RData")</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- To-do:   --&gt;</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - Finish coffee data example --&gt;</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - rework the loss function theory stuff --&gt;</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - change code to quarto formatting --&gt;</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - perhaps remove the nsw example here --&gt;</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - change the tutorial code to be the replication study --&gt;</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - exposed vs treated (using treatment here). --&gt;</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make quarto and ggplot themes consistent. also changing colors of note callouts.  --&gt;</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make sure the link here says chapter not section --&gt;</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - better intro that explains a probability machine.  --&gt;</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - round off the structure and ensure header labels are consistent.  --&gt;</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - ml background to be transferred/written up to the background chapter. cite in this. --&gt;</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - state generally about how classification is binary and give examples of how ml is used for this. then transition it all a lot better.  --&gt;</span></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - more organised comparison of simulation results --&gt;</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - clarify the gini splitting vs accuracy loss function for rf/bag --&gt;</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- add rf and bagging to reduce words. replace all.  --&gt;</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ensure that covariate balance measures are noted and that there is a clear flow down to the simulation settting where balance is discussed.  --&gt;</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make some notes in the application that say why im not comparing balance across polynomicals. perhaps addd these to an appendix somewhere.   --&gt;</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a><span class="fu"># Propensity Scores with Machine Learning {#sec-propensity}</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Conventional Approach: Propensity Scores and Balance</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. For similar groups, the average treatment effect (ATT) is a contrast of means from @eq-ate-estimate. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, @eq-ate-estimate leads to a biased estimate of the treatment effect.</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>For example, consider the causal question: *"How much does obtaining a bachelors degree increase lifetime earnings?"*. Individuals who complete a bachelor’s degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the *cause* of higher incomes when the effect is confounded by the differences in covariates between groups. Recall that @fig-dag-confounder shows a confounding relationship. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of *covariate balance* which is a key concept behind underlying propensity score methods.</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>::: {#nte-balance-intution .callout-note title="What is Covariate Balance"}</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate are the same for each group, then those covariates are *balanced*. If covariates are similar across groups, then there should not be any confounding. Equally, similar covariates across groups implies exchangability between groups as the two groups should be similar (thus can be exchanged). There is a conceptual equivalence between covariate balance, unconfoundedness, and exchangeability meaning that @eq-independence is satisfied when covariates are balanced. </span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>In bachelor’s degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parent income, geographic residence etc. Comparing the balanced matched pairs should result in a robust estimate of a bachelor’s degree's impact on earnings because the individuals are exchangeable. As pairs are exchangeable, @eq-conditional-independence is satisfied. The covariates are said to be "conditioned on" by matching individuals on these covariates. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a *dimensionality* problem as the dimension of the number of covariates increases. </span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>@Rosenbaum1983 offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as $e(X)$ and be expressed as:</span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>e(X)=P(T=1|X).</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pscore}</span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>A prediction of the probability of treatment based on the covariates is the best summary of the individual covariates. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects. </span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in @eq-conditional-independence. There are many sources that offer a comprehensive guide to propensity score methods such as <span class="co">[</span><span class="ot">@C5Mixtape2021, Chapter 4</span><span class="co">]</span> who provides applications and coded examples in R, Python, and Stata.</span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>::: {#nte-balance-pscore .callout-note title="Balance and Propensity Scores"}</span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>Note that an RCT will satisfy @eq-independence as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy @eq-conditional-independence as the potential outcomes are independent of the treatment status conditioned on some covariates. Conditioning on the propensity score aims to replicate an RCT in the observational data by balancing covariates between groups. When units are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates. The variables used to predict the propensity score are said to be conditioned on.</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- there is repetition here of eqs in the boxes and writing.  --&gt;</span></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>@King2019 provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### Propensity Score Modelling with Logistic Regression</span></span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>A conventional propensity score model uses logistic regression to predict a probability between $0$ and $1$. Models may be specified to include interaction terms and polynomial terms so the model captures complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is driven by heuristics <span class="co">[</span><span class="ot">@Brookhart2006; @Heinrich2010</span><span class="co">]</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are statistically significant. Many researchers do not discuss the specification of their propensity model in papers. @Austin2008 review 47 papers that use propensity scores and few assess balance, perform adequate model selection and diagnosis, or apply correct statistical tests.</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>It’s important to note that the true value of a propensity score is never observable. A propensity score that is close to the theoretical probability is well calibrated. Using poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. The calibration of propensity scores depends on correctly specifying the model used to estimate them. Covariates may be omitted by error, poorly measured, or be unobservable. If the true relationship is non-linear or involves complex interactions between covariates, logistic regression may not predict calibrated scores. Another important note is that the propensity model itself does not have an informative causal interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for a variable which is not informative of the desired estimand.</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probability Machines: Probability Theory and Machine Learning</span></span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a>Predicting probabilities is not a typical machine learning task. Supervised machine learning usually focuses on classifying observations into groups, or regression to predict continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability that an observation will belong to a certain class. In this context, these applications are sometimes called probability machines.</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>Probability machines are valuable in applications requiring calibrated probability predictions. Probability machines can predict loan defaults or other adverse events in finance. They estimate the likelihood of customer response to a campaign in marketing. In criminal justice, they help forecast recidivism or future arrests, informing parole decisions. Weather forecasting uses probability machines to predict events like the chance of rain. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>Probability machines offer many advantages over parametric methods like logistic regression:</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**Improved Calibration**: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>**Flexible Modelling**: Unlike parametric methods like logistic regression, probability machines don’t rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>**Efficient Feature Selection**: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>**Handling Missing Data**: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>**Simplified Data Exploration**: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a>In causal inference, probability machines can predict propensity scores to maximize covariate balance and better estimate treatment effects. The first use of machine learning in economics and social sciences was for predicting propensity scores, driven by strong theoretical and practical motivations. This discussion aims to clarify the use of probability machines in causal inference given the sometimes unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a>Please note that this chapter assumes a reader is familiar with *CART (Classification and Regression Tree)*, *Boosting*, *Bagging (Bootstrap Aggregation)*, *Random Forests*, *LASSO (Least Absolute Shrinkage and Selection Operator*, and *Logistic Regression*. These methods are briefly discussed in @sec-background-ml.</span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choice of Loss Function and Probability Prediction</span></span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a>The loss function measures the difference between a model’s predictions and the actual target values, serving as an measure of the model’s performance. The model with the lowest error is found when the loss function is minimised. In standard least squares regression, the loss function is the residual sum of squares that can be stated as: $\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$. This loss function says that the model must reduce the squared differences between the observed and predicted values. Different loss functions influence the model’s behaviour and so the choice of loss function is important.</span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a>Classification models determine the category to which each observation belongs. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Another example is in email filtering, where classifiers are used to predict whether or not an email is spam. Given these binary classification objectives, many loss functions minimize classification errors and improve accuracy.</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a>A probability machine might employ a classification approach suitable for binary outcomes. While a loss function like the Gini (introduced in @sec-background-cart) index is effective for classification problems, its effectiveness in calculating class probabilities is uncertain. In other words, minimizing misclassification error may not lead to accurate probability predictions.</span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a>To classify an observation as either $A$ or $B$, a model needs to determine if $P(A)$ is less than or greater than $0.5$. Thus, it is trivial if the probability of that classification is $0.51$ or $0.99$ as this makes no difference to the classification. For a probability machine, the difference between $\hat{P}(A) = 0.51$ and $\hat{P}(A) = 0.99$ is extreme. Understanding that classification models are optimized for classification accuracy rather than probability prediction is important. This distinction affects the performance of ensemble methods like random forests or bagging ensembles which use classification trees.</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bagging and Random Forest as Probability Machines {#sec-bagg-rf-probmachines}</span></span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- todo:  --&gt;</span></span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - reference to appendix --&gt;</span></span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-122"><a href="#cb23-122" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - additional reserach for nsw inclding the cite ther.  --&gt;</span></span>
<span id="cb23-123"><a href="#cb23-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-124"><a href="#cb23-124" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - section reference --&gt;</span></span>
<span id="cb23-125"><a href="#cb23-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-126"><a href="#cb23-126" aria-hidden="true" tabindex="-1"></a>In a bagging or random forest ensemble, class probabilities are determined through a *vote count* method. Each tree in the ensemble makes a class prediction based on the majority class in a terminal node. For instance, if $x_i$ lies in a terminal node where $80\%$ of the observations are classified as $A$, that *individual tree* will classify $x_i$ as $A$. The ensemble’s overall prediction for $x_i$ is derived from the proportion of trees that classify $x_i$ as $A$ or $B$. Let $T$ be the total number of trees and $b_t$ be the $t$-th tree in the ensemble. Let $\mathbb{I}(b_t(x_i) = A)$ be the indicator function that returns $1$ when $b_t$ predicts that observation $x_i$ belongs to class $A$. The probability of class $A$ for observation $x_i$ is calculated as:</span>
<span id="cb23-127"><a href="#cb23-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-128"><a href="#cb23-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb23-129"><a href="#cb23-129" aria-hidden="true" tabindex="-1"></a>\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A). </span>
<span id="cb23-130"><a href="#cb23-130" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ensemble-vote-method}</span>
<span id="cb23-131"><a href="#cb23-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-132"><a href="#cb23-132" aria-hidden="true" tabindex="-1"></a>In discussing the theoretical properties of random forests and bagging ensembles for probability predictions, @Olson2018 notes a potential bias towards predictions of $0$ or $1$ when trees in an ensemble are highly correlated and a voting mechanism is used. When trees in an ensemble are highly correlated, a vote count method can bias predicted probabilities towards $\hat{P}(x_i=A) \in <span class="sc">\{</span>0,1<span class="sc">\}</span>$ because each individual tree gives an identical prediction for each $x_i$. Across the whole ensemble, probability predictions will bias towards $0$ or $1$. Although having an ensemble of identical trees is unrealistic, the notion illustrates that tree correlation can introduce a *divergence bias*. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</span>
<span id="cb23-133"><a href="#cb23-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-134"><a href="#cb23-134" aria-hidden="true" tabindex="-1"></a>However, divergence bias is problematic in probability applications. If $x_i$ has a known membership of $A$, and an unknown $P_{\text{true}}(x_i=A) = 0.6$, the ensemble might classify $x_i$ correctly $90\%$ of the time leading to $\hat{P}(x_i=A) = 0.9$. As a probability machine, the ensemble has overestimated the probability by $0.3$ even though $90\%$ accuracy is excellent. To predict $P_{\text{true}}(x_i=A) = 0.6$, an ensemble would need to incorrectly classify $x_i$ in $40\%$ of its trees. However, random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</span>
<span id="cb23-135"><a href="#cb23-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-136"><a href="#cb23-136" aria-hidden="true" tabindex="-1"></a>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected subset of the data. Random forests further reduce tree correlation by considering only a random number of variables at each split, referred to as $mtry$. When $mtry$ is near to to number of predictors, the model considers more variables at each split, making the random forest closer to a bagging ensemble. A lower $mtry$ should reduce the correlation between trees and decrease divergence bias, but a lower $mtry$ also introduces other theoretical problems.</span>
<span id="cb23-137"><a href="#cb23-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-138"><a href="#cb23-138" aria-hidden="true" tabindex="-1"></a>Consider the scenario where the binary outcome of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If $mtry$ is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. For example, each predictor has a chance of $\frac{mtry}{\text{number of predictors}}$ of selection at each split implying a lower $mtry$ decreases the chance of a split considering the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of $x_i$ in each tree. Additionally, consider there is a class imbalance and the majority of obvervations are classified as $A$ not $B$. If sucessive noisy splits result in impure terminal nodes, then terminal nodes may be dominated by the majority class $A$. Consequently, there is a *majority class* effect as each tree in the ensemble is more likely to misclassify an observation as an $A$ because the terminal nodes have a higher proportion of $A$ due to the higher proportion of $A$'s in the data overall.</span>
<span id="cb23-139"><a href="#cb23-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-140"><a href="#cb23-140" aria-hidden="true" tabindex="-1"></a>To exemplify this theoretical discussion, consider the National Supported Work (NSW) programme, which is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with $445$ total participants, $185$ in the program group, and $260$ in the control group, so the true probability of treatment for each individual can be calculated as $185/445=0.42$ or $42$%. Further information about this data is found in @sec-data-nsw-jobs.</span>
<span id="cb23-141"><a href="#cb23-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-142"><a href="#cb23-142" aria-hidden="true" tabindex="-1"></a>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although @fig-rf-varimp and @tbl-combined-btab do suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by @Smith2005 who suggests that self-selection bias is prevalent in the NSW data.</span>
<span id="cb23-143"><a href="#cb23-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-144"><a href="#cb23-144" aria-hidden="true" tabindex="-1"></a>@fig-rf-theory-demo shows both divergence bias and majority class effect using <span class="in">`randomForest`</span> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when $mtry$ is equal to the number of predictors and so specifying <span class="in">`mtry = 7`</span> in the <span class="in">`randomForest`</span> function will fit a bagging ensemble. Logistic regression using the <span class="in">`gbm()`</span> function provides a meaningful comparison.</span>
<span id="cb23-145"><a href="#cb23-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-148"><a href="#cb23-148" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-149"><a href="#cb23-149" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-150"><a href="#cb23-150" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-151"><a href="#cb23-151" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb23-152"><a href="#cb23-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-theory-demo</span></span>
<span id="cb23-153"><a href="#cb23-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-154"><a href="#cb23-154" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-155"><a href="#cb23-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](fig-rf-theory-demo)" </span></span>
<span id="cb23-156"><a href="#cb23-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "This figure compares the kernel density estimates of propensity score of each observation in the National Supported Work programme. The random forest and bagging ensemle are fitted with the randomForest default value of 500 trees."</span></span>
<span id="cb23-157"><a href="#cb23-157" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb23-158"><a href="#cb23-158" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb23-159"><a href="#cb23-159" aria-hidden="true" tabindex="-1"></a>nsw_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">as.factor</span>(treat) <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> </span>
<span id="cb23-160"><a href="#cb23-160" aria-hidden="true" tabindex="-1"></a>                          black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> marr)</span>
<span id="cb23-161"><a href="#cb23-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-162"><a href="#cb23-162" aria-hidden="true" tabindex="-1"></a>logit_preds <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb23-163"><a href="#cb23-163" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="fu">binomial</span>())<span class="sc">$</span>fitted.values </span>
<span id="cb23-164"><a href="#cb23-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-165"><a href="#cb23-165" aria-hidden="true" tabindex="-1"></a>rf_mtry1_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">randomForest</span>(nsw_formula, </span>
<span id="cb23-166"><a href="#cb23-166" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">data =</span> nsw_data), </span>
<span id="cb23-167"><a href="#cb23-167" aria-hidden="true" tabindex="-1"></a>                          <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb23-168"><a href="#cb23-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-169"><a href="#cb23-169" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(nsw_formula, <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>, </span>
<span id="cb23-170"><a href="#cb23-170" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> nsw_data)</span>
<span id="cb23-171"><a href="#cb23-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-172"><a href="#cb23-172" aria-hidden="true" tabindex="-1"></a>bagged_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagging_model, <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb23-173"><a href="#cb23-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-174"><a href="#cb23-174" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb23-175"><a href="#cb23-175" aria-hidden="true" tabindex="-1"></a>plot_pmachines <span class="ot">&lt;-</span> <span class="cf">function</span>(preds, title) {</span>
<span id="cb23-176"><a href="#cb23-176" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(nsw_data, <span class="fu">aes</span>(<span class="at">x =</span> preds, <span class="at">fill =</span> <span class="fu">factor</span>(treat))) <span class="sc">+</span></span>
<span id="cb23-177"><a href="#cb23-177" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb23-178"><a href="#cb23-178" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb23-179"><a href="#cb23-179" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Participants"</span>)) <span class="sc">+</span></span>
<span id="cb23-180"><a href="#cb23-180" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">subtitle =</span> title, <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, </span>
<span id="cb23-181"><a href="#cb23-181" aria-hidden="true" tabindex="-1"></a>         <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb23-182"><a href="#cb23-182" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb23-183"><a href="#cb23-183" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb23-184"><a href="#cb23-184" aria-hidden="true" tabindex="-1"></a>    custom_ggplot_theme</span>
<span id="cb23-185"><a href="#cb23-185" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-186"><a href="#cb23-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-187"><a href="#cb23-187" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(logit_preds, <span class="st">"Logistic Regression"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb23-188"><a href="#cb23-188" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span> </span>
<span id="cb23-189"><a href="#cb23-189" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb23-190"><a href="#cb23-190" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb23-191"><a href="#cb23-191" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb23-192"><a href="#cb23-192" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb23-193"><a href="#cb23-193" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)</span>
<span id="cb23-194"><a href="#cb23-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-195"><a href="#cb23-195" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(rf_mtry1_preds, <span class="st">"Random Forest (mtry = 1)"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb23-196"><a href="#cb23-196" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>)</span>
<span id="cb23-197"><a href="#cb23-197" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(bagged_preds, <span class="st">"Bagging (Bootstrap Aggregation)"</span>)</span>
<span id="cb23-198"><a href="#cb23-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-199"><a href="#cb23-199" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb23-200"><a href="#cb23-200" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">/</span> p2 <span class="sc">/</span> p3 <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb23-201"><a href="#cb23-201" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>)</span>
<span id="cb23-202"><a href="#cb23-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-203"><a href="#cb23-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-206"><a href="#cb23-206" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-207"><a href="#cb23-207" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-208"><a href="#cb23-208" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-209"><a href="#cb23-209" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-varimp</span></span>
<span id="cb23-210"><a href="#cb23-210" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-211"><a href="#cb23-211" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-212"><a href="#cb23-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create the Plot"</span></span>
<span id="cb23-213"><a href="#cb23-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The figure compares the variable importance assigned to each variable from a baggin ensemble. The data originates from the National Supported Work programme. The difference in relative important of some variables indicates that randomisation may not have created exchangability between the groups."</span></span>
<span id="cb23-214"><a href="#cb23-214" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb23-215"><a href="#cb23-215" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb23-216"><a href="#cb23-216" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb23-217"><a href="#cb23-217" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb23-218"><a href="#cb23-218" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb23-219"><a href="#cb23-219" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb23-220"><a href="#cb23-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-221"><a href="#cb23-221" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb23-222"><a href="#cb23-222" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb23-223"><a href="#cb23-223" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb23-224"><a href="#cb23-224" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb23-225"><a href="#cb23-225" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb23-226"><a href="#cb23-226" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb23-227"><a href="#cb23-227" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb23-228"><a href="#cb23-228" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb23-229"><a href="#cb23-229" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb23-230"><a href="#cb23-230" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb23-231"><a href="#cb23-231" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>,</span>
<span id="cb23-232"><a href="#cb23-232" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb23-233"><a href="#cb23-233" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb23-234"><a href="#cb23-234" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb23-235"><a href="#cb23-235" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"none"</span></span>
<span id="cb23-236"><a href="#cb23-236" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb23-237"><a href="#cb23-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-238"><a href="#cb23-238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-239"><a href="#cb23-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-240"><a href="#cb23-240" aria-hidden="true" tabindex="-1"></a>The logistic regression model has identified a central tendency and most probabilities are between $0.25$ and $0.75$ which roughly aligns with the true probability. For the random forest with $mtry=1$, a significant number of the treatment and control observations are centred near $0$ with a wide range of other predictions. Such behaviour is consistent with a model overly predicting the majority class and having unstable predictions otherwise. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside $0.25$ and $0.75$. Compared to the theoretically true probability, both random forest and bagging ensembles have performed poorly.</span>
<span id="cb23-241"><a href="#cb23-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-242"><a href="#cb23-242" aria-hidden="true" tabindex="-1"></a>The tuning of $mtry$ faces double jeopardy and is another important area of discussion in probability machines. The selection of $mtry$ is typically completed in with a classification loss function such as accuracy or out-of-bag error. @Olson2018 compares tuning $mtry$ measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of $mtry$ for classification differs greatly from for probability prediction.<span class="ot">[^propensity-2]</span> In other words, if a grid search finds that $mtry=3$ is optimal for a classification task, this does not imply that $mtry=3$ is optimal for predicting probabilities.</span>
<span id="cb23-243"><a href="#cb23-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-244"><a href="#cb23-244" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-2]: </span>Note that tuning $mtry$ for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.</span>
<span id="cb23-245"><a href="#cb23-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-246"><a href="#cb23-246" aria-hidden="true" tabindex="-1"></a>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in @sec-mlps-sims. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</span>
<span id="cb23-247"><a href="#cb23-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-248"><a href="#cb23-248" aria-hidden="true" tabindex="-1"></a>Anecdotally, divergence bias and majority class effects will most effect a probability machine when there is considerable overlap between groups. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common overlap may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low $mtry$, the terminal nodes of each tree may be relatively impure leading to a majority class effect. Alternatively, if true probabilities exist near $0$ or $1$ and there is a clear separation of class, divergence effects may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low $mtry$ may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</span>
<span id="cb23-249"><a href="#cb23-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-250"><a href="#cb23-250" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- maybe need a little chat about cross entropy here and why its not as good as in the gbm case.  --&gt;</span></span>
<span id="cb23-251"><a href="#cb23-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-252"><a href="#cb23-252" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Boosting Machines as Probability Machines</span></span>
<span id="cb23-253"><a href="#cb23-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-254"><a href="#cb23-254" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- needs a little more comparison to rf and bagging. perhaps do some more reserach about why these are good. note and differentiate the different types of boosting. perhaps also clarify how the gradient descent works for my own learning (dont write it to be too technical).  --&gt;</span></span>
<span id="cb23-255"><a href="#cb23-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-256"><a href="#cb23-256" aria-hidden="true" tabindex="-1"></a>Moving beyond classification trees in random forests or bagging ensembles, @Friedman2001 introduced the *Gradient Boosting Machine* (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss until it reaches its minimum.</span>
<span id="cb23-257"><a href="#cb23-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-258"><a href="#cb23-258" aria-hidden="true" tabindex="-1"></a>GBM’s can be be *generalised* to many different applications by providing different loss functions that can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple *boosted* regression trees and a logistic function to transform regression predictions into probabilities. This logistic function is the same as in logistic regression, and so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood function. The model is expected to be well-calibrated, as maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The GBM outputs probability predictions, avoiding the issues associated with vote count methods used by random forests and bagging ensembles.</span>
<span id="cb23-259"><a href="#cb23-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-260"><a href="#cb23-260" aria-hidden="true" tabindex="-1"></a>Additionally, each split considers all variables and makes the most informative splits that descend the loss function most effectively. GBMs utilize many weak learners, where each learner is only slightly better than random guessing. These weak learners are often regression stumps, which are CART models with only a single split. However, additional splits enable the model to capture interactions between terms which may increase performance in complex or high-dimensional datasets.</span>
<span id="cb23-261"><a href="#cb23-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-262"><a href="#cb23-262" aria-hidden="true" tabindex="-1"></a>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques as well as allowing a probability distribution based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. The implementation and workflow to fit a GBM for propensity scores, is discussed in @sec-gbm-tune-workflow.</span>
<span id="cb23-263"><a href="#cb23-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-264"><a href="#cb23-264" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting</span></span>
<span id="cb23-265"><a href="#cb23-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-266"><a href="#cb23-266" aria-hidden="true" tabindex="-1"></a>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting is used to prevent overfitting to ensure a model can generalize to unseen data.</span>
<span id="cb23-267"><a href="#cb23-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-268"><a href="#cb23-268" aria-hidden="true" tabindex="-1"></a>However, the model is not required to generalise when predicting propensity scores, as a different propensity score model is fit for a other datasets. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</span>
<span id="cb23-269"><a href="#cb23-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-270"><a href="#cb23-270" aria-hidden="true" tabindex="-1"></a>::: {#nte-overfit-logistic .callout-note title="Overfitting in Logistic Regression"}</span>
<span id="cb23-271"><a href="#cb23-271" aria-hidden="true" tabindex="-1"></a>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is @Schuster2016, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards $0$ or $1$ because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to $0$ or $1$ receive artificially inflated weighting.</span>
<span id="cb23-272"><a href="#cb23-272" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-273"><a href="#cb23-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-274"><a href="#cb23-274" aria-hidden="true" tabindex="-1"></a>@Lee2010 simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</span>
<span id="cb23-275"><a href="#cb23-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-276"><a href="#cb23-276" aria-hidden="true" tabindex="-1"></a>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</span>
<span id="cb23-277"><a href="#cb23-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-278"><a href="#cb23-278" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparison of Machine Learning Algorithms: Simulation Results {#sec-mlps-sims}</span></span>
<span id="cb23-279"><a href="#cb23-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-280"><a href="#cb23-280" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- todo:  --&gt;</span></span>
<span id="cb23-281"><a href="#cb23-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-282"><a href="#cb23-282" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- fix early cite --&gt;</span></span>
<span id="cb23-283"><a href="#cb23-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-284"><a href="#cb23-284" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- clarify that sim studies are propensity score based. maybe look for general probability machine example sims.  --&gt;</span></span>
<span id="cb23-285"><a href="#cb23-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-286"><a href="#cb23-286" aria-hidden="true" tabindex="-1"></a>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="co">[</span><span class="ot">see @McCaffrey2004; @Setoguchi2008; @Lee2010; @Cannas2019; @Tu2019; @Goller2020; @Ferri2020</span><span class="co">]</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</span>
<span id="cb23-287"><a href="#cb23-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-288"><a href="#cb23-288" aria-hidden="true" tabindex="-1"></a>@Tu2019 compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</span>
<span id="cb23-289"><a href="#cb23-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-290"><a href="#cb23-290" aria-hidden="true" tabindex="-1"></a>Despite poor theoretical properties as a probability machine, @Lee2010 find that bagging results in the lowest standard error across many datasets.<span class="ot">[^propensity-3]</span> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</span>
<span id="cb23-291"><a href="#cb23-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-292"><a href="#cb23-292" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-3]: </span>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.</span>
<span id="cb23-293"><a href="#cb23-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-294"><a href="#cb23-294" aria-hidden="true" tabindex="-1"></a>Additionally, @Lee2010 finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best $95\%$ coverage in all simulation scenarios, with $98.6\%$ coverage.<span class="ot">[^propensity-4]</span></span>
<span id="cb23-295"><a href="#cb23-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-296"><a href="#cb23-296" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-4]: </span>In this context, the coverage is the proportion of times that the true treatment effect is within the $95\%$ confidence interval across the number of simulations. This author implements $1000$ simulations of each scenario.</span>
<span id="cb23-297"><a href="#cb23-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-298"><a href="#cb23-298" aria-hidden="true" tabindex="-1"></a>@Cannas2019 also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <span class="ot">[^propensity-5]</span> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</span>
<span id="cb23-299"><a href="#cb23-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-300"><a href="#cb23-300" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-5]: </span>@Cannas2019 provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <span class="in">`twang`</span> and <span class="in">`gbm`</span> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benifits discussed in @cross-reference.</span>
<span id="cb23-301"><a href="#cb23-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-302"><a href="#cb23-302" aria-hidden="true" tabindex="-1"></a>@Goller2020 adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<span class="ot">[^propensity-6]</span> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</span>
<span id="cb23-303"><a href="#cb23-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-304"><a href="#cb23-304" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-6]: </span>@Goller2020 calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the *true* treatment effect. Thus, the covariate balance table offers a clearer view of each method’s performance.</span>
<span id="cb23-305"><a href="#cb23-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-306"><a href="#cb23-306" aria-hidden="true" tabindex="-1"></a>Based on a review of the literature, the findings can be distilled into five important points:</span>
<span id="cb23-307"><a href="#cb23-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-308"><a href="#cb23-308" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</span>
<span id="cb23-309"><a href="#cb23-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-310"><a href="#cb23-310" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="co">[</span><span class="ot">@Rosenbaum1983</span><span class="co">]</span>.</span>
<span id="cb23-311"><a href="#cb23-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-312"><a href="#cb23-312" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Bagging ensembles perform poorly, a finding replicated across multiple studies.</span>
<span id="cb23-313"><a href="#cb23-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-314"><a href="#cb23-314" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</span>
<span id="cb23-315"><a href="#cb23-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-316"><a href="#cb23-316" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</span>
<span id="cb23-317"><a href="#cb23-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-318"><a href="#cb23-318" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</span>
<span id="cb23-319"><a href="#cb23-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-320"><a href="#cb23-320" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</span>
<span id="cb23-321"><a href="#cb23-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-322"><a href="#cb23-322" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implimentation and Hyperparameter Tuning with `WeightIt` and`gbm` in R</span></span>
<span id="cb23-323"><a href="#cb23-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-324"><a href="#cb23-324" aria-hidden="true" tabindex="-1"></a>Based on @Friedman2001, the <span class="in">`gbm`</span> package implements a *Generalized Boosting Machine*. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. <span class="in">`gbm`</span> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <span class="in">`bag.fraction`</span> parameter.</span>
<span id="cb23-325"><a href="#cb23-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-326"><a href="#cb23-326" aria-hidden="true" tabindex="-1"></a>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <span class="in">`gbm`</span> package does not support this type of tuning, a wrapper package like <span class="in">`WeightIt`</span> is necessary. <span class="in">`WeightIt`</span> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <span class="in">`WeightIt`</span> supports hyperparameter turning of <span class="in">`shrinkage`</span>, <span class="in">`interaction.depth`</span>, and <span class="in">`n.trees`</span>. Once the best model is identified, propensity scores are predicted inside <span class="in">`WeightIt`</span>. These can be used inside <span class="in">`WeightIt`</span> to perform IPW or extracted for other implementations. <span class="in">`WeightIt`</span> also supports an offset meaning that logistic regression predictions are supplied to the <span class="in">`GBM`</span> package.</span>
<span id="cb23-327"><a href="#cb23-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-328"><a href="#cb23-328" aria-hidden="true" tabindex="-1"></a>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="co">[</span><span class="ot">see @McCaffrey2004; @Ridgeway2024</span><span class="co">]</span>. A very low learning rate, such as $0.01$ or $0.0005$, allows a smooth descent of the loss function. The model should include a high number of trees, with $10,000$ or $20,000$ being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for $3$ splits. Based on anecdotal experience, $1$ to $5$ splits per tree is optimal, consistent with recommendations by @McCaffrey2004.</span>
<span id="cb23-329"><a href="#cb23-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-330"><a href="#cb23-330" aria-hidden="true" tabindex="-1"></a>Another package, <span class="in">`twang`</span>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <span class="in">`caret`</span> must be used. Although <span class="in">`twang`</span> has other useful functionalities which users may wish to implement.</span>
<span id="cb23-331"><a href="#cb23-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-332"><a href="#cb23-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperparameter Tuning and Workflow {#sec-gbm-tune-workflow}</span></span>
<span id="cb23-333"><a href="#cb23-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-334"><a href="#cb23-334" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  --&gt;</span></span>
<span id="cb23-335"><a href="#cb23-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-336"><a href="#cb23-336" aria-hidden="true" tabindex="-1"></a>The <span class="in">`WeigthtIt`</span> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <span class="in">`cobalt`</span>. The best information for this package can be found on this <span class="co">[</span><span class="ot">website</span><span class="co">](https://ngreifer.github.io/WeightIt/index.html)</span> or accessed with <span class="in">`vignette("WeightIt")`</span> inside R after installation using <span class="in">`install.packages("WeightIt")`</span>.</span>
<span id="cb23-337"><a href="#cb23-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-338"><a href="#cb23-338" aria-hidden="true" tabindex="-1"></a>A workflow for hyperparameter tuning in <span class="in">`WeightIt`</span> may be completed as follows:</span>
<span id="cb23-339"><a href="#cb23-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-340"><a href="#cb23-340" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specify the <span class="in">`criterion`</span> option, which specifies the measure of the *”best model”*. The available options are the options that the <span class="in">`cobalt`</span> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <span class="in">`sdm.mean`</span> or the smallest maximum SDM across covariates called <span class="in">`sdm.max`</span>.</span>
<span id="cb23-341"><a href="#cb23-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-342"><a href="#cb23-342" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Set the number of trees high. The package default is <span class="in">`n.trees = 10000`</span> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <span class="in">`n.tree`</span> if optimal.</span>
<span id="cb23-343"><a href="#cb23-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-344"><a href="#cb23-344" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specify the grid search for the depth of the tree called <span class="in">`interaction.depth`</span> and the learning rate called <span class="in">`shrinkage`</span>. These values can be specified using <span class="in">`c()`</span> such as <span class="in">`shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)`</span> or as integers such as <span class="in">`interaction.depth = 1:5`</span>. These particular values are heuristically selected *suggestions* of good starting values. Additionally, an offset can be considered by performing a grid search across <span class="in">`offset=c(TRUE,FALSE)`</span>.</span>
<span id="cb23-345"><a href="#cb23-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-346"><a href="#cb23-346" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <span class="in">`my_weightit_object$info$best.tune`</span>.</span>
<span id="cb23-347"><a href="#cb23-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-348"><a href="#cb23-348" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <span class="in">`interaction.depth = 1:5`</span> and the best fit is $5$, then a new search can consider <span class="in">`interaction.depth = 3:7`</span> so that the local area around $5$ can be searched.</span>
<span id="cb23-349"><a href="#cb23-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-350"><a href="#cb23-350" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Experiment with <span class="in">`bag.fraction`</span>, which means each tree will consider a drawn proportion of observations equal to <span class="in">`bag.fraction`</span>. Iteratively changing <span class="in">`bag.fraction`</span> and assessing balance at each value should be practical. Consider $0.5$, $0.67$, and $1$.</span>
<span id="cb23-351"><a href="#cb23-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-352"><a href="#cb23-352" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <span class="in">`love.plot()`</span> such as @fig-coffee-replication-lplot.</span>
<span id="cb23-353"><a href="#cb23-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-354"><a href="#cb23-354" aria-hidden="true" tabindex="-1"></a><span class="ss">8.  </span>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</span>
<span id="cb23-355"><a href="#cb23-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-356"><a href="#cb23-356" aria-hidden="true" tabindex="-1"></a><span class="ss">9.  </span>Estimation and reporting of treatment effect.</span>
<span id="cb23-357"><a href="#cb23-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-358"><a href="#cb23-358" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: NSW Jobs Dataset Using R</span></span>
<span id="cb23-359"><a href="#cb23-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-360"><a href="#cb23-360" aria-hidden="true" tabindex="-1"></a>For demonstration, propensity scores are estimated following the workflow discussed in @sec-gbm-tune-workflow to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in @sec-data-nsw-jobs. Randomisation should eliminate structural differences between groups, but @Rosenbaum1983 notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <span class="in">`WeightIt`</span> and a logistic regression model using <span class="in">`glm()`</span>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</span>
<span id="cb23-361"><a href="#cb23-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-362"><a href="#cb23-362" aria-hidden="true" tabindex="-1"></a>::: {#nte-ipw .callout-note title="Inverse Probability of Treatment Weighting"}</span>
<span id="cb23-363"><a href="#cb23-363" aria-hidden="true" tabindex="-1"></a>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a *pseudo-population* where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</span>
<span id="cb23-364"><a href="#cb23-364" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-365"><a href="#cb23-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-366"><a href="#cb23-366" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 1-6: Model Fitting and Tuning</span></span>
<span id="cb23-367"><a href="#cb23-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-368"><a href="#cb23-368" aria-hidden="true" tabindex="-1"></a>The <span class="in">`glm()`</span> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <span class="in">`binomial()`</span>. Recall the <span class="in">`nsw_formula`</span> is specified in @sec-bagg-rf-probmachines</span>
<span id="cb23-369"><a href="#cb23-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-370"><a href="#cb23-370" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-371"><a href="#cb23-371" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_logistc_pmodel</span></span>
<span id="cb23-372"><a href="#cb23-372" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb23-373"><a href="#cb23-373" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-374"><a href="#cb23-374" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false </span></span>
<span id="cb23-375"><a href="#cb23-375" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_pmodel &lt;- glm(nsw_formula, data = nsw_data,</span></span>
<span id="cb23-376"><a href="#cb23-376" aria-hidden="true" tabindex="-1"></a><span class="in">                        family=binomial()) #&lt;1&gt;</span></span>
<span id="cb23-377"><a href="#cb23-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-378"><a href="#cb23-378" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_pscores &lt;- nsw_logit_pmodel$fitted.values#&lt;2&gt;</span></span>
<span id="cb23-379"><a href="#cb23-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-380"><a href="#cb23-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-381"><a href="#cb23-381" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb23-382"><a href="#cb23-382" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<span id="cb23-383"><a href="#cb23-383" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>()) <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-384"><a href="#cb23-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-385"><a href="#cb23-385" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values<span class="co">#&lt;2&gt;</span></span>
<span id="cb23-386"><a href="#cb23-386" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-387"><a href="#cb23-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-388"><a href="#cb23-388" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Fits a logistic regression model using the <span class="in">`glm()`</span> function specified to be a logistic model with <span class="in">`family=binomial()`</span> using the previously created <span class="in">`nsw_formula`</span>.</span>
<span id="cb23-389"><a href="#cb23-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-390"><a href="#cb23-390" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Extracts the fitted values (propensity scores) from the model.</span>
<span id="cb23-391"><a href="#cb23-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-392"><a href="#cb23-392" aria-hidden="true" tabindex="-1"></a>Using the propensity score column of <span class="in">`nsw_data`</span>, the <span class="in">`WeightIt`</span> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <span class="in">`nsw_logit_weight`</span>.</span>
<span id="cb23-393"><a href="#cb23-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-396"><a href="#cb23-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-397"><a href="#cb23-397" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb23-398"><a href="#cb23-398" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-399"><a href="#cb23-399" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-400"><a href="#cb23-400" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb23-401"><a href="#cb23-401" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-402"><a href="#cb23-402" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,<span class="co">#&lt;2&gt;</span></span>
<span id="cb23-403"><a href="#cb23-403" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)<span class="co">#&lt;3&gt;</span></span>
<span id="cb23-404"><a href="#cb23-404" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-405"><a href="#cb23-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-406"><a href="#cb23-406" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb23-407"><a href="#cb23-407" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb23-408"><a href="#cb23-408" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-409"><a href="#cb23-409" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,<span class="co">#&lt;2&gt;</span></span>
<span id="cb23-410"><a href="#cb23-410" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)<span class="co">#&lt;3&gt;</span></span>
<span id="cb23-411"><a href="#cb23-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-412"><a href="#cb23-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-413"><a href="#cb23-413" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specifies the formula and data.</span>
<span id="cb23-414"><a href="#cb23-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-415"><a href="#cb23-415" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Provides <span class="in">`weightit()`</span> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <span class="in">`weightit()`</span> function with <span class="in">`method = "glm"`</span>. The separate estimation of the propensity scores is for illustrative purposes.</span>
<span id="cb23-416"><a href="#cb23-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-417"><a href="#cb23-417" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
<span id="cb23-418"><a href="#cb23-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-419"><a href="#cb23-419" aria-hidden="true" tabindex="-1"></a>A GBM model for propensity scores can be specified using <span class="in">`method = "gbm"`</span> inside the <span class="in">`weightit()`</span> function. To ensure consistent results, running <span class="in">`set.seed(88)`</span> will ensure each tree uses the same <span class="in">`seed`</span> if <span class="in">`bag.fraction`</span> less than $1$. The model is fit using the heuristically suggested starting values. Note that this model may take approximately $30$ second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</span>
<span id="cb23-420"><a href="#cb23-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-421"><a href="#cb23-421" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb23-422"><a href="#cb23-422" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb23-423"><a href="#cb23-423" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-424"><a href="#cb23-424" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>, <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-425"><a href="#cb23-425" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-426"><a href="#cb23-426" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>), <span class="co">#&lt;3&gt;</span></span>
<span id="cb23-427"><a href="#cb23-427" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="co">#&lt;3&gt;</span></span>
<span id="cb23-428"><a href="#cb23-428" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>, <span class="co">#&lt;4&gt;</span></span>
<span id="cb23-429"><a href="#cb23-429" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>), <span class="co">#&lt;4&gt;</span></span>
<span id="cb23-430"><a href="#cb23-430" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, <span class="co">#&lt;5&gt;</span></span>
<span id="cb23-431"><a href="#cb23-431" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>) <span class="co">#&lt;5&gt;</span></span>
<span id="cb23-432"><a href="#cb23-432" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune) <span class="co">#&lt;6&gt;</span></span>
<span id="cb23-433"><a href="#cb23-433" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-434"><a href="#cb23-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-435"><a href="#cb23-435" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specifies the formula and data.</span>
<span id="cb23-436"><a href="#cb23-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-437"><a href="#cb23-437" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
<span id="cb23-438"><a href="#cb23-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-439"><a href="#cb23-439" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Performs a grid search over these values of the learning rate and depth of tree.</span>
<span id="cb23-440"><a href="#cb23-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-441"><a href="#cb23-441" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
<span id="cb23-442"><a href="#cb23-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-443"><a href="#cb23-443" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be $10000$ which is the package default.</span>
<span id="cb23-444"><a href="#cb23-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-445"><a href="#cb23-445" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Prints the tune details of the model with the best covariate balance.</span>
<span id="cb23-446"><a href="#cb23-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-447"><a href="#cb23-447" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- clarify the meaning of learning rate/shrinkage --&gt;</span></span>
<span id="cb23-448"><a href="#cb23-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-449"><a href="#cb23-449" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- change all the instructions to active speech not passive.  --&gt;</span></span>
<span id="cb23-450"><a href="#cb23-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-451"><a href="#cb23-451" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-452"><a href="#cb23-452" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_boosted_weight</span></span>
<span id="cb23-453"><a href="#cb23-453" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb23-454"><a href="#cb23-454" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-455"><a href="#cb23-455" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-456"><a href="#cb23-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-457"><a href="#cb23-457" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb23-458"><a href="#cb23-458" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_weight &lt;- weightit(nsw_formula, data = nsw_data, </span></span>
<span id="cb23-459"><a href="#cb23-459" aria-hidden="true" tabindex="-1"></a><span class="in">                               method="gbm",</span></span>
<span id="cb23-460"><a href="#cb23-460" aria-hidden="true" tabindex="-1"></a><span class="in">                               estimand = "ATE", </span></span>
<span id="cb23-461"><a href="#cb23-461" aria-hidden="true" tabindex="-1"></a><span class="in">                               shrinkage= c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3),</span></span>
<span id="cb23-462"><a href="#cb23-462" aria-hidden="true" tabindex="-1"></a><span class="in">                               interaction.depth = 1:5,</span></span>
<span id="cb23-463"><a href="#cb23-463" aria-hidden="true" tabindex="-1"></a><span class="in">                               bag.fraction = 1,</span></span>
<span id="cb23-464"><a href="#cb23-464" aria-hidden="true" tabindex="-1"></a><span class="in">                               offset = c(TRUE, FALSE),</span></span>
<span id="cb23-465"><a href="#cb23-465" aria-hidden="true" tabindex="-1"></a><span class="in">                               criterion = "smd.mean", </span></span>
<span id="cb23-466"><a href="#cb23-466" aria-hidden="true" tabindex="-1"></a><span class="in">                               n.trees = 10000)</span></span>
<span id="cb23-467"><a href="#cb23-467" aria-hidden="true" tabindex="-1"></a><span class="in">print(nsw_boosted_weight$info$best.tune)</span></span>
<span id="cb23-468"><a href="#cb23-468" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-469"><a href="#cb23-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-470"><a href="#cb23-470" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- cite what the balance statistics should be in the intro when discussing propensity score and balance.  --&gt;</span></span>
<span id="cb23-471"><a href="#cb23-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-472"><a href="#cb23-472" aria-hidden="true" tabindex="-1"></a>The best balance across all tuning combinations yields an average SMD of $0.023$ showing strong balance. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in @sec-nsw-balance accompanying a discussion of balance.</span>
<span id="cb23-473"><a href="#cb23-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-474"><a href="#cb23-474" aria-hidden="true" tabindex="-1"></a>The best machine has a learning rate of $0.3$ and contains $2392$ decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to $0.3$. A reduction in the depth of tree and number of trees will reduce computation time.</span>
<span id="cb23-475"><a href="#cb23-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-476"><a href="#cb23-476" aria-hidden="true" tabindex="-1"></a>The new tune grid will consider <span class="in">`shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)`</span> as this allows the GBM to consider values between $0.2$ and $0.3$ and above $0.3$ which were missing in the previous grid.</span>
<span id="cb23-477"><a href="#cb23-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-478"><a href="#cb23-478" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-479"><a href="#cb23-479" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_boosted_weight2</span></span>
<span id="cb23-480"><a href="#cb23-480" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-481"><a href="#cb23-481" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-482"><a href="#cb23-482" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb23-483"><a href="#cb23-483" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "PALCEHOLDER"</span></span>
<span id="cb23-484"><a href="#cb23-484" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb23-485"><a href="#cb23-485" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_weight2 &lt;- weightit(nsw_formula, data = nsw_data, </span></span>
<span id="cb23-486"><a href="#cb23-486" aria-hidden="true" tabindex="-1"></a><span class="in">                               method="gbm",</span></span>
<span id="cb23-487"><a href="#cb23-487" aria-hidden="true" tabindex="-1"></a><span class="in">                               estimand = "ATE", </span></span>
<span id="cb23-488"><a href="#cb23-488" aria-hidden="true" tabindex="-1"></a><span class="in">                               shrinkage= c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5),</span></span>
<span id="cb23-489"><a href="#cb23-489" aria-hidden="true" tabindex="-1"></a><span class="in">                               interaction.depth = 1:3,</span></span>
<span id="cb23-490"><a href="#cb23-490" aria-hidden="true" tabindex="-1"></a><span class="in">                               bag.fraction = 1,</span></span>
<span id="cb23-491"><a href="#cb23-491" aria-hidden="true" tabindex="-1"></a><span class="in">                               offset = c(TRUE, FALSE),</span></span>
<span id="cb23-492"><a href="#cb23-492" aria-hidden="true" tabindex="-1"></a><span class="in">                               criterion = "smd.mean", </span></span>
<span id="cb23-493"><a href="#cb23-493" aria-hidden="true" tabindex="-1"></a><span class="in">                               n.trees = 5000)</span></span>
<span id="cb23-494"><a href="#cb23-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-495"><a href="#cb23-495" aria-hidden="true" tabindex="-1"></a><span class="in">print(nsw_boosted_weight2$info$best.tune)</span></span>
<span id="cb23-496"><a href="#cb23-496" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-497"><a href="#cb23-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-498"><a href="#cb23-498" aria-hidden="true" tabindex="-1"></a>Comparing the two iterations, there is a reduction from $0.022$ to $0.02$. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of $0.45$, a tree depth of $2$, and $95$ trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</span>
<span id="cb23-499"><a href="#cb23-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-500"><a href="#cb23-500" aria-hidden="true" tabindex="-1"></a>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, @fig-balance-iterations shows the optimal number of trees is highly variable. If the learning rate is set to <span class="in">`shrinkage = 0.05`</span>, then the best balance is not achieved until near to $20,000$ trees.</span>
<span id="cb23-501"><a href="#cb23-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-504"><a href="#cb23-504" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-505"><a href="#cb23-505" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-balance-iterations</span></span>
<span id="cb23-506"><a href="#cb23-506" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Average Standardised Mean Differernce (Covaraite Balance) and the number of interations. Please note the difference in horozontal scale between the two plots."</span></span>
<span id="cb23-507"><a href="#cb23-507" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-508"><a href="#cb23-508" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-509"><a href="#cb23-509" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Make the @Rosenbaum1983"</span></span>
<span id="cb23-510"><a href="#cb23-510" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-511"><a href="#cb23-511" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-512"><a href="#cb23-512" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb23-513"><a href="#cb23-513" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb23-514"><a href="#cb23-514" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb23-515"><a href="#cb23-515" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb23-516"><a href="#cb23-516" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb23-517"><a href="#cb23-517" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb23-518"><a href="#cb23-518" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb23-519"><a href="#cb23-519" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb23-520"><a href="#cb23-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-521"><a href="#cb23-521" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb23-522"><a href="#cb23-522" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb23-523"><a href="#cb23-523" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb23-524"><a href="#cb23-524" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb23-525"><a href="#cb23-525" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb23-526"><a href="#cb23-526" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb23-527"><a href="#cb23-527" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb23-528"><a href="#cb23-528" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb23-529"><a href="#cb23-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-530"><a href="#cb23-530" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb23-531"><a href="#cb23-531" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb23-532"><a href="#cb23-532" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb23-533"><a href="#cb23-533" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb23-534"><a href="#cb23-534" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb23-535"><a href="#cb23-535" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span> </span>
<span id="cb23-536"><a href="#cb23-536" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb23-537"><a href="#cb23-537" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb23-538"><a href="#cb23-538" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb23-539"><a href="#cb23-539" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb23-540"><a href="#cb23-540" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb23-541"><a href="#cb23-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-542"><a href="#cb23-542" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb23-543"><a href="#cb23-543" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>)</span>
<span id="cb23-544"><a href="#cb23-544" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-545"><a href="#cb23-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-546"><a href="#cb23-546" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb23-547"><a href="#cb23-547" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;!-- showtext doesnt seem to be working here. Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :</span></span>
<span id="cb23-548"><a href="#cb23-548" aria-hidden="true" tabindex="-1"></a><span class="in">font family 'Source Sans Pro' not found, will use 'sans' instead. perhaps need to define the font in side the plot_annotation()--&gt;</span></span>
<span id="cb23-549"><a href="#cb23-549" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-550"><a href="#cb23-550" aria-hidden="true" tabindex="-1"></a>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</span>
<span id="cb23-551"><a href="#cb23-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-552"><a href="#cb23-552" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 7 and 8: Assessing Balance {#sec-nsw-balance}</span></span>
<span id="cb23-553"><a href="#cb23-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-554"><a href="#cb23-554" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title="The Importance of Discussing Balance"}</span>
<span id="cb23-555"><a href="#cb23-555" aria-hidden="true" tabindex="-1"></a>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study’s conclusions. @King2019 notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</span>
<span id="cb23-556"><a href="#cb23-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-557"><a href="#cb23-557" aria-hidden="true" tabindex="-1"></a>A good resource of information for assessing balance is documentation from the <span class="in">`cobalt`</span> package, which can be viewed by running <span class="in">`vignette(“cobalt”, package = “cobalt”)`</span> in R.</span>
<span id="cb23-558"><a href="#cb23-558" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-559"><a href="#cb23-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-560"><a href="#cb23-560" aria-hidden="true" tabindex="-1"></a><span class="in">`cobalt`</span> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <span class="in">`WeightIt`</span> for IPW and <span class="in">`MatchIt`</span> for propensity score matching. Balance tables are created using <span class="in">`bal.tab()`</span>.</span>
<span id="cb23-561"><a href="#cb23-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-562"><a href="#cb23-562" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make sure this comment about integration is not repeditive  --&gt;</span></span>
<span id="cb23-563"><a href="#cb23-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-566"><a href="#cb23-566" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-567"><a href="#cb23-567" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nsw-btab-logit</span></span>
<span id="cb23-568"><a href="#cb23-568" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb23-569"><a href="#cb23-569" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-570"><a href="#cb23-570" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-571"><a href="#cb23-571" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt) <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-572"><a href="#cb23-572" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight, <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-573"><a href="#cb23-573" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data, <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-574"><a href="#cb23-574" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),<span class="co">#&lt;3&gt;</span></span>
<span id="cb23-575"><a href="#cb23-575" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,<span class="co">#&lt;3&gt;</span></span>
<span id="cb23-576"><a href="#cb23-576" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))<span class="co">#&lt;4&gt;</span></span>
<span id="cb23-577"><a href="#cb23-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-578"><a href="#cb23-578" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] <span class="co">#&lt;5&gt;</span></span>
<span id="cb23-579"><a href="#cb23-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-580"><a href="#cb23-580" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-581"><a href="#cb23-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-582"><a href="#cb23-582" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb23-583"><a href="#cb23-583" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt) <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-584"><a href="#cb23-584" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight, <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-585"><a href="#cb23-585" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data, <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-586"><a href="#cb23-586" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>), <span class="co">#&lt;3&gt;</span></span>
<span id="cb23-587"><a href="#cb23-587" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>, <span class="co">#&lt;3&gt;</span></span>
<span id="cb23-588"><a href="#cb23-588" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>)) <span class="co">#&lt;4&gt;</span></span>
<span id="cb23-589"><a href="#cb23-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-590"><a href="#cb23-590" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] <span class="co">#&lt;5&gt;</span></span>
<span id="cb23-591"><a href="#cb23-591" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-592"><a href="#cb23-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-593"><a href="#cb23-593" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Loads the <span class="in">`cobalt`</span> package. This assumes the package is already installed with <span class="in">`install.packages("cobalt")`</span></span>
<span id="cb23-594"><a href="#cb23-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-595"><a href="#cb23-595" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Uses the <span class="in">`bal.tab()`</span> fucntion to create balance statistics for the previously created <span class="in">`nsw_logit_weight`</span> model.</span>
<span id="cb23-596"><a href="#cb23-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-597"><a href="#cb23-597" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
<span id="cb23-598"><a href="#cb23-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-599"><a href="#cb23-599" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Sets a threshold of balance to be $0.1$ to determine if a covariate is balanced.</span>
<span id="cb23-600"><a href="#cb23-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-601"><a href="#cb23-601" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Extracts the balance table of the <span class="in">`nsw_logit_btab`</span> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
<span id="cb23-602"><a href="#cb23-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-603"><a href="#cb23-603" aria-hidden="true" tabindex="-1"></a>Additionally, <span class="in">`bal.tab()`</span> will create balance tables for the GBM method's IPWs and the raw data. For presentation, <span class="in">`dplyr`</span> combines each of the individual balance tables for presentation using <span class="in">`kable`</span> and <span class="in">`kableExtra`</span>.</span>
<span id="cb23-604"><a href="#cb23-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-605"><a href="#cb23-605" aria-hidden="true" tabindex="-1"></a><span class="in">```{r nsw_boosted_btab, cache=TRUE}</span></span>
<span id="cb23-606"><a href="#cb23-606" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw-btab-raw-boosted</span></span>
<span id="cb23-607"><a href="#cb23-607" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-608"><a href="#cb23-608" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-609"><a href="#cb23-609" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb23-610"><a href="#cb23-610" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show the Code to See Creation of Balance Tables"</span></span>
<span id="cb23-611"><a href="#cb23-611" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_btab &lt;- bal.tab(nsw_boosted_weight, </span></span>
<span id="cb23-612"><a href="#cb23-612" aria-hidden="true" tabindex="-1"></a><span class="in">                            data = nsw_data,</span></span>
<span id="cb23-613"><a href="#cb23-613" aria-hidden="true" tabindex="-1"></a><span class="in">                            stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb23-614"><a href="#cb23-614" aria-hidden="true" tabindex="-1"></a><span class="in">                            binary = "std", continuous = "std",</span></span>
<span id="cb23-615"><a href="#cb23-615" aria-hidden="true" tabindex="-1"></a><span class="in">                            thresholds = c(mean.diffs = 0.1))</span></span>
<span id="cb23-616"><a href="#cb23-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-617"><a href="#cb23-617" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_raw_btab &lt;- bal.tab(nsw_formula, </span></span>
<span id="cb23-618"><a href="#cb23-618" aria-hidden="true" tabindex="-1"></a><span class="in">                        data = nsw_data, </span></span>
<span id="cb23-619"><a href="#cb23-619" aria-hidden="true" tabindex="-1"></a><span class="in">                        stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb23-620"><a href="#cb23-620" aria-hidden="true" tabindex="-1"></a><span class="in">                        binary = "std", continuous = "std",</span></span>
<span id="cb23-621"><a href="#cb23-621" aria-hidden="true" tabindex="-1"></a><span class="in">                        thresholds = c(mean.diffs = 0.1),</span></span>
<span id="cb23-622"><a href="#cb23-622" aria-hidden="true" tabindex="-1"></a><span class="in">                        s.d.denom = "treated")</span></span>
<span id="cb23-623"><a href="#cb23-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-624"><a href="#cb23-624" aria-hidden="true" tabindex="-1"></a><span class="in"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb23-625"><a href="#cb23-625" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_btab &lt;- nsw_boosted_btab$Balance[-1,-c(2,3)]</span></span>
<span id="cb23-626"><a href="#cb23-626" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_raw_btab &lt;- nsw_raw_btab$Balance[-c(5,6)]</span></span>
<span id="cb23-627"><a href="#cb23-627" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-628"><a href="#cb23-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-629"><a href="#cb23-629" aria-hidden="true" tabindex="-1"></a>::: {#tbl-combined-btab}</span>
<span id="cb23-632"><a href="#cb23-632" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-633"><a href="#cb23-633" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-combined-btab</span></span>
<span id="cb23-634"><a href="#cb23-634" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-635"><a href="#cb23-635" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-636"><a href="#cb23-636" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-637"><a href="#cb23-637" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code"</span></span>
<span id="cb23-638"><a href="#cb23-638" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Placeholder"</span></span>
<span id="cb23-639"><a href="#cb23-639" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb23-640"><a href="#cb23-640" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb23-641"><a href="#cb23-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-642"><a href="#cb23-642" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb23-643"><a href="#cb23-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-644"><a href="#cb23-644" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb23-645"><a href="#cb23-645" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb23-646"><a href="#cb23-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-647"><a href="#cb23-647" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb23-648"><a href="#cb23-648" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb23-649"><a href="#cb23-649" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb23-650"><a href="#cb23-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-651"><a href="#cb23-651" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb23-652"><a href="#cb23-652" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb23-653"><a href="#cb23-653" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb23-654"><a href="#cb23-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-655"><a href="#cb23-655" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb23-656"><a href="#cb23-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-657"><a href="#cb23-657" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb23-658"><a href="#cb23-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-659"><a href="#cb23-659" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb23-660"><a href="#cb23-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-661"><a href="#cb23-661" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb23-662"><a href="#cb23-662" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb23-663"><a href="#cb23-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-664"><a href="#cb23-664" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T,<span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb23-665"><a href="#cb23-665" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-666"><a href="#cb23-666" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb23-667"><a href="#cb23-667" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-668"><a href="#cb23-668" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-669"><a href="#cb23-669" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width=</span><span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-670"><a href="#cb23-670" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span>
<span id="cb23-671"><a href="#cb23-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-672"><a href="#cb23-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-673"><a href="#cb23-673" aria-hidden="true" tabindex="-1"></a>Balance Table for NSW Data</span>
<span id="cb23-674"><a href="#cb23-674" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-675"><a href="#cb23-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-676"><a href="#cb23-676" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- double check that the variables are in the righ tpalces in the table  --&gt;</span></span>
<span id="cb23-677"><a href="#cb23-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-678"><a href="#cb23-678" aria-hidden="true" tabindex="-1"></a>@tbl-combined-btab shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</span>
<span id="cb23-679"><a href="#cb23-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-680"><a href="#cb23-680" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- perhaps find the threshold for variance ratios --&gt;</span></span>
<span id="cb23-681"><a href="#cb23-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-682"><a href="#cb23-682" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 9: Results</span></span>
<span id="cb23-683"><a href="#cb23-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-684"><a href="#cb23-684" aria-hidden="true" tabindex="-1"></a>Finally, the treatment effect can be estimated using <span class="in">`lm_weightit()`</span> from the <span class="in">`WeightIt`</span> package and <span class="in">`avg_comparisons()`</span> from the <span class="in">`marginaleffects`</span> package. <span class="in">`lm_weightit()`</span> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <span class="in">`avg_comparisons()`</span> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</span>
<span id="cb23-685"><a href="#cb23-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-686"><a href="#cb23-686" aria-hidden="true" tabindex="-1"></a>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="co">[</span><span class="ot">@Naimi2017</span><span class="co">]</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <span class="in">`re78`</span> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</span>
<span id="cb23-687"><a href="#cb23-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-688"><a href="#cb23-688" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-689"><a href="#cb23-689" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: compute-ate-nsw-boosted</span></span>
<span id="cb23-690"><a href="#cb23-690" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-691"><a href="#cb23-691" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb23-692"><a href="#cb23-692" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-693"><a href="#cb23-693" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_lm &lt;- lm_weightit(re78 ~ treat * (age + educ + re75 + black + </span></span>
<span id="cb23-694"><a href="#cb23-694" aria-hidden="true" tabindex="-1"></a><span class="in">                              hisp + degree + marr), data = nsw_data, </span></span>
<span id="cb23-695"><a href="#cb23-695" aria-hidden="true" tabindex="-1"></a><span class="in">                              weights = nsw_boosted_weight$weights)</span></span>
<span id="cb23-696"><a href="#cb23-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-697"><a href="#cb23-697" aria-hidden="true" tabindex="-1"></a><span class="in">library(marginaleffects)</span></span>
<span id="cb23-698"><a href="#cb23-698" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_result &lt;- avg_comparisons(nsw_boosted_lm, variables = "treat")</span></span>
<span id="cb23-699"><a href="#cb23-699" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-700"><a href="#cb23-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-701"><a href="#cb23-701" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb23-702"><a href="#cb23-702" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span> <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-703"><a href="#cb23-703" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb23-704"><a href="#cb23-704" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights) <span class="co">#&lt;2&gt;</span></span>
<span id="cb23-705"><a href="#cb23-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-706"><a href="#cb23-706" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects) <span class="co">#&lt;3&gt;</span></span>
<span id="cb23-707"><a href="#cb23-707" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>) <span class="co">#&lt;3&gt;</span></span>
<span id="cb23-708"><a href="#cb23-708" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-709"><a href="#cb23-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-710"><a href="#cb23-710" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Uses <span class="in">`lm_weightit()`</span> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <span class="in">`*`</span> indicates multiplication in R.</span>
<span id="cb23-711"><a href="#cb23-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-712"><a href="#cb23-712" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Specifies the <span class="in">`weights`</span> from the <span class="in">`nsw_boosted_weight`</span> object created earlier by the <span class="in">`weightit()`</span> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <span class="in">`nsw_boosted_weight$weights`</span>.</span>
<span id="cb23-713"><a href="#cb23-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-714"><a href="#cb23-714" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
<span id="cb23-715"><a href="#cb23-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-716"><a href="#cb23-716" aria-hidden="true" tabindex="-1"></a>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</span>
<span id="cb23-717"><a href="#cb23-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-718"><a href="#cb23-718" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-719"><a href="#cb23-719" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw-comparisons</span></span>
<span id="cb23-720"><a href="#cb23-720" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-721"><a href="#cb23-721" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: true</span></span>
<span id="cb23-722"><a href="#cb23-722" aria-hidden="true" tabindex="-1"></a><span class="in">#| eval: false</span></span>
<span id="cb23-723"><a href="#cb23-723" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-724"><a href="#cb23-724" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb23-725"><a href="#cb23-725" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show the Code to Create the Table"</span></span>
<span id="cb23-726"><a href="#cb23-726" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_lm &lt;- lm_weightit(re78~treat*(age + educ + </span></span>
<span id="cb23-727"><a href="#cb23-727" aria-hidden="true" tabindex="-1"></a><span class="in">                             re75 + black + hisp + </span></span>
<span id="cb23-728"><a href="#cb23-728" aria-hidden="true" tabindex="-1"></a><span class="in">                             degree + marr), data = nsw_data, </span></span>
<span id="cb23-729"><a href="#cb23-729" aria-hidden="true" tabindex="-1"></a><span class="in">                             weights = nsw_logit_weight$weights)</span></span>
<span id="cb23-730"><a href="#cb23-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-731"><a href="#cb23-731" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_result &lt;- avg_comparisons(nsw_logit_lm, variables = "treat")</span></span>
<span id="cb23-732"><a href="#cb23-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-733"><a href="#cb23-733" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_comparisons_tab &lt;- rbind(extract_comparison_results(nsw_logit_result),</span></span>
<span id="cb23-734"><a href="#cb23-734" aria-hidden="true" tabindex="-1"></a><span class="in">                             extract_comparison_results(nsw_boosted_result))</span></span>
<span id="cb23-735"><a href="#cb23-735" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(nsw_comparisons_tab) &lt;- c("Logistic Regression", "GBM")</span></span>
<span id="cb23-736"><a href="#cb23-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-737"><a href="#cb23-737" aria-hidden="true" tabindex="-1"></a><span class="in">kbl(nsw_comparisons_tab, digits=2,booktabs= T, align = "c", </span></span>
<span id="cb23-738"><a href="#cb23-738" aria-hidden="true" tabindex="-1"></a><span class="in">      font_size=10) %&gt;%</span></span>
<span id="cb23-739"><a href="#cb23-739" aria-hidden="true" tabindex="-1"></a><span class="in">  kable_styling(full_width = T)</span></span>
<span id="cb23-740"><a href="#cb23-740" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-741"><a href="#cb23-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-742"><a href="#cb23-742" aria-hidden="true" tabindex="-1"></a>::: {#tbl-nsw-comparisons}</span>
<span id="cb23-743"><a href="#cb23-743" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-744"><a href="#cb23-744" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-745"><a href="#cb23-745" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb23-746"><a href="#cb23-746" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-747"><a href="#cb23-747" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_lm &lt;- lm_weightit(re78~treat*(age + educ + </span></span>
<span id="cb23-748"><a href="#cb23-748" aria-hidden="true" tabindex="-1"></a><span class="in">                             re75 + black + hisp + </span></span>
<span id="cb23-749"><a href="#cb23-749" aria-hidden="true" tabindex="-1"></a><span class="in">                             degree + marr), data = nsw_data, </span></span>
<span id="cb23-750"><a href="#cb23-750" aria-hidden="true" tabindex="-1"></a><span class="in">                             weights = nsw_logit_weight$weights)</span></span>
<span id="cb23-751"><a href="#cb23-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-752"><a href="#cb23-752" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_result &lt;- avg_comparisons(nsw_logit_lm, variables = "treat")</span></span>
<span id="cb23-753"><a href="#cb23-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-754"><a href="#cb23-754" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_comparisons_tab &lt;- rbind(extract_comparison_results(nsw_logit_result),</span></span>
<span id="cb23-755"><a href="#cb23-755" aria-hidden="true" tabindex="-1"></a><span class="in">                             extract_comparison_results(nsw_boosted_result))</span></span>
<span id="cb23-756"><a href="#cb23-756" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(nsw_comparisons_tab) &lt;- c("Logistic Regression", "Generalized Boosting Machine ")</span></span>
<span id="cb23-757"><a href="#cb23-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-758"><a href="#cb23-758" aria-hidden="true" tabindex="-1"></a><span class="in">kbl(nsw_comparisons_tab, digits=2,booktabs= T, align = "c", </span></span>
<span id="cb23-759"><a href="#cb23-759" aria-hidden="true" tabindex="-1"></a><span class="in">      font_size=10) %&gt;%</span></span>
<span id="cb23-760"><a href="#cb23-760" aria-hidden="true" tabindex="-1"></a><span class="in">  kable_styling(full_width = T)</span></span>
<span id="cb23-761"><a href="#cb23-761" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-762"><a href="#cb23-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-763"><a href="#cb23-763" aria-hidden="true" tabindex="-1"></a>Comparison of ATE Estimates</span>
<span id="cb23-764"><a href="#cb23-764" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-765"><a href="#cb23-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-766"><a href="#cb23-766" aria-hidden="true" tabindex="-1"></a>@tbl-nsw-comparisons shows that both estimates of the treatment effect are nearly identical at $\$1610$ with logistic regression inferring a $\$0.86$ larger treatment effect. Additionally, these results are statistically significant at the $5\%$ level with nearly identical standard errors.</span>
<span id="cb23-767"><a href="#cb23-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-768"><a href="#cb23-768" aria-hidden="true" tabindex="-1"></a><span class="fu">## Replication Study (Don't read this. needs an honest days work)</span></span>
<span id="cb23-769"><a href="#cb23-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-770"><a href="#cb23-770" aria-hidden="true" tabindex="-1"></a>@coffecite aims to estimate the impact of the certification of coffee cooperatives on small-scale Ethiopian farmers' livelihoods. Certification is seen as a potential tool for socioeconomic change and environmental sustainability and so it is important to understand the impact on small-scale farmers. Propensity scores are used to balance covariates between certified and non-certified farmers, isolating the certification's effect on income.The paper did not assess the balance of propensity scores and it is difficult to replicate the results in the paper using best practice. However, this provides a good opportunity to assess covariate balance in the initial paper and the repeat the analysis using a machine learning propensity model.</span>
<span id="cb23-771"><a href="#cb23-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-772"><a href="#cb23-772" aria-hidden="true" tabindex="-1"></a><span class="fu">### Replication of Original Results</span></span>
<span id="cb23-773"><a href="#cb23-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-774"><a href="#cb23-774" aria-hidden="true" tabindex="-1"></a>@coffeecite provides a replication package including Stata code that uses the psmatch2 function. Nearest neighbour matching with replacement and common support trimming is performed. Common support trimming means that any observations outside the commonly overlapping are are discarded. The results of the paper can be fully replicated using the <span class="in">`MatchIt`</span> package inside R.</span>
<span id="cb23-775"><a href="#cb23-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-776"><a href="#cb23-776" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-777"><a href="#cb23-777" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-778"><a href="#cb23-778" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-779"><a href="#cb23-779" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb23-780"><a href="#cb23-780" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show Code to Replicate Results of @coffeecite"</span></span>
<span id="cb23-781"><a href="#cb23-781" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: coffee_rep_code</span></span>
<span id="cb23-782"><a href="#cb23-782" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_formula &lt;- as.formula(certified ~ age_hh + </span></span>
<span id="cb23-783"><a href="#cb23-783" aria-hidden="true" tabindex="-1"></a><span class="in">                  agesq + nonfarmincome_access + depratio +</span></span>
<span id="cb23-784"><a href="#cb23-784" aria-hidden="true" tabindex="-1"></a><span class="in">                  logtotal_land + badweat + edu + gender + </span></span>
<span id="cb23-785"><a href="#cb23-785" aria-hidden="true" tabindex="-1"></a><span class="in">                  years_cofeproduction + access_credit)</span></span>
<span id="cb23-786"><a href="#cb23-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-787"><a href="#cb23-787" aria-hidden="true" tabindex="-1"></a><span class="in">library(MatchIt)</span></span>
<span id="cb23-788"><a href="#cb23-788" aria-hidden="true" tabindex="-1"></a><span class="in">library(marginaleffects)</span></span>
<span id="cb23-789"><a href="#cb23-789" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_rep_pmodel &lt;- matchit(coffee_formula, data=coffee_data, distance="glm", </span></span>
<span id="cb23-790"><a href="#cb23-790" aria-hidden="true" tabindex="-1"></a><span class="in">                              method="nearest", replace = T, estimand="ATT", </span></span>
<span id="cb23-791"><a href="#cb23-791" aria-hidden="true" tabindex="-1"></a><span class="in">                              discard="both") </span></span>
<span id="cb23-792"><a href="#cb23-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-793"><a href="#cb23-793" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_md &lt;- match.data(coffee_rep_pmodel)</span></span>
<span id="cb23-794"><a href="#cb23-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-795"><a href="#cb23-795" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_rep_fit&lt;- lm(percapitaincome_day_maleeq ~ certified, data = coffee_logit_md, weights=weights)</span></span>
<span id="cb23-796"><a href="#cb23-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-797"><a href="#cb23-797" aria-hidden="true" tabindex="-1"></a><span class="in">replicated_result &lt;- avg_comparisons(coffee_rep_fit, variables = "certified",</span></span>
<span id="cb23-798"><a href="#cb23-798" aria-hidden="true" tabindex="-1"></a><span class="in">                vcov = TRUE,</span></span>
<span id="cb23-799"><a href="#cb23-799" aria-hidden="true" tabindex="-1"></a><span class="in">                newdata = subset(coffee_logit_md, certified == 1),</span></span>
<span id="cb23-800"><a href="#cb23-800" aria-hidden="true" tabindex="-1"></a><span class="in">                wts = "weights")</span></span>
<span id="cb23-801"><a href="#cb23-801" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-802"><a href="#cb23-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-803"><a href="#cb23-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-806"><a href="#cb23-806" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-807"><a href="#cb23-807" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-808"><a href="#cb23-808" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-replicated-result</span></span>
<span id="cb23-809"><a href="#cb23-809" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-810"><a href="#cb23-810" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-811"><a href="#cb23-811" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show to Code to Make [Figure #](#tbl-replicated-result)"</span></span>
<span id="cb23-812"><a href="#cb23-812" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Placeholder"</span></span>
<span id="cb23-813"><a href="#cb23-813" aria-hidden="true" tabindex="-1"></a>replicated_result_tbl <span class="ot">&lt;-</span> <span class="fu">extract_comparison_results</span>(replicated_result)</span>
<span id="cb23-814"><a href="#cb23-814" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(replicated_result_tbl) <span class="ot">&lt;-</span> <span class="st">"Replicated Result"</span></span>
<span id="cb23-815"><a href="#cb23-815" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(replicated_result_tbl, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb23-816"><a href="#cb23-816" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-817"><a href="#cb23-817" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span>
<span id="cb23-818"><a href="#cb23-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-819"><a href="#cb23-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-820"><a href="#cb23-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-821"><a href="#cb23-821" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb23-822"><a href="#cb23-822" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;!-- get the signifiance to match maybe another go </span></span>
<span id="cb23-823"><a href="#cb23-823" aria-hidden="true" tabindex="-1"></a><span class="in">doing mulitiple outcomes?--&gt;</span></span>
<span id="cb23-824"><a href="#cb23-824" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-825"><a href="#cb23-825" aria-hidden="true" tabindex="-1"></a>@tbl-replicated-result shows the replicated result obtained by @coffeecite. The intriguing finding of the paper is that the average treatment effect on the treated (ATT) of being certified on income is negative. That is, if a farmer becomes certified, this is predicted to decrease by $\$0.15$ per day. Intuition and proponents of certifications schemes suggest that certification leads to an increase of income. If certification negatively impacted well-being in this way, it would call into question a significant effort to engage in certification and fair trade practices.</span>
<span id="cb23-826"><a href="#cb23-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-827"><a href="#cb23-827" aria-hidden="true" tabindex="-1"></a>@coffeecite does not perform any discussion or consideration of balance in their paper and so it is not clear if their propensity score matching process has resulted in balanced covariates. A balance table created by the <span class="in">`cobalt`</span> package will provide the required information for balance assessment which will be aided by a graphical visualisation using <span class="in">`love.plot()`</span>.</span>
<span id="cb23-828"><a href="#cb23-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-831"><a href="#cb23-831" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-832"><a href="#cb23-832" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-833"><a href="#cb23-833" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-834"><a href="#cb23-834" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-coffee-rep-kbl</span></span>
<span id="cb23-835"><a href="#cb23-835" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-836"><a href="#cb23-836" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Table #](#tbl-coffee-rep-kbl)"</span></span>
<span id="cb23-837"><a href="#cb23-837" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "PALCHOLDER"</span></span>
<span id="cb23-838"><a href="#cb23-838" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb23-839"><a href="#cb23-839" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_rep_pmodel, </span>
<span id="cb23-840"><a href="#cb23-840" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb23-841"><a href="#cb23-841" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb23-842"><a href="#cb23-842" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb23-843"><a href="#cb23-843" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb23-844"><a href="#cb23-844" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb23-845"><a href="#cb23-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-846"><a href="#cb23-846" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab_ss <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Observations</span>
<span id="cb23-847"><a href="#cb23-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-848"><a href="#cb23-848" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb23-849"><a href="#cb23-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-850"><a href="#cb23-850" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb23-851"><a href="#cb23-851" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Household Age"</span>, <span class="st">"Squared Household Age"</span>, <span class="st">"Non-farm Income Access"</span>, </span>
<span id="cb23-852"><a href="#cb23-852" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Log Total Land"</span>, <span class="st">"Dependency Ratio"</span>, <span class="st">"Bad Weather"</span>,</span>
<span id="cb23-853"><a href="#cb23-853" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Education Level"</span>, <span class="st">"Gender"</span>, <span class="st">"Years of Coffee Production"</span>, </span>
<span id="cb23-854"><a href="#cb23-854" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Access to Credit"</span>)</span>
<span id="cb23-855"><a href="#cb23-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-856"><a href="#cb23-856" aria-hidden="true" tabindex="-1"></a>colnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Variable"</span>,<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balance Threshold"</span>, <span class="st">"Variance Ratio"</span>)</span>
<span id="cb23-857"><a href="#cb23-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-858"><a href="#cb23-858" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_rep_btab) <span class="ot">&lt;-</span> rowlabels</span>
<span id="cb23-859"><a href="#cb23-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-860"><a href="#cb23-860" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb23-861"><a href="#cb23-861" aria-hidden="true" tabindex="-1"></a>          coffee_rep_btab[,<span class="dv">3</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb23-862"><a href="#cb23-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-863"><a href="#cb23-863" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb23-864"><a href="#cb23-864" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb23-865"><a href="#cb23-865" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>)</span>
<span id="cb23-866"><a href="#cb23-866" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-867"><a href="#cb23-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-870"><a href="#cb23-870" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-871"><a href="#cb23-871" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb23-872"><a href="#cb23-872" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-873"><a href="#cb23-873" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: getndropped</span></span>
<span id="cb23-874"><a href="#cb23-874" aria-hidden="true" tabindex="-1"></a>nobs_coffee_dropped <span class="ot">&lt;-</span> <span class="fu">sum</span>(coffee_rep_pmodel<span class="sc">$</span>discarded, <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span>
<span id="cb23-875"><a href="#cb23-875" aria-hidden="true" tabindex="-1"></a>coffee_data<span class="sc">$</span>discarded <span class="ot">&lt;-</span> coffee_rep_pmodel<span class="sc">$</span>discarded</span>
<span id="cb23-876"><a href="#cb23-876" aria-hidden="true" tabindex="-1"></a>nobs_coffee_Tdropped <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(coffee_data,discarded<span class="sc">==</span><span class="cn">TRUE</span><span class="sc">&amp;</span>certified<span class="sc">==</span><span class="dv">1</span>))</span>
<span id="cb23-877"><a href="#cb23-877" aria-hidden="true" tabindex="-1"></a>nobs_coffee_Cdropped <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(coffee_data,discarded<span class="sc">==</span><span class="cn">TRUE</span><span class="sc">&amp;</span>certified<span class="sc">==</span><span class="dv">0</span>))</span>
<span id="cb23-878"><a href="#cb23-878" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-879"><a href="#cb23-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-882"><a href="#cb23-882" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-883"><a href="#cb23-883" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-coffee-replication-lplot</span></span>
<span id="cb23-884"><a href="#cb23-884" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-885"><a href="#cb23-885" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-886"><a href="#cb23-886" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-887"><a href="#cb23-887" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#fig-coffee-replication-lplot)"</span></span>
<span id="cb23-888"><a href="#cb23-888" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-889"><a href="#cb23-889" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "PLACEHOLDER"</span></span>
<span id="cb23-890"><a href="#cb23-890" aria-hidden="true" tabindex="-1"></a><span class="co"># add render info for showtext to yaml. also chang legend to be more informative.  </span></span>
<span id="cb23-891"><a href="#cb23-891" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb23-892"><a href="#cb23-892" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb23-893"><a href="#cb23-893" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb23-894"><a href="#cb23-894" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel),</span>
<span id="cb23-895"><a href="#cb23-895" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,</span>
<span id="cb23-896"><a href="#cb23-896" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb23-897"><a href="#cb23-897" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>),</span>
<span id="cb23-898"><a href="#cb23-898" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>, <span class="at">thresholds=</span><span class="fl">0.1</span>, <span class="at">s.d.denom=</span><span class="st">"treated"</span>) <span class="sc">+</span></span>
<span id="cb23-899"><a href="#cb23-899" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance"</span>,</span>
<span id="cb23-900"><a href="#cb23-900" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb23-901"><a href="#cb23-901" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb23-902"><a href="#cb23-902" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb23-903"><a href="#cb23-903" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>)))</span>
<span id="cb23-904"><a href="#cb23-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-905"><a href="#cb23-905" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-906"><a href="#cb23-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-907"><a href="#cb23-907" aria-hidden="true" tabindex="-1"></a>@tbl-coffee-rep-kbl and @fig-coffee-replication-lplot show that the propensity score matching process has obtained very poor balance. Based on the 10% rule, not a single variable is balanced and so the estimate of the treatment effect is likely to be biased by the structural differences in the groups.</span>
<span id="cb23-908"><a href="#cb23-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-909"><a href="#cb23-909" aria-hidden="true" tabindex="-1"></a>For key variables such as Age, Gender, or Education, balance is especially important. On a theoretical level, we expect that people who are more educated are more likely to become certified as they are better able to engage with the application process and also are expected to earn more as increased education should lead to greater productivity. There likely exists gender discrimination given the time period and geographic area which suggests woman are less likely to be certified than men while also earning less due to a wide gender pay gap. These variables are strong confounders in theory and so emphasising balance in these variables is critical to making a robust causal inference.</span>
<span id="cb23-910"><a href="#cb23-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-911"><a href="#cb23-911" aria-hidden="true" tabindex="-1"></a>@fig-replication-pscore shows trimmed regions that mostly impact the control group. @tbl-coffee-replication-ss shows <span class="in">`{r} nobs_coffee_dropped`</span> observations are dropped of which <span class="in">`{r} nobs_coffee_Tdropped`</span> are treated and <span class="in">`{r} nobs_coffee_Cdropped`</span> are control being dropped. This increases balance as the trimmed observations are extreme data points. When observations are discarded, the ATT, ATC, or ATE cannot be claimed. Instead, this is refereed to as the average treatment effect on the matched or ATM. There is a significant reduction in the effective sample size as due to dropped obervations in the control group which has an effective sample size of $21$ obervations down from $82$.</span>
<span id="cb23-912"><a href="#cb23-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-915"><a href="#cb23-915" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-916"><a href="#cb23-916" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-917"><a href="#cb23-917" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-918"><a href="#cb23-918" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb23-919"><a href="#cb23-919" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-replication-pscore</span></span>
<span id="cb23-920"><a href="#cb23-920" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-921"><a href="#cb23-921" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-922"><a href="#cb23-922" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#fig-replication-pscore)"</span></span>
<span id="cb23-923"><a href="#cb23-923" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "PLACEHOLDER"</span></span>
<span id="cb23-924"><a href="#cb23-924" aria-hidden="true" tabindex="-1"></a>discarded_scores <span class="ot">&lt;-</span> coffee_rep_pmodel<span class="sc">$</span>distance[coffee_rep_pmodel<span class="sc">$</span>discarded]</span>
<span id="cb23-925"><a href="#cb23-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-926"><a href="#cb23-926" aria-hidden="true" tabindex="-1"></a>discard_min <span class="ot">&lt;-</span> <span class="fu">min</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-927"><a href="#cb23-927" aria-hidden="true" tabindex="-1"></a>discard_max <span class="ot">&lt;-</span> <span class="fu">max</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-928"><a href="#cb23-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-929"><a href="#cb23-929" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(coffee_data, <span class="fu">aes</span>(<span class="at">x =</span> coffee_rep_pmodel<span class="sc">$</span>distance, <span class="at">fill =</span> <span class="fu">factor</span>(certified))) <span class="sc">+</span></span>
<span id="cb23-930"><a href="#cb23-930" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb23-931"><a href="#cb23-931" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb23-932"><a href="#cb23-932" aria-hidden="true" tabindex="-1"></a>                    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Certified"</span>)) <span class="sc">+</span></span>
<span id="cb23-933"><a href="#cb23-933" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Distribution of Propensity Scores in @coffeecite"</span>, </span>
<span id="cb23-934"><a href="#cb23-934" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb23-935"><a href="#cb23-935" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb23-936"><a href="#cb23-936" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb23-937"><a href="#cb23-937" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_min, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb23-938"><a href="#cb23-938" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_max, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb23-939"><a href="#cb23-939" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> <span class="dv">0</span>, <span class="at">xmax =</span> discard_min, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb23-940"><a href="#cb23-940" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> discard_max, <span class="at">xmax =</span> <span class="dv">1</span>, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb23-941"><a href="#cb23-941" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.02</span>, <span class="at">y =</span> <span class="fl">2.5</span>, </span>
<span id="cb23-942"><a href="#cb23-942" aria-hidden="true" tabindex="-1"></a>           <span class="at">label =</span> <span class="st">"Discarded Range"</span>, <span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">1.5</span>, <span class="at">size=</span><span class="dv">4</span>,<span class="at">fontface=</span><span class="st">"bold"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>) <span class="sc">+</span></span>
<span id="cb23-943"><a href="#cb23-943" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme</span>
<span id="cb23-944"><a href="#cb23-944" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-945"><a href="#cb23-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-948"><a href="#cb23-948" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-949"><a href="#cb23-949" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-coffee-replication-ss</span></span>
<span id="cb23-950"><a href="#cb23-950" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-951"><a href="#cb23-951" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-952"><a href="#cb23-952" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-953"><a href="#cb23-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-954"><a href="#cb23-954" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Table #](#tbl-coffee-replication-ss)"</span></span>
<span id="cb23-955"><a href="#cb23-955" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Placeholder"</span></span>
<span id="cb23-956"><a href="#cb23-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-957"><a href="#cb23-957" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab_ss, <span class="at">digits=</span><span class="dv">0</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb23-958"><a href="#cb23-958" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-959"><a href="#cb23-959" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span>F)</span>
<span id="cb23-960"><a href="#cb23-960" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-961"><a href="#cb23-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-962"><a href="#cb23-962" aria-hidden="true" tabindex="-1"></a>Overall, this model fit using logistic regression and propensity score matching has resulted in a poor model due to covariate imbalance and unidentifiable estimands. It is likely that improvement can be made using</span>
<span id="cb23-963"><a href="#cb23-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-964"><a href="#cb23-964" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Modelling</span></span>
<span id="cb23-965"><a href="#cb23-965" aria-hidden="true" tabindex="-1"></a>In the following model fitting process, I aim to obtain better results while preserving the estimand. To improve the poor balance achieved by the paper there are two strategies to obtain better balance. First, the propensity scores can be re-estimated using machine learning leading to better calibrated propensity scores. Second, inverse propensity weighting (IPW) can be used instead of propensity score matching (PSM). IPW should ensure that that the sample size remains the same as no observations are lost in a matching process. Additionally, IPW is generally more efficient as the pseudo-population is based on prescise weights compared to matching that is based on approximate similarity between treatment and control. </span>
<span id="cb23-966"><a href="#cb23-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-967"><a href="#cb23-967" aria-hidden="true" tabindex="-1"></a>The machine learning propensity scores will be estimated using <span class="in">`WeightIt`</span> in the same process as @sec-demo. To select the criteria that defines the best model, consider @fig-coffee-replication-lplot that shows there is a significant range of balance levels in the raw dataset. Knowing this, the model will be tuned using <span class="in">`criterion = "smd.max"`</span> as reducing the extremely unbalanced variables is important to achieving balance even if this leads to a higher average standardised mean difference (SMD). Additionally, the model fitting process was completed using <span class="in">`criterion = "smd.max"`</span> and is shown in @sec-appendix.  </span>
<span id="cb23-970"><a href="#cb23-970" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-971"><a href="#cb23-971" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: coffee-boost-btab</span></span>
<span id="cb23-972"><a href="#cb23-972" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-973"><a href="#cb23-973" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-974"><a href="#cb23-974" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-975"><a href="#cb23-975" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show to Code to Fit the GBM model using `WeightIt` and `cobalt`"</span></span>
<span id="cb23-976"><a href="#cb23-976" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb23-977"><a href="#cb23-977" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb23-978"><a href="#cb23-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-979"><a href="#cb23-979" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb23-980"><a href="#cb23-980" aria-hidden="true" tabindex="-1"></a>coffee_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data=</span>coffee_data, </span>
<span id="cb23-981"><a href="#cb23-981" aria-hidden="true" tabindex="-1"></a>                                <span class="at">method=</span><span class="st">"gbm"</span>, <span class="at">distribution=</span><span class="st">"bernoulli"</span>,</span>
<span id="cb23-982"><a href="#cb23-982" aria-hidden="true" tabindex="-1"></a>                                <span class="at">use.offset=</span><span class="fu">c</span>(T),</span>
<span id="cb23-983"><a href="#cb23-983" aria-hidden="true" tabindex="-1"></a>                                <span class="at">shrinkage=</span><span class="fu">seq</span>(<span class="fl">0.15</span>, <span class="fl">0.4</span>,<span class="at">length.out=</span><span class="dv">5</span>),</span>
<span id="cb23-984"><a href="#cb23-984" aria-hidden="true" tabindex="-1"></a>                                <span class="at">bag.fraction=</span><span class="fl">0.67</span>, </span>
<span id="cb23-985"><a href="#cb23-985" aria-hidden="true" tabindex="-1"></a>                                <span class="at">interaction.depth=</span><span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb23-986"><a href="#cb23-986" aria-hidden="true" tabindex="-1"></a>                                <span class="at">n.trees=</span><span class="dv">500</span>,</span>
<span id="cb23-987"><a href="#cb23-987" aria-hidden="true" tabindex="-1"></a>                                <span class="at">criterion=</span><span class="st">"smd.mean"</span>, </span>
<span id="cb23-988"><a href="#cb23-988" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand=</span><span class="st">"ATT"</span>)</span>
<span id="cb23-989"><a href="#cb23-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-990"><a href="#cb23-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-991"><a href="#cb23-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-992"><a href="#cb23-992" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_boosted_weight, </span>
<span id="cb23-993"><a href="#cb23-993" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb23-994"><a href="#cb23-994" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb23-995"><a href="#cb23-995" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb23-996"><a href="#cb23-996" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb23-997"><a href="#cb23-997" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb23-998"><a href="#cb23-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-999"><a href="#cb23-999" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance tabltune# Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb23-1000"><a href="#cb23-1000" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> coffee_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb23-1001"><a href="#cb23-1001" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1002"><a href="#cb23-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1003"><a href="#cb23-1003" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Discussion of Tuning"}</span>
<span id="cb23-1004"><a href="#cb23-1004" aria-hidden="true" tabindex="-1"></a>Initially, a tuning grid considering shrinkage values of $0.001,0.005,.01,0.05,0.1,\text{ and }0.2$ were considered using $10000$ trees with a depth between $1$ and $5$. The best tuning performance was found with shrinkage of $0.2$ and $9$ trees which were three splits $3$ deep. As such, the tuning grid was redefined in a second iteration to use $0.1, 0.15, 0.2, 0.25, 0.3,0.35,\text{ and } 0.4$ with only $1000$ trees with between $2$ and $5$ depth. The second fit, suggested a learning rate of $0.35$ so the local area of $0.3, 0.325, 0.350, 0.375, \text{ and }0.4$ is searched in the final fit.</span>
<span id="cb23-1005"><a href="#cb23-1005" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-1006"><a href="#cb23-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1007"><a href="#cb23-1007" aria-hidden="true" tabindex="-1"></a>Of course there is no guarantee that the GBM model will perform the best and so a logisic model is also fitted. An interesting comparison is between the balance visible in the matched sample like in @coffeecite and in the weighted sample. Any difference between these two samples relates to the difference between matching and weighting on the propensity score as the score is the same in both methods (generated by logistic regression). </span>
<span id="cb23-1008"><a href="#cb23-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1009"><a href="#cb23-1009" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb23-1010"><a href="#cb23-1010" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb23-1011"><a href="#cb23-1011" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb23-1012"><a href="#cb23-1012" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: coffee_logit_weight</span></span>
<span id="cb23-1013"><a href="#cb23-1013" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb23-1014"><a href="#cb23-1014" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show to Code to Fit the Logistic model using `WeightIt` and `cobalt`"</span></span>
<span id="cb23-1015"><a href="#cb23-1015" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_weight &lt;- weightit(coffee_formula, data=coffee_data, method="glm",</span></span>
<span id="cb23-1016"><a href="#cb23-1016" aria-hidden="true" tabindex="-1"></a><span class="in">                                estimand="ATT")</span></span>
<span id="cb23-1017"><a href="#cb23-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1018"><a href="#cb23-1018" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_btab &lt;- bal.tab(coffee_logit_weight, </span></span>
<span id="cb23-1019"><a href="#cb23-1019" aria-hidden="true" tabindex="-1"></a><span class="in">                             formula = coffee_formula,</span></span>
<span id="cb23-1020"><a href="#cb23-1020" aria-hidden="true" tabindex="-1"></a><span class="in">                             data = coffee_data, </span></span>
<span id="cb23-1021"><a href="#cb23-1021" aria-hidden="true" tabindex="-1"></a><span class="in">                             stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb23-1022"><a href="#cb23-1022" aria-hidden="true" tabindex="-1"></a><span class="in">                             binary = "std", continuous = "std",</span></span>
<span id="cb23-1023"><a href="#cb23-1023" aria-hidden="true" tabindex="-1"></a><span class="in">                             thresholds = c(mean.diffs = 0.1),</span></span>
<span id="cb23-1024"><a href="#cb23-1024" aria-hidden="true" tabindex="-1"></a><span class="in">                             s.d.denom = "treated")</span></span>
<span id="cb23-1025"><a href="#cb23-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1026"><a href="#cb23-1026" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_btab &lt;- coffee_logit_btab$Balance[-1,-c(2,3)]</span></span>
<span id="cb23-1027"><a href="#cb23-1027" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1028"><a href="#cb23-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1029"><a href="#cb23-1029" aria-hidden="true" tabindex="-1"></a>Additionally, the balance present in the raw data is calculated.  </span>
<span id="cb23-1030"><a href="#cb23-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1031"><a href="#cb23-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1034"><a href="#cb23-1034" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-1035"><a href="#cb23-1035" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: raw-btab</span></span>
<span id="cb23-1036"><a href="#cb23-1036" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-1037"><a href="#cb23-1037" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_formula, </span>
<span id="cb23-1038"><a href="#cb23-1038" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb23-1039"><a href="#cb23-1039" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb23-1040"><a href="#cb23-1040" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb23-1041"><a href="#cb23-1041" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb23-1042"><a href="#cb23-1042" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb23-1043"><a href="#cb23-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1044"><a href="#cb23-1044" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> coffee_raw_btab<span class="sc">$</span>Balance[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span>
<span id="cb23-1045"><a href="#cb23-1045" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1046"><a href="#cb23-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1047"><a href="#cb23-1047" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparison of Methods</span></span>
<span id="cb23-1048"><a href="#cb23-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1049"><a href="#cb23-1049" aria-hidden="true" tabindex="-1"></a>In each of the above code chunks, the <span class="in">`cobalt`</span> table has computed balance tables which are combined together in @tbl-coffee-comparison and @fig-coffee-love-all. </span>
<span id="cb23-1050"><a href="#cb23-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1053"><a href="#cb23-1053" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-1054"><a href="#cb23-1054" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: manipulate-btab</span></span>
<span id="cb23-1055"><a href="#cb23-1055" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-1056"><a href="#cb23-1056" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-1057"><a href="#cb23-1057" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-1058"><a href="#cb23-1058" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code Prepairing the Balance Table for Presentation"</span></span>
<span id="cb23-1059"><a href="#cb23-1059" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"data.table"</span>)</span>
<span id="cb23-1060"><a href="#cb23-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1061"><a href="#cb23-1061" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> <span class="fu">rbindlist</span>(<span class="fu">list</span>(coffee_raw_btab,</span>
<span id="cb23-1062"><a href="#cb23-1062" aria-hidden="true" tabindex="-1"></a>                                       coffee_logit_btab,</span>
<span id="cb23-1063"><a href="#cb23-1063" aria-hidden="true" tabindex="-1"></a>                                       coffee_boosted_btab), <span class="at">use.names=</span><span class="cn">FALSE</span>)</span>
<span id="cb23-1064"><a href="#cb23-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1065"><a href="#cb23-1065" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb23-1066"><a href="#cb23-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1067"><a href="#cb23-1067" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> coffee_combined_btab[,<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)]</span>
<span id="cb23-1068"><a href="#cb23-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1069"><a href="#cb23-1069" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb23-1070"><a href="#cb23-1070" aria-hidden="true" tabindex="-1"></a>          coffee_combined_btab[,<span class="dv">4</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb23-1071"><a href="#cb23-1071" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1072"><a href="#cb23-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1073"><a href="#cb23-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1076"><a href="#cb23-1076" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-1077"><a href="#cb23-1077" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: true</span></span>
<span id="cb23-1078"><a href="#cb23-1078" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb23-1079"><a href="#cb23-1079" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-1080"><a href="#cb23-1080" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Table #](#tbl-coffee-comparison)."</span></span>
<span id="cb23-1081"><a href="#cb23-1081" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-coffee-comparison</span></span>
<span id="cb23-1082"><a href="#cb23-1082" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Comparison of Balance for Coffee Data Using Different Propensity Models"</span></span>
<span id="cb23-1083"><a href="#cb23-1083" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb23-1084"><a href="#cb23-1084" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_combined_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb23-1085"><a href="#cb23-1085" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb23-1086"><a href="#cb23-1086" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-1087"><a href="#cb23-1087" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold=</span><span class="cn">TRUE</span>, <span class="at">width=</span><span class="st">"5cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-1088"><a href="#cb23-1088" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold=</span><span class="cn">FALSE</span>, <span class="at">width=</span><span class="st">"1cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-1089"><a href="#cb23-1089" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Raw Data"</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-1090"><a href="#cb23-1090" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Logistic Regression and IPTW"</span>, <span class="dv">11</span>, <span class="dv">20</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-1091"><a href="#cb23-1091" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Boosted Machine with IPTW"</span>, <span class="dv">21</span>, <span class="dv">30</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>)</span>
<span id="cb23-1092"><a href="#cb23-1092" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1093"><a href="#cb23-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1094"><a href="#cb23-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1097"><a href="#cb23-1097" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-1098"><a href="#cb23-1098" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-coffee-love-all</span></span>
<span id="cb23-1099"><a href="#cb23-1099" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-1100"><a href="#cb23-1100" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb23-1101"><a href="#cb23-1101" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-1102"><a href="#cb23-1102" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#fig-coffee-love-all)"</span></span>
<span id="cb23-1103"><a href="#cb23-1103" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of Balance for Coffee Data Using Different Methods"</span></span>
<span id="cb23-1104"><a href="#cb23-1104" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb23-1105"><a href="#cb23-1105" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb23-1106"><a href="#cb23-1106" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel,</span>
<span id="cb23-1107"><a href="#cb23-1107" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Logit =</span> coffee_logit_weight,</span>
<span id="cb23-1108"><a href="#cb23-1108" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Boosting=</span> coffee_boosted_weight),</span>
<span id="cb23-1109"><a href="#cb23-1109" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,<span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb23-1110"><a href="#cb23-1110" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>, <span class="st">"darkblue"</span>,<span class="st">"darkred"</span>), </span>
<span id="cb23-1111"><a href="#cb23-1111" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>, <span class="st">"triangle"</span>, <span class="st">"diamond"</span>),</span>
<span id="cb23-1112"><a href="#cb23-1112" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>,<span class="at">thresholds=</span><span class="fl">0.1</span>,<span class="at">s.d.denom=</span><span class="st">"treated"</span>,<span class="at">use.grid=</span>F)<span class="sc">+</span></span>
<span id="cb23-1113"><a href="#cb23-1113" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance Using Different Balance Methods"</span>,</span>
<span id="cb23-1114"><a href="#cb23-1114" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb23-1115"><a href="#cb23-1115" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb23-1116"><a href="#cb23-1116" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="sc">+</span> custom_ggplot_theme</span>
<span id="cb23-1117"><a href="#cb23-1117" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1118"><a href="#cb23-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1119"><a href="#cb23-1119" aria-hidden="true" tabindex="-1"></a>Viewing results of our balance shows three notable findings: </span>
<span id="cb23-1120"><a href="#cb23-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1121"><a href="#cb23-1121" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>PSM has performed very poorly relative to IPW even though matching dropped a significant number of observations.  </span>
<span id="cb23-1122"><a href="#cb23-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1123"><a href="#cb23-1123" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A GBM model has resulted in better covariate balance than logistic regression for most variables. Using a $10\%$ threshold for determining balance, logistic regression leaves $5$ variables unbalanced and the GBM leaves $3$ variables unbalanced. Additionally, the margin above being unbalanced is also larger for logistic regression. Although logistic regression results in better balance for "access to credit", "gender", and "education". Logistic regress has the average balance across all variables is <span class="in">`{r} mean(coffee_logit_btab$Diff.Adj)`</span> which is satisfactory. For boosting, The average standardised mean is <span class="in">`{r} mean(coffee_boosted_btab$Diff.Adj)`</span> which is quite impressive compared to the methodology used in the paper.</span>
<span id="cb23-1124"><a href="#cb23-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1125"><a href="#cb23-1125" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The variable with the worst balance for each model is Age with a SMD of $0.245$ for logistic regression and "bad weather" with $0.191$ for the GBM.</span>
<span id="cb23-1126"><a href="#cb23-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1127"><a href="#cb23-1127" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Using a looser balance threshold of $0.2$, the GBM achieves balance across all covariates while the logistic regression does not balance either of the Age variables. </span>
<span id="cb23-1128"><a href="#cb23-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1129"><a href="#cb23-1129" aria-hidden="true" tabindex="-1"></a>For the logistic regression model, the balance statistics are marginally balanced. Using a $10\%$ threshold, half of the variables are balanced. Using a relaxed $20\%$ threshold, only Age and Age Squared are unbalanced but balance with threshold should be interpreted with caution.  </span>
<span id="cb23-1130"><a href="#cb23-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1131"><a href="#cb23-1131" aria-hidden="true" tabindex="-1"></a>Now that satisfactory covariate balance is achieved, the treatment effect can be estimated under logistic regression, the GBM, and then compared to the result in the paper. Note that the estimand in the paper is intended to be the average treatment effect (ATT) but dropped observations mean the actual treatment effect is the average treatment effect on matched (ATM) individuals. </span>
<span id="cb23-1132"><a href="#cb23-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1135"><a href="#cb23-1135" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb23-1136"><a href="#cb23-1136" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb23-1137"><a href="#cb23-1137" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb23-1138"><a href="#cb23-1138" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb23-1139"><a href="#cb23-1139" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#tbl-comparison-coffee-results)"</span></span>
<span id="cb23-1140"><a href="#cb23-1140" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-comparison-coffee-results</span></span>
<span id="cb23-1141"><a href="#cb23-1141" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "PLACEHOLDER"</span></span>
<span id="cb23-1142"><a href="#cb23-1142" aria-hidden="true" tabindex="-1"></a>coffee_att_formula <span class="ot">&lt;-</span> <span class="fu">update.formula</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, <span class="fu">paste</span>(<span class="fu">attr</span>(<span class="fu">terms</span>(coffee_formula), <span class="st">"term.labels"</span>), <span class="at">collapse =</span> <span class="st">" + "</span>))), percapitaincome_day_maleeq <span class="sc">~</span> certified <span class="sc">*</span> .)</span>
<span id="cb23-1143"><a href="#cb23-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1144"><a href="#cb23-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1145"><a href="#cb23-1145" aria-hidden="true" tabindex="-1"></a>coffee_logit_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb23-1146"><a href="#cb23-1146" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_logit_weight)</span>
<span id="cb23-1147"><a href="#cb23-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1148"><a href="#cb23-1148" aria-hidden="true" tabindex="-1"></a>coffee_boosted_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb23-1149"><a href="#cb23-1149" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_boosted_weight)</span>
<span id="cb23-1150"><a href="#cb23-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1151"><a href="#cb23-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1152"><a href="#cb23-1152" aria-hidden="true" tabindex="-1"></a>coffee_logit_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_logit_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb23-1153"><a href="#cb23-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1154"><a href="#cb23-1154" aria-hidden="true" tabindex="-1"></a>coffee_boosted_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_boosted_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb23-1155"><a href="#cb23-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1156"><a href="#cb23-1156" aria-hidden="true" tabindex="-1"></a>coffee_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(replicated_result_tbl, <span class="fu">extract_comparison_results</span>(coffee_logit_att),</span>
<span id="cb23-1157"><a href="#cb23-1157" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(coffee_boosted_att))</span>
<span id="cb23-1158"><a href="#cb23-1158" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Rep. Result (Logistic with PSM)"</span>,<span class="st">"Logistic Regression and IPW"</span>, <span class="st">"Generalized Boosting Machine and IPW"</span>)</span>
<span id="cb23-1159"><a href="#cb23-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1160"><a href="#cb23-1160" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_comparisons_tab, <span class="at">digits=</span><span class="dv">4</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb23-1161"><a href="#cb23-1161" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-1162"><a href="#cb23-1162" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span>
<span id="cb23-1163"><a href="#cb23-1163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1164"><a href="#cb23-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1165"><a href="#cb23-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1166"><a href="#cb23-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1167"><a href="#cb23-1167" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb23-1168"><a href="#cb23-1168" aria-hidden="true" tabindex="-1"></a><span class="in">save.image(file = "my_environment.RData")</span></span>
<span id="cb23-1169"><a href="#cb23-1169" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>