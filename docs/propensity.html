<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.52">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Propensity Scores with Machine Learning – My project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./metalearners.html" rel="next">
<link href="./background.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./propensity.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propensity Scores with Machine Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">My project</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction DONT READ THIS YET. ITS STILL BLAAA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background: Causal Inference and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./propensity.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propensity Scores with Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metalearners.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Meta-Learners</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Causal Trees and Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-conventional-approach-propensity-scores-and-balance" id="toc-a-conventional-approach-propensity-scores-and-balance" class="nav-link active" data-scroll-target="#a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">3.1</span> A Conventional Approach: Propensity Scores and Balance</a>
  <ul class="collapse">
  <li><a href="#propensity-score-modelling-with-logistic-regression" id="toc-propensity-score-modelling-with-logistic-regression" class="nav-link" data-scroll-target="#propensity-score-modelling-with-logistic-regression"><span class="header-section-number">3.1.1</span> Propensity Score Modelling with Logistic Regression</a></li>
  </ul></li>
  <li><a href="#probability-machines-probability-theory-and-machine-learning" id="toc-probability-machines-probability-theory-and-machine-learning" class="nav-link" data-scroll-target="#probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">3.2</span> Probability Machines: Probability Theory and Machine Learning</a>
  <ul class="collapse">
  <li><a href="#choice-of-loss-function-and-probability-prediction" id="toc-choice-of-loss-function-and-probability-prediction" class="nav-link" data-scroll-target="#choice-of-loss-function-and-probability-prediction"><span class="header-section-number">3.2.1</span> Choice of Loss Function and Probability Prediction</a></li>
  <li><a href="#sec-bagg-rf-probmachines" id="toc-sec-bagg-rf-probmachines" class="nav-link" data-scroll-target="#sec-bagg-rf-probmachines"><span class="header-section-number">3.2.2</span> Bagging and Random Forest as Probability Machines</a></li>
  <li><a href="#sec-gbm" id="toc-sec-gbm" class="nav-link" data-scroll-target="#sec-gbm"><span class="header-section-number">3.2.3</span> Gradient Boosting Machines as Probability Machines</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting"><span class="header-section-number">3.2.4</span> Overfitting</a></li>
  <li><a href="#sec-mlps-sims" id="toc-sec-mlps-sims" class="nav-link" data-scroll-target="#sec-mlps-sims"><span class="header-section-number">3.2.5</span> Comparison of Machine Learning Algorithms: Simulation Results</a></li>
  </ul></li>
  <li><a href="#implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" id="toc-implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" class="nav-link" data-scroll-target="#implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r"><span class="header-section-number">3.3</span> Implimentation and Hyperparameter Tuning with <code>WeightIt</code> and<code>gbm</code> in R</a>
  <ul class="collapse">
  <li><a href="#sec-gbm-tune-workflow" id="toc-sec-gbm-tune-workflow" class="nav-link" data-scroll-target="#sec-gbm-tune-workflow"><span class="header-section-number">3.3.1</span> Hyperparameter Tuning and Workflow</a></li>
  </ul></li>
  <li><a href="#example-nsw-jobs-dataset-using-r" id="toc-example-nsw-jobs-dataset-using-r" class="nav-link" data-scroll-target="#example-nsw-jobs-dataset-using-r"><span class="header-section-number">3.4</span> Example: NSW Jobs Dataset Using R</a>
  <ul class="collapse">
  <li><a href="#step-1-6-model-fitting-and-tuning" id="toc-step-1-6-model-fitting-and-tuning" class="nav-link" data-scroll-target="#step-1-6-model-fitting-and-tuning"><span class="header-section-number">3.4.1</span> Step 1-6: Model Fitting and Tuning</a></li>
  <li><a href="#sec-nsw-balance" id="toc-sec-nsw-balance" class="nav-link" data-scroll-target="#sec-nsw-balance"><span class="header-section-number">3.4.2</span> Step 7 and 8: Assessing Balance</a></li>
  <li><a href="#sec-nsw-results" id="toc-sec-nsw-results" class="nav-link" data-scroll-target="#sec-nsw-results"><span class="header-section-number">3.4.3</span> Step 9: Results</a></li>
  </ul></li>
  <li><a href="#replication-study" id="toc-replication-study" class="nav-link" data-scroll-target="#replication-study"><span class="header-section-number">3.5</span> Replication Study</a>
  <ul class="collapse">
  <li><a href="#replication-of-original-results" id="toc-replication-of-original-results" class="nav-link" data-scroll-target="#replication-of-original-results"><span class="header-section-number">3.5.1</span> Replication of Original Results</a></li>
  <li><a href="#further-modelling" id="toc-further-modelling" class="nav-link" data-scroll-target="#further-modelling"><span class="header-section-number">3.5.2</span> Further Modelling</a></li>
  <li><a href="#comparison-of-methods" id="toc-comparison-of-methods" class="nav-link" data-scroll-target="#comparison-of-methods"><span class="header-section-number">3.5.3</span> Comparison of Methods</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3.5.4</span> Results</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-propensity" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propensity Scores with Machine Learning</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- To-do:   -->
<!-- - Finish coffee data example -->
<!-- - rework the loss function theory stuff -->
<!-- - change code to quarto formatting -->
<!-- - perhaps remove the nsw example here -->
<!-- - change the tutorial code to be the replication study -->
<!-- - exposed vs treated (using treatment here). -->
<!-- make quarto and ggplot themes consistent. also changing colors of note callouts.  -->
<!-- make sure the link here says chapter not section -->
<!-- - better intro that explains a probability machine.  -->
<!-- - round off the structure and ensure header labels are consistent.  -->
<!-- - ml background to be transferred/written up to the background chapter. cite in this. -->
<!-- - state generally about how classification is binary and give examples of how ml is used for this. then transition it all a lot better.  -->
<!-- - more organised comparison of simulation results -->
<!-- - clarify the gini splitting vs accuracy loss function for rf/bag -->
<!-- add rf and bagging to reduce words. replace all.  -->
<!-- ensure that covariate balance measures are noted and that there is a clear flow down to the simulation settting where balance is discussed.  -->
<!-- make some notes in the application that say why im not comparing balance across polynomicals. perhaps addd these to an appendix somewhere.   -->
<!-- cite r packages -->
<!-- add some comments about sdm thresholds in the intro -->
<section id="a-conventional-approach-propensity-scores-and-balance" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">3.1</span> A Conventional Approach: Propensity Scores and Balance</h2>
<p>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. For similar groups, the average treatment effect (ATT) is a contrast of means from <a href="background.html#eq-ate-estimate" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, <a href="background.html#eq-ate-estimate" class="quarto-xref">Equation&nbsp;<span>2.3</span></a> leads to a biased estimate of the treatment effect.</p>
<p>For example, consider the causal question: <em>“How much does obtaining a bachelors degree increase lifetime earnings?”</em>. Individuals who complete a bachelor’s degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the <em>cause</em> of higher incomes when the effect is confounded by the differences in covariates between groups. Recall that <a href="background.html#fig-dag-confounder" class="quarto-xref">Figure&nbsp;<span>2.1 (a)</span></a> shows a confounding relationship. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of <em>covariate balance</em> which is a key concept behind underlying propensity score methods.</p>
<div id="nte-balance-intution" class="callout callout-style-default callout-note callout-titled" title="What is Covariate Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.1: What is Covariate Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate are the same for each group, then those covariates are <em>balanced</em>. If covariates are similar across groups, then there should not be any confounding. Equally, similar covariates across groups implies exchangability between groups as the two groups should be similar (thus can be exchanged). There is a conceptual equivalence between covariate balance, unconfoundedness, and exchangeability meaning that <a href="background.html#eq-independence" class="quarto-xref">Equation&nbsp;<span>2.6</span></a> is satisfied when covariates are balanced.</p>
</div>
</div>
<p>In bachelor’s degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parent income, geographic residence etc. Comparing the balanced matched pairs should result in a robust estimate of a bachelor’s degree’s impact on earnings because the individuals are exchangeable and satisfy <a href="background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>2.7</span></a>. The covariates are said to be “conditioned on” by matching individuals on these covariates. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a <em>dimensionality</em> problem as the dimension of the number of covariates increases.</p>
<p><span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as <span class="math inline">\(e(X)\)</span> and be expressed as:</p>
<p><span id="eq-pscore"><span class="math display">\[
e(X)=P(T=1|X).
\tag{3.1}\]</span></span></p>
<p>A prediction of the probability of treatment based on the covariates is the best summary of each covariate. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects.</p>
<p>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in <a href="background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>2.7</span></a>. There are many sources that offer a comprehensive guide to propensity score methods such as <span class="citation" data-cites="C5Mixtape2021">(<a href="#ref-C5Mixtape2021" role="doc-biblioref">Cunningham 2021, chap. 4</a>)</span> who provides applications and coded examples in R, Python, and Stata.</p>
<div id="nte-balance-pscore" class="callout callout-style-default callout-note callout-titled" title="Balance and Propensity Scores">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.2: Balance and Propensity Scores
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that an RCT will satisfy <a href="background.html#eq-independence" class="quarto-xref">Equation&nbsp;<span>2.6</span></a> as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy <a href="background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> as the potential outcomes are independent of the treatment status conditioned on some covariates.</p>
<p>The propensity score is conditioned on the covariates because the covariates are the predictors of the propensity score. Conditioning on the propensity score aims to replicate an RCT in observational data by balancing covariates between groups. When observations are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates.</p>
</div>
</div>
<p>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</p>
<p>At a high level, the conditioned property of the propensity score is translated into a model by using PSM or IPW. <span class="citation" data-cites="King2019">King and Nielsen (<a href="#ref-King2019" role="doc-biblioref">2019</a>)</span> provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</p>
<section id="propensity-score-modelling-with-logistic-regression" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="propensity-score-modelling-with-logistic-regression"><span class="header-section-number">3.1.1</span> Propensity Score Modelling with Logistic Regression</h3>
<p>A conventional propensity score model uses logistic regression to predict a probability between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Models may be specified to include interaction terms and polynomial terms so the model captures complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is a heuristically driven art rather than science. <span class="citation" data-cites="Brookhart2006 Heinrich2010">(<a href="#ref-Brookhart2006" role="doc-biblioref">Brookhart et al. 2006</a>; <a href="#ref-Heinrich2010" role="doc-biblioref">Heinrich 2010</a>)</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are not statistically significant. Notably, many researchers do not discuss the specification of propensity models in their papers. <span class="citation" data-cites="Austin2008">Austin (<a href="#ref-Austin2008" role="doc-biblioref">2008</a>)</span> reviews <span class="math inline">\(47\)</span> papers that use propensity scores and find few perform adequate model selection, assess balance, or apply correct statistical tests.</p>
<p>It is important to note that the true propensity score is never observable. A propensity score that is close to the theoretically true probability is well calibrated. Poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. Propensity scores may be poorly calibrated as covariates may be omitted by error, poorly measured, or be unobservable. Logistic regression may not predict calibrated scores if the true relationship is non-linear or involves complex interactions between covariates. Another important note is that the propensity model itself does not have an informative <em>causal</em> interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for each variable which is not informative of the desired estimand.</p>
<p>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</p>
</section>
</section>
<section id="probability-machines-probability-theory-and-machine-learning" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">3.2</span> Probability Machines: Probability Theory and Machine Learning</h2>
<p>Probability prediction is not a typical machine learning task. Supervised machine learning usually focuses on classifying observations into groups, or predicting continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability an observation belongs to a certain class.Machine learning methods that predict probabilities are sometimes called probability machines.</p>
<p>Probability machines are valuable in applications requiring calibrated probability predictions. For example, probability machines can predict loan defaults or other adverse events in finance. In marketing, they estimate the likelihood of customer response to a campaign. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</p>
<p>Probability machines offer many advantages over parametric methods like logistic regression:</p>
<ol type="1">
<li><p><strong>Improved Calibration</strong>: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</p></li>
<li><p><strong>Flexible Modelling</strong>: Unlike parametric methods like logistic regression, probability machines don’t rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</p></li>
<li><p><strong>Efficient Feature Selection</strong>: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</p></li>
<li><p><strong>Handling Missing Data</strong>: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</p></li>
<li><p><strong>Simplified Data Exploration</strong>: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</p></li>
</ol>
<p>In causal inference, probability machines can predict better calibrated propensity scores and better estimate treatment effects. This discussion aims to clarify the use of probability machines in causal inference given the unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</p>
<p>Please note that this chapter assumes a reader is familiar with <em>CART (Classification and Regression Tree)</em>, <em>Boosting</em>, <em>Bagging (Bootstrap Aggregation)</em>, <em>Random Forests</em>, <em>LASSO (Least Absolute Shrinkage and Selection Operator)</em>, and <em>Logistic Regression</em>. These methods are briefly discussed in <a href="background.html#sec-background-ml" class="quarto-xref"><span>Section 2.2</span></a>.</p>
<section id="choice-of-loss-function-and-probability-prediction" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="choice-of-loss-function-and-probability-prediction"><span class="header-section-number">3.2.1</span> Choice of Loss Function and Probability Prediction</h3>
<p>The loss function measures the difference between a model’s predictions and the actual target values, serving as a measure of a model’s performance. The best model exists at the minimum of the loss function. In standard least squares regression from <a href="background.html#sec-background-linear" class="quarto-xref"><span>Section 2.1.9</span></a>, the loss function is the residual sum of squares stated as <span class="math inline">\(\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span>. This loss function states the model must reduce the squared differences between the observed and predicted values. Different loss functions influence a model’s behaviour so the choice of loss function is important.</p>
<p>Classification models predict the category that each observation belongs to not the probability of each class. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Many classification loss functions minimize classification errors and improve accuracy as this results in the best classification. A loss function like the Gini index (introduced in <a href="background.html#sec-background-cart" class="quarto-xref"><span>Section 2.2.1</span></a>) is effective for classification problems but it is unclear if this applies to probability problems. In other words, minimizing misclassification error may not lead to accurate probability predictions.</p>
<p>At a high level, to classify an observation, <span class="math inline">\(x_i\)</span> as an <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, a model needs to determine if <span class="math inline">\(\Pr(x_i=A)\)</span> is less than or greater than <span class="math inline">\(0.5\)</span>. If <span class="math inline">\(\widehat{\Pr}(x_i=A) &gt; 0.5\)</span>, then it is more likely to be an <span class="math inline">\(A\)</span> and if <span class="math inline">\(\widehat{\Pr}(x_i=A) &lt;0.5\)</span> then it is more likely to be a <span class="math inline">\(B\)</span>. Thus, if <span class="math inline">\(x_i\)</span> is an <span class="math inline">\(A\)</span> it is trivial if <span class="math inline">\(\widehat{\Pr}(x_i=A)\)</span> is <span class="math inline">\(0.51\)</span> or <span class="math inline">\(0.99\)</span> as this makes no difference to the classification as an <span class="math inline">\(A\)</span>. But the difference between <span class="math inline">\(\widehat{\Pr}(x_i=A) = 0.51\)</span> and <span class="math inline">\(0.99\)</span> is extreme for a probability machine. It is important to understand that classification models are optimized for classification accuracy rather than probability prediction. This distinction affects the performance of ensemble methods like random forests or bagging ensembles that use classification trees for probability prediction.</p>
</section>
<section id="sec-bagg-rf-probmachines" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="sec-bagg-rf-probmachines"><span class="header-section-number">3.2.2</span> Bagging and Random Forest as Probability Machines</h3>
<!-- todo:  -->
<!-- - reference to appendix -->
<!-- - additional reserach for nsw inclding the cite ther.  -->
<!-- - section reference -->
<p>Across an entire bagging or random forest ensemble, class probabilities are determined through a <em>vote count</em> method. Within that ensemble, each tree makes a class prediction based on the majority class in a terminal node. For instance, if <span class="math inline">\(x_i\)</span> lies in a terminal node where <span class="math inline">\(80\%\)</span> of the observations are classified as an <span class="math inline">\(A\)</span>, that <em>individual tree</em> will classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span>. The ensemble’s overall prediction for <span class="math inline">\(x_i\)</span> is derived from the proportion of trees that classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. Thus the ensemble <em>counts votes</em> for each class across the ensemble. Let <span class="math inline">\(T\)</span> be the total number of trees and <span class="math inline">\(b_t\)</span> be the <span class="math inline">\(t\)</span>-th tree in the ensemble. Let <span class="math inline">\(\mathbb{I}(b_t(x_i) = A)\)</span> be the indicator function that returns <span class="math inline">\(1\)</span> when <span class="math inline">\(b_t\)</span> predicts that observation <span class="math inline">\(x_i\)</span> belongs to class <span class="math inline">\(A\)</span>. The probability of class <span class="math inline">\(A\)</span> for observation <span class="math inline">\(x_i\)</span> is calculated as:</p>
<p><span id="eq-ensemble-vote-method"><span class="math display">\[
\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A).
\tag{3.2}\]</span></span></p>
<p><span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="#ref-Olson2018" role="doc-biblioref">2018</a>)</span> notes a potential bias towards predictions of <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> when trees in a bagged ensemble or random forest are highly correlated. Using a vote count method, an ensemble will bias predicted probabilities towards <span class="math inline">\(\hat{P}(x_i=A) \in \{0,1\}\)</span> when trees are highly correlated. Imagine that each tree in the ensemble is perfectly correlated implying that each tree will make the the same class prediction for each <span class="math inline">\(x_i\)</span>. For such an ensemble, the predicted probabilities will will be exactly <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> using a vote count. Of course an ensemble of identical trees is unrealistic but the intuition still applies in the real world where ensembles may have some degree of correlation. The larger the correlation, the more the probability predictions will exhibit a <em>divergence bias</em> towards <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</p>
<p>If <span class="math inline">\(x_i\)</span> has a known membership of <span class="math inline">\(A\)</span>, and an unknown <span class="math inline">\(\Pr_{\text{true}}(x_i=A) = 0.6\)</span>, the ensemble might classify <span class="math inline">\(x_i\)</span> correctly <span class="math inline">\(90\%\)</span> of the time leading to <span class="math inline">\(\widehat{\Pr}(x_i=A) = 0.9\)</span>. As a probability machine, the ensemble has overestimated the probability by <span class="math inline">\(0.3\)</span> even though a <span class="math inline">\(90\%\)</span> classification accuracy is excellent. To predict <span class="math inline">\(P_{\text{true}}(x_i=A) = 0.6\)</span>, an ensemble needs to incorrectly classify <span class="math inline">\(x_i\)</span> in <span class="math inline">\(40\%\)</span> of its trees. However, bagging ensembles and random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</p>
<p>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected bootstrapped sample of the data. Random forests further reduce tree correlation by considering a random number of variables at each split, referred to as <span class="math inline">\(mtry\)</span>. When <span class="math inline">\(mtry\)</span> is equal to to number of predictors, the model considers all variables at each split and the random forest is equal to a bagging ensemble. A lower <span class="math inline">\(mtry\)</span> should reduce the correlation between trees and decrease divergence bias as the structure of the tree is modified by the selected variables at each split. However, a lower <span class="math inline">\(mtry\)</span> also introduces other theoretical problems.</p>
<p>Consider the scenario where the binary outcome (treatment and control) of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If <span class="math inline">\(mtry\)</span> is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. Each predictor has a chance of <span class="math inline">\(\frac{mtry}{\text{number of predictors}}\)</span> of selection at each split implying a lower <span class="math inline">\(mtry\)</span> decreases the chance of a splitting on the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of <span class="math inline">\(x_i\)</span> in each tree. Such an ensemble may have highly unstable probability predictions.</p>
<p>Additionally, consider there is a class imbalance and the majority of observations are classified as <span class="math inline">\(A\)</span> not <span class="math inline">\(B\)</span>. The terminal nodes of each tree within an ensemble are more likely to contain the majority class. Consequently, there is a <em>majority class bias</em> as each tree in the ensemble is more likely to misclassifying an observation as an <span class="math inline">\(A\)</span> because the terminal nodes have a higher proportion of <span class="math inline">\(A\)</span> due to the higher proportion of <span class="math inline">\(A\)</span>’s in the data overall.</p>
<div id="nte-class-imbalance" class="callout callout-style-default callout-note callout-titled" title="Class Imbalance and Machine Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.3: Class Imbalance and Machine Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>When there is a difference in the number of observations in each class, this is called class imbalance. It is important to note that majority class bias exists in conventional classification tasks with machine learning. Bagging ensembles and random forest are well known to be sensitive to class imbalance meaning that class predictions are biased towards the majority <span class="citation" data-cites="Bader2019">(see <a href="#ref-Bader2019" role="doc-biblioref">Bader-El-Den, Teitei, and Perry 2019</a>)</span>.</p>
<p>However, the class imbalance problem is particularly notable when predicting probabilities. The probability prediction from a vote count method is very sensitive to a change in the votes from each tree. Suppose that balanced data results in <span class="math inline">\(80/100\)</span> trees classifying <span class="math inline">\(B\)</span> as <span class="math inline">\(B\)</span> and imbalanced data (more <span class="math inline">\(A\)</span> than <span class="math inline">\(B\)</span>) reduces correct classifications of <span class="math inline">\(B\)</span> to <span class="math inline">\(60/100\)</span>. This results in a <span class="math inline">\(20\%\)</span> margin of error in probability estimates but the classification remains as <span class="math inline">\(B\)</span>.</p>
</div>
</div>
<p>Individually, a low <span class="math inline">\(mtry\)</span> can lead to unstable probability predictions and class imbalance can create bias towards the majority class. But probability machines are particularly effected when there is both a low <span class="math inline">\(mtry\)</span> <em>and</em> class imbalance. Because successive noisy splits (relating to a low <span class="math inline">\(mtry\)</span>) result in impure child nodes, the effects of majority class bias are exaggerated. Without the ability to separate the classes, the majority class will dominate terminal nodes. If the ensemble was able to split on informative covariates each time (<span class="math inline">\(mtry\)</span> is higher), then it should still be able to create pure splits even when there is some class imbalance. In other words, if there is a small class imbalance, reducing <span class="math inline">\(mtry\)</span> may reveal majority class bias not visible at higher <span class="math inline">\(mtry\)</span>’s. Equally, if there is low <span class="math inline">\(mtry\)</span>, then even a small class imbalance can lead to majority class bias.</p>
<p>To exemplify these theoretical points, the National Supported Work (NSW) programme is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with <span class="math inline">\(445\)</span> total participants, <span class="math inline">\(185\)</span> in the program group, and <span class="math inline">\(260\)</span> in the control group, so the true probability of treatment for each individual can be calculated as <span class="math inline">\(185/445=0.42\)</span> or <span class="math inline">\(42\%\)</span>. Further information about this data is found in <a href="appendix.html#sec-data-nsw-jobs" class="quarto-xref"><span>Section 7.1.1</span></a>.</p>
<p>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although <a href="#fig-rf-varimp" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> and <a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;<span>3.1</span></a> suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by <span class="citation" data-cites="Smith2005">Smith and Todd (<a href="#ref-Smith2005" role="doc-biblioref">2005</a>)</span> who suggests that self-selection bias is prevalent in the NSW data.</p>
<p><a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows both divergence bias and majority class bias using <code>randomForest()</code> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when <span class="math inline">\(mtry\)</span> is equal to the number of predictors and so specifying <code>mtry = 7</code> in the <code>randomForest()</code> function fits a bagging ensemble. Additionally, logistic regression using the <code>gbm()</code> function provides a meaningful comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nsw_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">as.factor</span>(treat) <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                          black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> marr)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>logit_preds <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="fu">binomial</span>())<span class="sc">$</span>fitted.values </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>rf_mtry1_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">randomForest</span>(nsw_formula, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">data =</span> nsw_data), </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                          <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(nsw_formula, <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> nsw_data)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>bagged_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagging_model, <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plot_pmachines <span class="ot">&lt;-</span> <span class="cf">function</span>(pscores, plot_subtitle) {</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(nsw_data, <span class="fu">aes</span>(<span class="at">x =</span> pscores, <span class="at">fill =</span> <span class="fu">factor</span>(treat))) <span class="sc">+</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Participant"</span>)) <span class="sc">+</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">subtitle =</span> plot_subtitle, <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>         <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    custom_ggplot_theme</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(logit_preds, <span class="st">"Logistic Regression"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>) <span class="sc">+</span> </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(rf_mtry1_preds, <span class="st">"Random Forest (mtry = 1)"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(bagged_preds, <span class="st">"Bagging (Bootstrap Aggregation)"</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">/</span> p2 <span class="sc">/</span> p3 <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-theory-demo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-rf-theory-demo-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Shows kernel density estimates of propensity scores for each controls and participants in the National Supported Work programme. <code>randomForest()</code> fits a random forest with <code>mtry = 1</code> and bagging ensemble with <code>mtry = 7</code>. Uses the default values of <code>ntree = 500</code> and <code>nodesize = 1</code>.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"none"</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-varimp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-rf-varimp-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: The figure compares the variable importance assigned to each variable from a baggin ensemble. The data originates from the National Supported Work programme. The difference in relative important of some variables indicates that randomisation may not have created exchangability between the groups.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows the logistic regression model has identified a central tendency and most propensities are between <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span> which roughly aligns with the true probability. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span> which is likely related to tree correlation. For the random forest with <span class="math inline">\(mtry=1\)</span>, a significant number of the treatment and control observations are centred near the control area (<span class="math inline">\(T=0\)</span>) with a wide range of other predictions. Recall that the control group is the majority class. Reducing <span class="math inline">\(mtry\)</span> from <span class="math inline">\(7\)</span> to <span class="math inline">\(1\)</span> reveals the majority class bias reinforcing the theoretical discussion that a combination of low <span class="math inline">\(mtry\)</span> and class imbalance is especially troubling. The models over predicts the majority class and has unstable predictions otherwise. Both random forest and bagging ensembles have performed poorly compared to the true probability of <span class="math inline">\(0.42\%\)</span>.</p>
<p>The tuning of <span class="math inline">\(mtry\)</span> faces double jeopardy and is another important area of discussion in probability machines. The selection of <span class="math inline">\(mtry\)</span> is typically completed in with a classification loss function such as accuracy or out-of-bag error. <span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="#ref-Olson2018" role="doc-biblioref">2018</a>)</span> compares tuning <span class="math inline">\(mtry\)</span> measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of <span class="math inline">\(mtry\)</span> for classification differs from probability prediction.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In other words, if a grid search finds that <span class="math inline">\(mtry=3\)</span> is optimal for a classification task, this does not imply that <span class="math inline">\(mtry=3\)</span> is optimal for predicting probabilities so the tuning of <span class="math inline">\(mtry\)</span> is difficult.</p>
<p>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in <a href="#sec-mlps-sims" class="quarto-xref"><span>Section 3.2.5</span></a>. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</p>
<p>Heuristically, divergence bias and majority class bias will most effect a probability machine when there is considerable overlap between groups. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common overlap may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low <span class="math inline">\(mtry\)</span>, the terminal nodes of each tree may be relatively impure leading to a majority class bias. Alternatively, if true probabilities exist near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and there is a clear separation of class, divergence bias may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low <span class="math inline">\(mtry\)</span> may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</p>
<!-- maybe need a little chat about cross entropy here and why its not as good as in the gbm case.  -->
</section>
<section id="sec-gbm" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-gbm"><span class="header-section-number">3.2.3</span> Gradient Boosting Machines as Probability Machines</h3>
<!-- needs a little more comparison to rf and bagging. perhaps do some more reserach about why these are good. note and differentiate the different types of boosting. perhaps also clarify how the gradient descent works for my own learning (dont write it to be too technical).  -->
<p>Moving beyond classification trees in random forests or bagging ensembles, <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> introduced the <em>Gradient Boosting Machine</em> (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss function to reach a minimum. The update rule for the model after each iteration can be expressed as:</p>
<p><span class="math display">\[
\hat{p}_i^{(t)} = \hat{p}_i^{(t-1)} + \lambda \cdot b_t(x_i),
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the learning rate, and <span class="math inline">\(b_t(x_i)\)</span> is the <span class="math inline">\(b\)</span>-th regression tree fitted on the pseudo-residuals of the previous regression tree. In words, the current overall iteration <span class="math inline">\(t\)</span>, is a combination of the previous model plus the current iteration scaled by a learning rate.</p>
<p>A learning rate controls the contribution of each weak learner to the final model. By using a small learning rate, the machine learns slowly so that it can slowly descend the loss function. This allows for finer adjustments during the iterative process to better capture patterns in the data.</p>
<p>GBMs can be generalized to many different applications by minimizing a different loss function which can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple regression trees and a logistic function to transform regression predictions into probabilities. Specifically, the logistic function used is:</p>
<p><span class="math display">\[
\hat{p}_i = \frac{1}{1 + \exp(-\text{model}(x_i))}.
\]</span></p>
<p>This logistic function is the same as in logistic regression, so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood with logistic regression. Maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The full GBM model, <span class="math inline">\(f_T(x)\)</span> after <span class="math inline">\(T\)</span> iterations can be written as:</p>
<p><span class="math display">\[
f_T(x_i) = b_1(x_i) + \lambda \sum_{t=1}^{T} b_t(x_i).
\]</span></p>
<p>Inside a base tree, each split considers all variables and makes the most informative split to descend the loss function using gradient descent. GBMs utilize many weak learners, such as a regression tree with a single split called a regression stump. However, additional splits enable the model to capture interactions between terms, which may increase probability calibration in complex or high-dimensional datasets.</p>
<p>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques, as well as allowing a probability distribution-based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. Since GBMs predict probabilities from a logistic function, they avoid problems associated with a vote count method. Additionally, there are no difficult parameters to tune, such as <span class="math inline">\(mtry\)</span> in a random forest. The implementation and workflow to fit a GBM for propensity scores is discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref"><span>Section 3.3.1</span></a>.</p>
</section>
<section id="overfitting" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">3.2.4</span> Overfitting</h3>
<p>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting can prevent overfitting to ensure a model can generalise to unseen data.</p>
<p>However, the model is not required to generalise a propensity score model as different datasets will have a different model. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</p>
<div id="nte-overfit-logistic" class="callout callout-style-default callout-note callout-titled" title="Overfitting in Logistic Regression">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.4: Overfitting in Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is <span class="citation" data-cites="Schuster2016">Schuster, Lowe, and Platt (<a href="#ref-Schuster2016" role="doc-biblioref">2016</a>)</span>, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> receive artificially inflated weighting.</p>
</div>
</div>
<p><span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</p>
<p>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</p>
</section>
<section id="sec-mlps-sims" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="sec-mlps-sims"><span class="header-section-number">3.2.5</span> Comparison of Machine Learning Algorithms: Simulation Results</h3>
<!-- todo:  -->
<!-- fix early cite -->
<!-- clarify that sim studies are propensity score based. maybe look for general probability machine example sims.  -->
<p>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="citation" data-cites="McCaffrey2004 Setoguchi2008 Lee2010 Cannas2019 Tu2019 Goller2020 Ferri2020">(see <a href="#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="#ref-Setoguchi2008" role="doc-biblioref">Setoguchi et al. 2008</a>; <a href="#ref-Lee2010" role="doc-biblioref">Lee, Lessler, and Stuart 2010</a>; <a href="#ref-Cannas2019" role="doc-biblioref">Cannas and Arpino 2019</a>; <a href="#ref-Tu2019" role="doc-biblioref">Tu 2019</a>; <a href="#ref-Goller2020" role="doc-biblioref">Goller et al. 2020</a>; <a href="#ref-Ferri2020" role="doc-biblioref">Ferri-García and Del Mar Rueda 2020</a>)</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</p>
<p><span class="citation" data-cites="Tu2019">Tu (<a href="#ref-Tu2019" role="doc-biblioref">2019</a>)</span> compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</p>
<p>Despite poor theoretical properties as a probability machine, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> find that bagging results in the lowest standard error across many datasets.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</p>
<p>Additionally, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best <span class="math inline">\(95\%\)</span> coverage in all simulation scenarios, with <span class="math inline">\(98.6\%\)</span> coverage.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</p>
<p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="#ref-Goller2020" role="doc-biblioref">2020</a>)</span> adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</p>
<p>Based on a review of the literature, the findings can be distilled into five important points:</p>
<ol type="1">
<li><p>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</p></li>
<li><p>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="citation" data-cites="Rosenbaum1983">(<a href="#ref-Rosenbaum1983" role="doc-biblioref">Rosenbaum and Rubin 1983</a>)</span>.</p></li>
<li><p>Bagging ensembles perform poorly, a finding replicated across multiple studies.</p></li>
<li><p>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</p></li>
<li><p>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</p></li>
<li><p>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</p></li>
<li><p>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</p></li>
</ol>
</section>
</section>
<section id="implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r"><span class="header-section-number">3.3</span> Implimentation and Hyperparameter Tuning with <code>WeightIt</code> and<code>gbm</code> in R</h2>
<p>Based on <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span>, the <code>gbm</code> package implements a <em>Generalized Boosting Machine</em>. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. Although this implementation very closely follows <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> who introduced the gradient boosting machine. <code>gbm</code> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <code>bag.fraction</code> parameter.</p>
<p>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <code>gbm</code> package does not support this type of tuning, a wrapper package like <code>WeightIt</code> is necessary. <code>WeightIt</code> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <code>WeightIt</code> supports hyperparameter turning of <code>shrinkage</code>, <code>interaction.depth</code>, and <code>n.trees</code>. Once the best model is identified, propensity scores are predicted inside <code>WeightIt</code>. These can be used inside <code>WeightIt</code> to perform IPW or extracted for other implementations. <code>WeightIt</code> also supports an offset meaning that logistic regression predictions are supplied to the <code>GBM</code> package.</p>
<p>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="citation" data-cites="McCaffrey2004 Ridgeway2024">(see <a href="#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="#ref-Ridgeway2024" role="doc-biblioref">Ridgeway et al. 2024</a>)</span>. A very low learning rate, such as <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.0005\)</span>, allows a smooth descent of the loss function. The model should include a high number of trees, with <span class="math inline">\(10,000\)</span> or <span class="math inline">\(20,000\)</span> being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for <span class="math inline">\(3\)</span> splits. Based on anecdotal experience, <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span> splits per tree is optimal, consistent with recommendations by <span class="citation" data-cites="McCaffrey2004">McCaffrey, Ridgeway, and Morral (<a href="#ref-McCaffrey2004" role="doc-biblioref">2004</a>)</span>.</p>
<p>Another package, <code>twang</code>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <code>caret</code> must be used. Although <code>twang</code> has other useful functionalities which users may wish to implement.</p>
<section id="sec-gbm-tune-workflow" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-gbm-tune-workflow"><span class="header-section-number">3.3.1</span> Hyperparameter Tuning and Workflow</h3>
<!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  -->
<p>The <code>WeigthtIt</code> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <code>cobalt</code>. The best information for this package can be found on this <a href="https://ngreifer.github.io/WeightIt/index.html">website</a> or accessed with <code>vignette("WeightIt")</code> inside R after installation using <code>install.packages("WeightIt")</code>.</p>
<p>A workflow for hyperparameter tuning in <code>WeightIt</code> may be completed as follows:</p>
<ol type="1">
<li><p>Specify the <code>criterion</code> option, which specifies the measure of the <em>”best model”</em>. The available options are the options that the <code>cobalt</code> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <code>sdm.mean</code> or the smallest maximum SDM across covariates called <code>sdm.max</code>.</p></li>
<li><p>Set the number of trees high. The package default is <code>n.trees = 10000</code> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <code>n.tree</code> if optimal.</p></li>
<li><p>Specify the grid search for the depth of the tree called <code>interaction.depth</code> and the learning rate called <code>shrinkage</code>. These values can be specified using <code>c()</code> such as <code>shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)</code> or as integers such as <code>interaction.depth = 1:5</code>. These particular values are heuristically selected <em>suggestions</em> of good starting values. Additionally, an offset can be considered by performing a grid search across <code>offset=c(TRUE,FALSE)</code>.</p></li>
<li><p>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <code>my_weightit_object$info$best.tune</code>.</p></li>
<li><p>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <code>interaction.depth = 1:5</code> and the best fit is <span class="math inline">\(5\)</span>, then a new search can consider <code>interaction.depth = 3:7</code> so that the local area around <span class="math inline">\(5\)</span> can be searched.</p></li>
<li><p>Experiment with <code>bag.fraction</code>, which means each tree will consider a drawn proportion of observations equal to <code>bag.fraction</code>. Iteratively changing <code>bag.fraction</code> and assessing balance at each value should be practical. Consider <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.67\)</span>, and <span class="math inline">\(1\)</span>.</p></li>
<li><p>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <code>love.plot()</code> such as <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>.</p></li>
<li><p>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</p></li>
<li><p>Estimation and reporting of treatment effect.</p></li>
</ol>
</section>
</section>
<section id="example-nsw-jobs-dataset-using-r" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="example-nsw-jobs-dataset-using-r"><span class="header-section-number">3.4</span> Example: NSW Jobs Dataset Using R</h2>
<p>For demonstration, propensity scores are estimated following the workflow discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref"><span>Section 3.3.1</span></a> to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in <a href="appendix.html#sec-data-nsw-jobs" class="quarto-xref"><span>Section 7.1.1</span></a>. Randomisation should eliminate structural differences between groups, but <span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <code>WeightIt</code> and a logistic regression model using <code>glm()</code>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</p>
<div id="nte-ipw" class="callout callout-style-default callout-note callout-titled" title="Inverse Probability of Treatment Weighting">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.5: Inverse Probability of Treatment Weighting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a <em>pseudo-population</em> where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</p>
</div>
</div>
<section id="step-1-6-model-fitting-and-tuning" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="step-1-6-model-fitting-and-tuning"><span class="header-section-number">3.4.1</span> Step 1-6: Model Fitting and Tuning</h3>
<p>The <code>glm()</code> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <code>binomial()</code>. Recall the <code>nsw_formula</code> is specified in <a href="#sec-bagg-rf-probmachines" class="quarto-xref"><span>Section 3.2.2</span></a></p>
<div class="sourceCode" id="annotated-cell-3"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-3-2" class="code-annotation-target"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>())</span>
<span id="annotated-cell-3-3"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-3-4" class="code-annotation-target"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="2" data-code-annotation="1">Fits a logistic regression model using the <code>glm()</code> function specified to be a logistic model with <code>family=binomial()</code> using the previously created <code>nsw_formula</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="4" data-code-annotation="2">Extracts the fitted values (propensity scores) from the model.</span>
</dd>
</dl>
<p>Using the propensity score column of <code>nsw_data</code>, the <code>WeightIt</code> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <code>nsw_logit_weight</code>.</p>
<div class="sourceCode" id="annotated-cell-4"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-4-1"><a href="#annotated-cell-4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-4-2" class="code-annotation-target"><a href="#annotated-cell-4-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-4-3" class="code-annotation-target"><a href="#annotated-cell-4-3" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-4-4" class="code-annotation-target"><a href="#annotated-cell-4-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-4" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="3" data-code-annotation="2">Provides <code>weightit()</code> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <code>weightit()</code> function with <code>method = "glm"</code>. The separate estimation of the propensity scores is for illustrative purposes.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="4" data-code-annotation="3">Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
</dd>
</dl>
<p>A GBM model for propensity scores can be specified using <code>method = "gbm"</code> inside the <code>weightit()</code> function. To ensure consistent results, running <code>set.seed(88)</code> will ensure each tree uses the same <code>seed</code> if <code>bag.fraction</code> less than <span class="math inline">\(1\)</span>. The model is fit using the heuristically suggested starting values. Note that this model may take approximately <span class="math inline">\(30\)</span> second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</p>
<div class="sourceCode" id="annotated-cell-5"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-5-1"><a href="#annotated-cell-5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-5-2" class="code-annotation-target"><a href="#annotated-cell-5-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-5-3" class="code-annotation-target"><a href="#annotated-cell-5-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="annotated-cell-5-4"><a href="#annotated-cell-5-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-5-5" class="code-annotation-target"><a href="#annotated-cell-5-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>),</span>
<span id="annotated-cell-5-6"><a href="#annotated-cell-5-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-5-7" class="code-annotation-target"><a href="#annotated-cell-5-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="annotated-cell-5-8"><a href="#annotated-cell-5-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-5-9" class="code-annotation-target"><a href="#annotated-cell-5-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>,</span>
<span id="annotated-cell-5-10"><a href="#annotated-cell-5-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="6" onclick="event.preventDefault();">6</a><span id="annotated-cell-5-11" class="code-annotation-target"><a href="#annotated-cell-5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-5" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="3,4" data-code-annotation="2">Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="5,6" data-code-annotation="3">Performs a grid search over these values of the learning rate and depth of tree.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="7,8" data-code-annotation="4">Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="9,10" data-code-annotation="5">Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be <span class="math inline">\(10000\)</span> which is the package default.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="11" data-code-annotation="6">Prints the tune details of the model with the best covariate balance.</span>
</dd>
</dl>
<!-- clarify the meaning of learning rate/shrinkage -->
<!-- change all the instructions to active speech not passive.  -->
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
6       0.3                 1    bernoulli      FALSE    0.02253485      2392</code></pre>
</div>
</div>
<!-- cite what the balance statistics should be in the intro when discussing propensity score and balance.  -->
<p>The best balance across all tuning combinations yields an average SMD of <span class="math inline">\(0.023\)</span> showing strong balance. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in <a href="#sec-nsw-balance" class="quarto-xref"><span>Section 3.4.2</span></a> accompanying a discussion of balance.</p>
<p>The best machine has a learning rate of <span class="math inline">\(0.3\)</span> and contains <span class="math inline">\(2392\)</span> decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to <span class="math inline">\(0.3\)</span>. A reduction in the depth of tree and number of trees will reduce computation time.</p>
<p>The new tune grid will consider <code>shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)</code> as this allows the GBM to consider values between <span class="math inline">\(0.2\)</span> and <span class="math inline">\(0.3\)</span> and above <span class="math inline">\(0.3\)</span> which were missing in the previous grid.</p>
<div class="cell">
<details class="code-fold">
<summary>PALCEHOLDER</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight2 <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method=</span><span class="st">"gbm"</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage=</span> <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.3</span>, <span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.45</span>, <span class="fl">0.5</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
11      0.45                 2    bernoulli      FALSE    0.01965492        95</code></pre>
</div>
</div>
<p>Comparing the two iterations, there is a reduction from <span class="math inline">\(0.022\)</span> to <span class="math inline">\(0.02\)</span>. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of <span class="math inline">\(0.45\)</span>, a tree depth of <span class="math inline">\(2\)</span>, and <span class="math inline">\(95\)</span> trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</p>
<p>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, <a href="#fig-balance-iterations" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> shows the optimal number of trees is highly variable. If the learning rate is set to <code>shrinkage = 0.05</code>, then the best balance is not achieved until near to <span class="math inline">\(20,000\)</span> trees.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Make the <span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span></summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span> </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-balance-iterations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-balance-iterations-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Average Standardised Mean Differernce (Covaraite Balance) and the number of interations. Please note the difference in horozontal scale between the two plots.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</p>
</section>
<section id="sec-nsw-balance" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="sec-nsw-balance"><span class="header-section-number">3.4.2</span> Step 7 and 8: Assessing Balance</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="The Importance of Discussing Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Importance of Discussing Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study’s conclusions. <span class="citation" data-cites="King2019">King and Nielsen (<a href="#ref-King2019" role="doc-biblioref">2019</a>)</span> notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</p>
<p>A good resource of information for assessing balance is documentation from the <code>cobalt</code> package, which can be viewed by running <code>vignette(“cobalt”, package = “cobalt”)</code> in R.</p>
</div>
</div>
<p><code>cobalt</code> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <code>WeightIt</code> for IPW and <code>MatchIt</code> for propensity score matching. Balance tables are created using <code>bal.tab()</code>.</p>
<!-- make sure this comment about integration is not repeditive  -->
<div class="sourceCode" id="annotated-cell-8"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-8-1" class="code-annotation-target"><a href="#annotated-cell-8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-8-2" class="code-annotation-target"><a href="#annotated-cell-8-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight,</span>
<span id="annotated-cell-8-3"><a href="#annotated-cell-8-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-8-4" class="code-annotation-target"><a href="#annotated-cell-8-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="annotated-cell-8-5"><a href="#annotated-cell-8-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-8-6" class="code-annotation-target"><a href="#annotated-cell-8-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="annotated-cell-8-7"><a href="#annotated-cell-8-7" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-8-8" class="code-annotation-target"><a href="#annotated-cell-8-8" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="1" data-code-annotation="1">Loads the <code>cobalt</code> package. This assumes the package is already installed with <code>install.packages("cobalt")</code></span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="2,3" data-code-annotation="2">Uses the <code>bal.tab()</code> fucntion to create balance statistics for the previously created <code>nsw_logit_weight</code> model.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="4,5" data-code-annotation="3">Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="6" data-code-annotation="4">Sets a threshold of balance to be <span class="math inline">\(0.1\)</span> to determine if a covariate is balanced.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="8" data-code-annotation="5">Extracts the balance table of the <code>nsw_logit_btab</code> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
</dd>
</dl>
<p>Additionally, <code>bal.tab()</code> will create balance tables for the GBM method’s IPWs and the raw data. For presentation, <code>dplyr</code> combines each of the individual balance tables for presentation using <code>kable</code> and <code>kableExtra</code>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to See Creation of Balance Tables</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_boosted_weight, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> nsw_data,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_formula, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> nsw_data, </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> nsw_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> nsw_raw_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<details class="code-fold">
<summary>Show the Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T,<span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width=</span><span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-combined-btab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Balance Table for NSW Data
</figcaption>
<div aria-describedby="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell">
<div id="tbl-combined-btab" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Placeholder
</figcaption>
<div aria-describedby="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variable</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Type</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">SMD</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Balanced</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.11</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.03</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.13</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.55</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.08</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.08</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.04</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.20</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.28</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.09</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Logistic Regression</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.98</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.27</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.80</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Boosting</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.91</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.02</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.14</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.02</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.01</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.05</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.05</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
<!-- double check that the variables are in the righ tpalces in the table  -->
<p><a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;<span>3.1</span></a> shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</p>
<!-- perhaps find the threshold for variance ratios -->
</section>
<section id="sec-nsw-results" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="sec-nsw-results"><span class="header-section-number">3.4.3</span> Step 9: Results</h3>
<p>Finally, the treatment effect can be estimated using <code>lm_weightit()</code> from the <code>WeightIt</code> package and <code>avg_comparisons()</code> from the <code>marginaleffects</code> package. <code>lm_weightit()</code> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <code>avg_comparisons()</code> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</p>
<p>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="citation" data-cites="Naimi2017">(<a href="#ref-Naimi2017" role="doc-biblioref">Naimi, Cole, and Kennedy 2017</a>)</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <code>re78</code> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</p>
<div class="sourceCode" id="annotated-cell-10"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-10-1" class="code-annotation-target"><a href="#annotated-cell-10-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span></span>
<span id="annotated-cell-10-2"><a href="#annotated-cell-10-2" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-10-3" class="code-annotation-target"><a href="#annotated-cell-10-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights)</span>
<span id="annotated-cell-10-4"><a href="#annotated-cell-10-4" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-10-5" class="code-annotation-target"><a href="#annotated-cell-10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="annotated-cell-10-6"><a href="#annotated-cell-10-6" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-10" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="1,2" data-code-annotation="1">Uses <code>lm_weightit()</code> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <code>*</code> indicates multiplication in R.</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="3" data-code-annotation="2">Specifies the <code>weights</code> from the <code>nsw_boosted_weight</code> object created earlier by the <code>weightit()</code> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <code>nsw_boosted_weight$weights</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="5,6" data-code-annotation="3">Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
</dd>
</dl>
<p>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create the Table</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78<span class="sc">~</span>treat<span class="sc">*</span>(age <span class="sc">+</span> educ <span class="sc">+</span> </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                             re75 <span class="sc">+</span> black <span class="sc">+</span> hisp <span class="sc">+</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                             degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">weights =</span> nsw_logit_weight<span class="sc">$</span>weights)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>nsw_logit_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_logit_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>nsw_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">extract_comparison_results</span>(nsw_logit_result),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(nsw_boosted_result))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(nsw_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Logistic Regression"</span>, <span class="st">"GBM"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(nsw_comparisons_tab, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="tbl-nsw-comparisons" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Comparison of ATE Estimates
</figcaption>
<div aria-describedby="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell">
<div class="cell-output-display">
<table class="table caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: center;">1610.79</td>
<td style="text-align: center;">668.49</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">300.58</td>
<td style="text-align: center;">2921.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">Generalized Boosting Machine</td>
<td style="text-align: center;">1609.95</td>
<td style="text-align: center;">669.42</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">297.91</td>
<td style="text-align: center;">2921.99</td>
</tr>
</tbody>
</table>


</div>
</div>
</div>
</figure>
</div>
<p><a href="#tbl-nsw-comparisons" class="quarto-xref">Table&nbsp;<span>3.2</span></a> shows that both estimates of the treatment effect are nearly identical at <span class="math inline">\(\$1610\)</span> with logistic regression inferring a <span class="math inline">\(\$0.86\)</span> larger treatment effect. Additionally, these results are statistically significant at the <span class="math inline">\(5\%\)</span> level with nearly identical standard errors.</p>
</section>
</section>
<section id="replication-study" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="replication-study"><span class="header-section-number">3.5</span> Replication Study</h2>
<p>The replication study focuses on a paper titled “The Impact of Coffee Certification on Small-Scale Producers’ Livelihoods: A Case Study from the Jimma Zone, Ethiopia,” published in Agricultural Economics (2012) by Pradyot Ranjan Jena, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. This paper explores the effects of coffee certification schemes on the economic wellbeing of small-scale coffee farmers in Ethiopia, particularly examining whether these schemes contribute to poverty reduction and improved livelihoods among smallholders.</p>
<p>The central theme of the paper is the evaluation of certification schemes, such as Fairtrade and organic certification, as tools for enhancing the income stability and economic resilience of small-scale coffee producers. Certification is seen as a potential tool for economic growth and and environmental sustainability and so it is important to understand the impact on small-scale farmers. <a href="#tbl-coffee-var-summary" class="quarto-xref">Table&nbsp;<span>3.3</span></a> summarises the variables used in the propensity score model.</p>
<div class="cell">
<div id="tbl-coffee-var-summary" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-var-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.3: bla
</figcaption>
<div aria-describedby="tbl-coffee-var-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Variable name</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Description of variable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left; font-weight: bold;">Certification (Treatment/ Control)</td>
<td style="text-align: left; font-weight: bold;">If the farming household is certified (=1) or otherwise (=0)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Household Age</td>
<td style="text-align: left;">Age of the head of the household in years</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Squared Household Age</td>
<td style="text-align: left;">Age of the head of the household squared</td>
</tr>
<tr class="even">
<td style="text-align: left;">Gender</td>
<td style="text-align: left;">Gender of the head of household (male = 1 and female = 0)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dependency Ratio</td>
<td style="text-align: left;">Household members below 14 and above 65 years divided by rest of the household member</td>
</tr>
<tr class="even">
<td style="text-align: left;">Education Level</td>
<td style="text-align: left;">Education of the head of household in years</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Years of Coffee Production</td>
<td style="text-align: left;">Years of experience in coffee farming</td>
</tr>
<tr class="even">
<td style="text-align: left;">Log Total Land</td>
<td style="text-align: left;">Logarithm of total land size in hectares</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Access to Credit</td>
<td style="text-align: left;">Household has access to credit (yes = 1, otherwise = 0)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bad Weather</td>
<td style="text-align: left;">If the household was affected by floods/droughts during the last year (2008–2009)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Non-farm Income Access</td>
<td style="text-align: left;">If the household has access to nonfarm income</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p><span class="citation" data-cites="Jana2012">(<a href="#ref-Jana2012" role="doc-biblioref"><strong>Jana2012?</strong></a>)</span> define livelihood as a combination of per capita income, total income, per capita consumption and yield per hectare. For simplicity, this replication will only use per capita income as a dependent variable. This measure is selected as per capita income best quantifies the individual income which most strongly impacts overall livelihood <span class="citation" data-cites="cite">(see <a href="#ref-cite" role="doc-biblioref"><strong>cite?</strong></a>)</span>.</p>
<p>Randomisation into certified and uncertified is not possible and it is likely that farmers who seek certification are different than farmers who don’t. Thus, there is selection bias leading to structural differences between groups so a contrast in means between the certified (treated) and uncertified (control) farmers would be biased. Propensity scores are used to create covariate balance and reduce bias of the estimated treatment effect. The paper did not assess the balance of covariates. However, this provides a good opportunity to assess covariate balance in the initial paper and the repeat the analysis using a machine learning propensity model.</p>
<section id="replication-of-original-results" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="replication-of-original-results"><span class="header-section-number">3.5.1</span> Replication of Original Results</h3>
<p><span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span> provides a replication package including Stata code that uses Stata’s psmatch2 package to perform nearest neighbour matching with replacement and common support trimming. Common support trimming means that any observations outside the commonly overlapping are are discarded. The results of the paper are be fully replicated using the <code>MatchIt</code> package inside R.</p>
<div class="cell">
<details class="code-fold">
<summary>Show Code to Replicate Results of <span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span></summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>coffee_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(certified <span class="sc">~</span> age_hh <span class="sc">+</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                  agesq <span class="sc">+</span> nonfarmincome_access <span class="sc">+</span> depratio <span class="sc">+</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                  logtotal_land <span class="sc">+</span> badweat <span class="sc">+</span> edu <span class="sc">+</span> gender <span class="sc">+</span> </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                  years_cofeproduction <span class="sc">+</span> access_credit)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MatchIt)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>coffee_rep_pmodel <span class="ot">&lt;-</span> <span class="fu">matchit</span>(coffee_formula, <span class="at">data=</span>coffee_data, <span class="at">distance=</span><span class="st">"glm"</span>, </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>                              <span class="at">method=</span><span class="st">"nearest"</span>, <span class="at">replace =</span> T, <span class="at">estimand=</span><span class="st">"ATT"</span>, </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                              <span class="at">discard=</span><span class="st">"both"</span>) </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>coffee_logit_md <span class="ot">&lt;-</span> <span class="fu">match.data</span>(coffee_rep_pmodel)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>coffee_rep_fit<span class="ot">&lt;-</span> <span class="fu">lm</span>(percapitaincome_day_maleeq <span class="sc">~</span> certified, <span class="at">data =</span> coffee_logit_md, <span class="at">weights=</span>weights)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>replicated_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_rep_fit, <span class="at">variables =</span> <span class="st">"certified"</span>,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                <span class="at">vcov =</span> <span class="cn">TRUE</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">newdata =</span> <span class="fu">subset</span>(coffee_logit_md, certified <span class="sc">==</span> <span class="dv">1</span>),</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                <span class="at">wts =</span> <span class="st">"weights"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show to Code to Make <a href="#tbl-replicated-result">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>replicated_result_tbl <span class="ot">&lt;-</span> <span class="fu">extract_comparison_results</span>(replicated_result)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(replicated_result_tbl) <span class="ot">&lt;-</span> <span class="st">"Replicated Result"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(replicated_result_tbl, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-replicated-result" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-replicated-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.4: Placeholder
</figcaption>
<div aria-describedby="tbl-replicated-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Replicated Result</td>
<td style="text-align: center;">-0.15</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">-1.6</td>
<td style="text-align: center;">1.29</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p><a href="#tbl-replicated-result" class="quarto-xref">Table&nbsp;<span>3.4</span></a> shows the replicated result obtained by <span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span>. The intriguing finding of the paper is that the average treatment effect on the treated (ATT) is negative. That is, of the farmers that become certified, their per capita income is expected to decrease by <span class="math inline">\(\$0.15\)</span> per day. Intuition and proponents of certification schemes suggest that certification leads to an increase of income. If certification negatively impacts income, it would call into question a significant effort to engage in certification and fair trade practices.</p>
<p><span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span> does not perform any discussion or consideration of balance in their paper and so it is unclear if propensity score matching results in covariate balance. The <code>cobalt</code> package creates balance tables using <code>bal.tab()</code> and a visualisation using <code>love.plot()</code>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-coffee-rep-kbl">Table #</a></summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_rep_pmodel, </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab_ss <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Observations</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Household Age"</span>, <span class="st">"Squared Household Age"</span>, <span class="st">"Non-farm Income Access"</span>, </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Log Total Land"</span>, <span class="st">"Dependency Ratio"</span>, <span class="st">"Bad Weather"</span>,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Education Level"</span>, <span class="st">"Gender"</span>, <span class="st">"Years of Coffee Production"</span>, </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Access to Credit"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>colnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Variable"</span>,<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balance Threshold"</span>, <span class="st">"Variance Ratio"</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_rep_btab) <span class="ot">&lt;-</span> rowlabels</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>          coffee_rep_btab[,<span class="dv">3</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-rep-kbl" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-rep-kbl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.5: PALCHOLDER
</figcaption>
<div aria-describedby="tbl-coffee-rep-kbl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Variable</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Type</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SMD</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Balance Threshold</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Household Age</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.272</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.073</td>
</tr>
<tr class="even">
<td style="text-align: left;">Squared Household Age</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.255</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.143</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Non-farm Income Access</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="even">
<td style="text-align: left;">Log Total Land</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.297</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dependency Ratio</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.400</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.979</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bad Weather</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Education Level</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">1.034</td>
</tr>
<tr class="even">
<td style="text-align: left;">Gender</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">-0.132</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Years of Coffee Production</td>
<td style="text-align: center;">Contin.</td>
<td style="text-align: center;">-0.340</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.911</td>
</tr>
<tr class="even">
<td style="text-align: left;">Access to Credit</td>
<td style="text-align: center;">Binary</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#fig-coffee-replication-lplot">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add render info for showtext to yaml. also chang legend to be more informative.  </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>, <span class="at">thresholds=</span><span class="fl">0.1</span>, <span class="at">s.d.denom=</span><span class="st">"treated"</span>) <span class="sc">+</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance"</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-coffee-replication-lplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coffee-replication-lplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-coffee-replication-lplot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coffee-replication-lplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: PLACEHOLDER
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#tbl-coffee-rep-kbl" class="quarto-xref">Table&nbsp;<span>3.5</span></a> and <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> show that propensity score matching has obtained very poor balance. Based on the <span class="math inline">\(10\%\)</span> rule discussed in <span class="citation" data-cites="sec">(<a href="#ref-sec" role="doc-biblioref"><strong>sec?</strong></a>)</span>-, not a single variable is balanced and so the estimate of the treatment effect is likely to be biased by structural differences between control and certified.</p>
<p>Four key variables: <em>age</em>, <em>gender</em>, <em>education</em>, and <em>access to credit</em> all exhibit poor balance. These variables are strong confounders in theory and so emphasising balance in these variables is critical to making a robust causal inference. Perhaps there is gender or age discrimination in the certification process. Perhaps, those with lesser education may struggle to obtain certification. Perhaps those who have less access to credit are unable to afford to become certified. Moving forward, these variables must exhibit better covariate balance to make a robust conclusion.</p>
<p><a href="#fig-replication-pscore" class="quarto-xref">Figure&nbsp;<span>3.5</span></a> shows the effect of common support trimming. <a href="#tbl-coffee-replication-ss" class="quarto-xref">Table&nbsp;<span>3.6</span></a> shows 34 total observations are dropped of which 33 are treated and 1 are control. By dropping these observations, PSM avoids making poor matching which should lead to better covariate balance. When observations are discarded, the estimand is no longer the ATT. Instead, it is refereed to as the average treatment effect on the matched or ATM. There is a significant reduction in the effective sample size in the control group from <span class="math inline">\(82\)</span> to <span class="math inline">\(21\)</span> individuals.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#fig-replication-pscore">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>discarded_scores <span class="ot">&lt;-</span> coffee_rep_pmodel<span class="sc">$</span>distance[coffee_rep_pmodel<span class="sc">$</span>discarded]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>discard_min <span class="ot">&lt;-</span> <span class="fu">min</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>discard_max <span class="ot">&lt;-</span> <span class="fu">max</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(coffee_data, <span class="fu">aes</span>(<span class="at">x =</span> coffee_rep_pmodel<span class="sc">$</span>distance, <span class="at">fill =</span> <span class="fu">factor</span>(certified))) <span class="sc">+</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Certified"</span>)) <span class="sc">+</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Distribution of Propensity Scores in @Jena2012"</span>, </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_min, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_max, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> <span class="dv">0</span>, <span class="at">xmax =</span> discard_min, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> discard_max, <span class="at">xmax =</span> <span class="dv">1</span>, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.02</span>, <span class="at">y =</span> <span class="fl">2.5</span>, </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>           <span class="at">label =</span> <span class="st">"Discarded Range"</span>, <span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">1.5</span>, <span class="at">size=</span><span class="dv">4</span>,<span class="at">fontface=</span><span class="st">"bold"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>) <span class="sc">+</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-replication-pscore" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-replication-pscore-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-replication-pscore-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-replication-pscore-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: PLACEHOLDER
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-coffee-replication-ss">Table #</a></summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab_ss, <span class="at">digits=</span><span class="dv">0</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span>F)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-replication-ss" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-replication-ss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.6: Placeholder
</figcaption>
<div aria-describedby="tbl-coffee-replication-ss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Control</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Treated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">All (ESS)</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">164</td>
</tr>
<tr class="even">
<td style="text-align: left;">All (Unweighted)</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">164</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Matched (ESS)</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">131</td>
</tr>
<tr class="even">
<td style="text-align: left;">Matched (Unweighted)</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">131</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Unmatched</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Discarded</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">33</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p>Overall, the propensity score matching in <span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span> is poor and results in unbalanced covariates and a loss of estimand.</p>
</section>
<section id="further-modelling" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="further-modelling"><span class="header-section-number">3.5.2</span> Further Modelling</h3>
<p>To improve the poor balance achieved by the <span class="citation" data-cites="Jana2012">(<a href="#ref-Jana2012" role="doc-biblioref"><strong>Jana2012?</strong></a>)</span>, there are two strategies to obtain better balance. First, the propensity scores can be re-estimated using machine learning to obtain better calibrated propensity scores. Second, inverse propensity weighting (IPW) can be used instead of propensity score matching (PSM). IPW should ensure that the sample size remains the same as no observations are lost through a matching process. IPW should retain all observations and preserve the estimand as the ATT. Additionally, IPW is generally more efficient as a pseudo-population is based on precise weights compared to matched observations that are based on approximate similarity.</p>
<p>The machine learning propensity scores will be estimated using the <code>WeightIt</code> package in the same process as <span class="quarto-unresolved-ref">?sec-demo</span>. To select the tuning criteria, consider that <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> that shows a significant range of balance levels in the raw data. Knowing this, the model is tuned using <code>criterion = “smd.max”</code> as reducing the priority is to reduce the extremely unbalanced covariates even if this leads to a higher average SMD.</p>
<div class="cell">
<details class="code-fold">
<summary>Show to Code to Fit the GBM model using <code>WeightIt</code> and <code>cobalt</code></summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>coffee_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data=</span>coffee_data, </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                                <span class="at">method=</span><span class="st">"gbm"</span>, <span class="at">distribution=</span><span class="st">"bernoulli"</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                                <span class="at">use.offset=</span><span class="fu">c</span>(T),</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>                                <span class="at">shrinkage=</span><span class="fu">seq</span>(<span class="fl">0.15</span>, <span class="fl">0.4</span>,<span class="at">length.out=</span><span class="dv">5</span>),</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>                                <span class="at">bag.fraction=</span><span class="fl">0.67</span>, </span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>                                <span class="at">interaction.depth=</span><span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>                                <span class="at">n.trees=</span><span class="dv">500</span>,</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>                                <span class="at">criterion=</span><span class="st">"smd.mean"</span>, </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand=</span><span class="st">"ATT"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_boosted_weight, </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance tabltune# Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> coffee_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Discussion of Tuning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion of Tuning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initially, a tuning grid considering shrinkage values of <span class="math inline">\(0.001,0.005,.01,0.05,0.1,\text{ and }0.2\)</span> were considered using <span class="math inline">\(10000\)</span> trees with a depth between <span class="math inline">\(1\)</span> and <span class="math inline">\(5\)</span>. The best tuning performance was found with shrinkage of <span class="math inline">\(0.2\)</span> and <span class="math inline">\(9\)</span> trees which were three splits <span class="math inline">\(3\)</span> deep. As such, the tuning grid was redefined in a second iteration to use <span class="math inline">\(0.1, 0.15, 0.2, 0.25, 0.3,0.35,\text{ and } 0.4\)</span> with only <span class="math inline">\(1000\)</span> trees with between <span class="math inline">\(2\)</span> and <span class="math inline">\(5\)</span> depth. The second fit, suggested a learning rate of <span class="math inline">\(0.35\)</span> so the local area of <span class="math inline">\(0.3, 0.325, 0.350, 0.375, \text{ and }0.4\)</span> is searched in the final fit.</p>
</div>
</div>
<p>Of course there is no guarantee that the GBM model will perform the best and so a logistic model is also fitted. An interesting comparison is between the SMDs in the matched data and in the weighted sample. Any differences between the two samples relates to the difference between PSM and IPW as the propensity scores are identical.</p>
<div class="cell">
<details class="code-fold">
<summary>Show to Code to Perform IPW with Logistic Regression.</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>coffee_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data =</span> coffee_data, <span class="at">method=</span> <span class="st">"glm"</span>,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand =</span> <span class="st">"ATT"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>coffee_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_logit_weight, </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                             <span class="at">formula =</span> coffee_formula,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>                             <span class="at">data =</span> coffee_data, </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>                             <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>                             <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                             <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                             <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>coffee_logit_btab <span class="ot">&lt;-</span> coffee_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="comparison-of-methods" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="comparison-of-methods"><span class="header-section-number">3.5.3</span> Comparison of Methods</h3>
<p>In some of the earlier code chucnks, the <code>cobalt</code> package’s <code>bal.tab()</code> computed balance tables which are combined together in <a href="#tbl-coffee-comparison" class="quarto-xref">Table&nbsp;<span>3.7</span></a> to provide a comparison between methods. For a visual interpretation, <code>love.plot()</code> creates <a href="#fig-coffee-love-all" class="quarto-xref">Figure&nbsp;<span>3.6</span></a>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code Prepairing the Balance Table for Presentation</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"data.table"</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_formula, </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> coffee_raw_btab<span class="sc">$</span>Balance[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> <span class="fu">rbindlist</span>(<span class="fu">list</span>(coffee_raw_btab,</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>                                       coffee_logit_btab,</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>                                       coffee_boosted_btab), <span class="at">use.names=</span><span class="cn">FALSE</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> coffee_combined_btab[,<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)]</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>          coffee_combined_btab[,<span class="dv">4</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-coffee-comparison">Table #</a>.</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_combined_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold=</span><span class="cn">TRUE</span>, <span class="at">width=</span><span class="st">"5cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold=</span><span class="cn">FALSE</span>, <span class="at">width=</span><span class="st">"1cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Raw Data"</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Logistic Regression and IPTW"</span>, <span class="dv">11</span>, <span class="dv">20</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Boosted Machine with IPTW"</span>, <span class="dv">21</span>, <span class="dv">30</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-comparison" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.7: Comparison of Balance for Coffee Data Using Different Propensity Models
</figcaption>
<div aria-describedby="tbl-coffee-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: center;" data-quarto-table-cell-role="th">Variable</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Type</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SMD</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Balance Threshold</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.563</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.865</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.491</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.007</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.393</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.405</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.551</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.049</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.237</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.250</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.002</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.727</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.275</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.456</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.362</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.597</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Logistic Regression and IPTW</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.245</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.927</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.228</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.072</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.170</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.092</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.856</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.114</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.388</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.194</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.047</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.922</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.046</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.061</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.112</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.029</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Boosted Machine with IPTW</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.067</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.269</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.099</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.491</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.058</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.028</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.876</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.062</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.766</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.191</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.138</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.073</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.082</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.006</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.970</td>
</tr>
<tr class="odd">
<td style="text-align: center; width: 5cm; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.123</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#fig-coffee-love-all">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Logit =</span> coffee_logit_weight,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Boosting=</span> coffee_boosted_weight),</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,<span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>, <span class="st">"darkblue"</span>,<span class="st">"darkred"</span>), </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>, <span class="st">"triangle"</span>, <span class="st">"diamond"</span>),</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>,<span class="at">thresholds=</span><span class="fl">0.1</span>,<span class="at">s.d.denom=</span><span class="st">"treated"</span>,<span class="at">use.grid=</span>F)<span class="sc">+</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance Using Different Balance Methods"</span>,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="sc">+</span> custom_ggplot_theme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-coffee-love-all" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coffee-love-all-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-coffee-love-all-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coffee-love-all-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Comparison of Balance for Coffee Data Using Different Methods
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are three notable findings:</p>
<ol type="1">
<li><p>PSM has performed very poorly relative to IPW even when matching dropps a significant number of observations.</p></li>
<li><p>A GBM model has resulted in better covariate balance than logistic regression for most covariates. Using a <span class="math inline">\(10\%\)</span> threshold for determining balance, logistic regression leaves <span class="math inline">\(5\)</span> variables unbalanced and the GBM leaves <span class="math inline">\(3\)</span> variables unbalanced. Additionally, the degree of unbalance is larger for logistic regression.</p></li>
<li><p>Logistic regress has a satisfactory average SMD of 0.0768603. Boosting has an average SMD of 0.0498114 which is excellent and meets a rigorous threshold of <span class="math inline">\(5\%\)</span>.</p></li>
<li><p>The covariate with the highest SMD is <em>household age</em> (<span class="math inline">\(0.245\%\)</span>) in logistic regression and <em>bad weather</em> (<span class="math inline">\(0.191\)</span>) in the GBM.</p></li>
</ol>
</section>
<section id="results" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="results"><span class="header-section-number">3.5.4</span> Results</h3>
<p>Now that satisfactory covariate balance is achieved, the treatment effect can be estimated under logistic regression, the GBM, and then compared to the result in the paper. Note that the estimand in the paper is intended to be the average treatment effect (ATT) but dropped observations mean the actual treatment effect is the average treatment effect on matched (ATM) individuals. In theory, better covariate balance should lead to a better estimate of the ATT so a comparison of the estimates is interesting. As in <a href="#sec-nsw-results" class="quarto-xref"><span>Section 3.4.3</span></a>, the results will be completed using G-computation with the <code>lm_weightit()</code> and <code>avg_comparisons()</code> functions.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="#tbl-comparison-coffee-results">Figure #</a></summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>coffee_att_formula <span class="ot">&lt;-</span> <span class="fu">update.formula</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, <span class="fu">paste</span>(<span class="fu">attr</span>(<span class="fu">terms</span>(coffee_formula), <span class="st">"term.labels"</span>), <span class="at">collapse =</span> <span class="st">" + "</span>))), percapitaincome_day_maleeq <span class="sc">~</span> certified <span class="sc">*</span> .)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>coffee_logit_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_logit_weight)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>coffee_boosted_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_boosted_weight)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>coffee_logit_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_logit_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>coffee_boosted_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_boosted_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>coffee_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(replicated_result_tbl, <span class="fu">extract_comparison_results</span>(coffee_logit_att),</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(coffee_boosted_att))</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Rep. Result (Logistic with PSM)"</span>,<span class="st">"Logistic Regression and IPW"</span>, <span class="st">"Generalized Boosting Machine and IPW"</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_comparisons_tab, <span class="at">digits =</span> <span class="dv">4</span>,<span class="at">booktabs =</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-comparison-coffee-results" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comparison-coffee-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.8: PLACEHOLDER
</figcaption>
<div aria-describedby="tbl-comparison-coffee-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Rep. Result (Logistic with PSM)</td>
<td style="text-align: center;">-0.1538</td>
<td style="text-align: center;">0.7384</td>
<td style="text-align: center;">0.8350</td>
<td style="text-align: center;">-1.6009</td>
<td style="text-align: center;">1.2934</td>
</tr>
<tr class="even">
<td style="text-align: left;">Logistic Regression and IPW</td>
<td style="text-align: center;">-1.5824</td>
<td style="text-align: center;">0.6072</td>
<td style="text-align: center;">0.0092</td>
<td style="text-align: center;">-2.7724</td>
<td style="text-align: center;">-0.3924</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Generalized Boosting Machine and IPW</td>
<td style="text-align: center;">-1.0187</td>
<td style="text-align: center;">0.5196</td>
<td style="text-align: center;">0.0499</td>
<td style="text-align: center;">-2.0372</td>
<td style="text-align: center;">-0.0003</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p><a href="#tbl-comparison-coffee-results" class="quarto-xref">Table&nbsp;<span>3.8</span></a> shows the estimates of the treatment effect across different methods. Recall that <span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span> estimate a an effect of <span class="math inline">\(-0.15\)</span> implying that daily income reduces by <span class="math inline">\(0.15\)</span> if a farmer becomes certified. This result is not statistically significant.</p>
<p>The IPW estimate is <span class="math inline">\(-1.58\)</span> implying that certification leads to a <span class="math inline">\(\$1.58\)</span> decrease in daily income. This coefficient is much larger than than the original paper by a magnitude of <span class="math inline">\(10\)</span>. Additionally, this estimate is statistically significant at the <span class="math inline">\(1\%\)</span> level. The GBM estimate is <span class="math inline">\(-1.02\)</span> which predicts a decrease in daily income by <span class="math inline">\(\$1.02\)</span> when a farmer becomes certified. This finding is statistically significant at the <span class="math inline">\(5\%\)</span> level.</p>
<p>The reason for a large difference is threefold. First, better covariate balance by using a GBM and IPW and should result in a more robust estimate. Of course better covariate balance alone does not guarantee robust results but it is a step in the right direction. Theoretically, weighting on the inverse of the propensity scores from a GBM results in the best estimate so the paper may significantly underestimate the coefficient. Second, a different estimand will often lead to a different estimate of the treatment effect. It is not clear or directly estimable how much of the difference in estimate results from switching from the ATM to the ATT. Related to this, the data used to estimate the treatment effect is different as there are no dropped observations either of the IPW estimates. Third…</p>
<p>The most interesting result is that the estimates become even more negative. One may expect that the result from a better balanced sample would become positive to align with theoretical motivations for cerification policies. A possible answer for why the estimand is negative is reverse causality which is suggested by @</p>
<p><span class="citation" data-cites="Jena2012">Jena et al. (<a href="#ref-Jena2012" role="doc-biblioref">2012</a>)</span> presented two explanations for why certification shows no positive impact. Firstly, the authors note that the prices offered by certified cooperatives are not significantly different from those provided by non-certified cooperatives. Secondly, a substantial portion of coffee—about 75%—is sold to private traders, who often pay higher prices to non-certified farmers. Additionally, from qualitative interviews with farmers, the authors note that policies and arrangements within different cooperatives exhibit heterogeneity so the impact of certification may relate more to the structure of the cooperatives.</p>
<p>An additional answer is the impact of reverse causality. A general problem is causal inference is that the direction of causality is not always known. While it is most intuitive that coffee certification would impact income, it is also possible that a farmers daily income might determine their certification. Suppose that proponents of fairtrade and certification are correct that it will increase income and benefit livelihood. If farmers are are of this, then perhaps the lowest income farmers are most likely to attempt to become certified to increase their income. Additionally, income likely has a reverse causal relationship with many of the explanatory variables. For example, a higher income may lead to better access to credit and the accumulation of land.</p>
<p>a final concluding remark.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Austin2008" class="csl-entry" role="listitem">
Austin, Peter C. 2008. <span>“<span class="nocase">A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003</span>.”</span> <em>Statistics in Medicine</em> 27 (April): 2037–49. <a href="https://doi.org/10.1002/sim.3150">https://doi.org/10.1002/sim.3150</a>.
</div>
<div id="ref-Bader2019" class="csl-entry" role="listitem">
Bader-El-Den, Mohammed, Eleman Teitei, and Todd Perry. 2019. <span>“<span class="nocase">Biased Random Forest for Dealing with the Class Imbalance Problem</span>.”</span> <em>IEEE Transactions on Neural Networks and Learning Systems</em> 30 (7): 2163–72. <a href="https://doi.org/10.1109/TNNLS.2018.2878400">https://doi.org/10.1109/TNNLS.2018.2878400</a>.
</div>
<div id="ref-Brookhart2006" class="csl-entry" role="listitem">
Brookhart, M. Alan, Sebastian Schneeweiss, Kenneth J. Rothman, Robert J. Glynn, Jerry Avorn, and Til Stürmer. 2006. <span>“<span class="nocase">Variable selection for propensity score models</span>.”</span> <em>American Journal of Epidemiology</em> 163 (12): 1149–56. <a href="https://doi.org/10.1093/aje/kwj149">https://doi.org/10.1093/aje/kwj149</a>.
</div>
<div id="ref-Cannas2019" class="csl-entry" role="listitem">
Cannas, Massimo, and Bruno Arpino. 2019. <span>“<span class="nocase">A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting</span>.”</span> <em>Biometrical Journal</em> 61 (4): 1049–72. <a href="https://doi.org/10.1002/bimj.201800132">https://doi.org/10.1002/bimj.201800132</a>.
</div>
<div id="ref-C5Mixtape2021" class="csl-entry" role="listitem">
Cunningham, Scott. 2021. <span>“<span class="nocase">Matching and Subclassification</span>.”</span> In <em>Causal Inference: The Mixtape</em>, 175–240. Yale University Press. <a href="https://doi.org/10.2307/j.ctv1c29t27.8">https://doi.org/10.2307/j.ctv1c29t27.8</a>.
</div>
<div id="ref-Ferri2020" class="csl-entry" role="listitem">
Ferri-García, Ramón, and María Del Mar Rueda. 2020. <span>“<span class="nocase">Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys</span>.”</span> <em>PLoS ONE</em> 15 (4): 1–19. <a href="https://doi.org/10.1371/journal.pone.0231500">https://doi.org/10.1371/journal.pone.0231500</a>.
</div>
<div id="ref-Friedman2001" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“<span>Greedy Function Approximation: A Gradient Boosting Machine</span>.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232. <a href="https://www.jstor.org/stable/2699986">https://www.jstor.org/stable/2699986</a>.
</div>
<div id="ref-Goller2020" class="csl-entry" role="listitem">
Goller, Daniel, Michael Lechner, Andreas Moczall, and Joachim Wolff. 2020. <span>“<span class="nocase">Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed</span>.”</span> <em>Labour Economics</em> 65 (March). <a href="https://doi.org/10.1016/j.labeco.2020.101855">https://doi.org/10.1016/j.labeco.2020.101855</a>.
</div>
<div id="ref-Heinrich2010" class="csl-entry" role="listitem">
Heinrich, Carolyn. 2010. <span>“<span class="nocase">A Primer for Applying Propensity-Score Matching</span>.”</span> <em>Development</em>, no. August: 59. <a href="http://www.iadb.org/document.cfm?id=35320229">http://www.iadb.org/document.cfm?id=35320229</a>.
</div>
<div id="ref-Jena2012" class="csl-entry" role="listitem">
Jena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. <span>“<span class="nocase">The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia</span>.”</span> <em>Agricultural Economics (United Kingdom)</em> 43 (4): 429–40. <a href="https://doi.org/10.1111/j.1574-0862.2012.00594.x">https://doi.org/10.1111/j.1574-0862.2012.00594.x</a>.
</div>
<div id="ref-King2019" class="csl-entry" role="listitem">
King, Gary, and Richard Nielsen. 2019. <span>“<span class="nocase">Why Propensity Scores Should Not Be Used for Matching</span>.”</span> <em>Political Analysis</em> 27 (4): 435–54. <a href="https://doi.org/10.1017/pan.2019.11">https://doi.org/10.1017/pan.2019.11</a>.
</div>
<div id="ref-Lee2010" class="csl-entry" role="listitem">
Lee, Brian K., Justin Lessler, and Elizabeth A. Stuart. 2010. <span>“<span class="nocase">Improving propensity score weighting using machine learning</span>.”</span> <em>Statistics in Medicine</em> 29: 337–46. <a href="https://doi.org/10.1002/sim.3782">https://doi.org/10.1002/sim.3782</a>.
</div>
<div id="ref-McCaffrey2004" class="csl-entry" role="listitem">
McCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. <span>“<span class="nocase">Propensity score estimation with boosted regression for evaluating causal effects in observational studies</span>.”</span> <em>Psychological Methods</em> 9 (4): 403–25. <a href="https://doi.org/10.1037/1082-989X.9.4.403">https://doi.org/10.1037/1082-989X.9.4.403</a>.
</div>
<div id="ref-Naimi2017" class="csl-entry" role="listitem">
Naimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. 2017. <span>“<span class="nocase">An introduction to g methods</span>.”</span> <em>International Journal of Epidemiology</em> 46 (2): 756–62. <a href="https://doi.org/10.1093/ije/dyw323">https://doi.org/10.1093/ije/dyw323</a>.
</div>
<div id="ref-Olson2018" class="csl-entry" role="listitem">
Olson, Matthew A., and Abraham J. Wyner. 2018. <span>“<span class="nocase">Making Sense of Random Forest Probabilities: a Kernel Perspective</span>,”</span> 1–35. <a href="http://arxiv.org/abs/1812.05792">http://arxiv.org/abs/1812.05792</a>.
</div>
<div id="ref-Ridgeway2024" class="csl-entry" role="listitem">
Ridgeway, Greg, Dan Mccaffrey, Andrew Morral, Matthew Cefalu, Lane Burgette, and Beth Ann Griffin. 2024. <span>“<span class="nocase">Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package</span>.”</span> <a href="https://doi.org/10.7249/tl136.1">https://doi.org/10.7249/tl136.1</a>.
</div>
<div id="ref-Rosenbaum1983" class="csl-entry" role="listitem">
Rosenbaum, Paul R., and Donald B. Rubin. 1983. <span>“<span class="nocase">The central role of the propensity score in observational studies for causal effects</span>.”</span> <em>Biometrika</em> 70 (1): 41–55. <a href="https://doi.org/10.1017/CBO9780511810725.016">https://doi.org/10.1017/CBO9780511810725.016</a>.
</div>
<div id="ref-Schuster2016" class="csl-entry" role="listitem">
Schuster, Tibor, Wilfrid Kouokam Lowe, and Robert W. Platt. 2016. <span>“<span class="nocase">Propensity score model overfitting led to inflated variance of estimated odds ratios</span>.”</span> <em>Journal of Clinical Epidemiology</em> 80: 97–106. <a href="https://doi.org/10.1016/j.jclinepi.2016.05.017">https://doi.org/10.1016/j.jclinepi.2016.05.017</a>.
</div>
<div id="ref-Setoguchi2008" class="csl-entry" role="listitem">
Setoguchi, Soko, Sebastian Schneeweiss, Alan M. Brookhart, Robert J. Glynn, and Francis E. Cook. 2008. <span>“<span class="nocase">Evaluating uses of data mining techniques in propensity score estimation: a simulation study</span>.”</span> <em>Pharmacoepidemiology and Drug Safety</em> 17 (March): 546–55. <a href="https://doi.org/10.1002/pds">https://doi.org/10.1002/pds</a>.
</div>
<div id="ref-Smith2005" class="csl-entry" role="listitem">
Smith, Jeffrey A., and Petra E. Todd. 2005. <em><span class="nocase">Does matching overcome LaLonde’s critique of nonexperimental estimators?</span></em> Vol. 125. 1-2 SPEC. ISS. <a href="https://doi.org/10.1016/j.jeconom.2004.04.011">https://doi.org/10.1016/j.jeconom.2004.04.011</a>.
</div>
<div id="ref-Tu2019" class="csl-entry" role="listitem">
Tu, Chunhao. 2019. <span>“<span class="nocase">Comparison of various machine learning algorithms for estimating generalized propensity score</span>.”</span> <em>Journal of Statistical Computation and Simulation</em> 89 (4): 708–19. <a href="https://doi.org/10.1080/00949655.2019.1571059">https://doi.org/10.1080/00949655.2019.1571059</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note that tuning <span class="math inline">\(mtry\)</span> for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In this context, the coverage is the proportion of times that the true treatment effect is within the <span class="math inline">\(95\%\)</span> confidence interval across the number of simulations. This author implements <span class="math inline">\(1000\)</span> simulations of each scenario.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <code>twang</code> and <code>gbm</code> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benefits discussed in <a href="#sec-gbm" class="quarto-xref"><span>Section 3.2.3</span></a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="#ref-Goller2020" role="doc-biblioref">2020</a>)</span> calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the <em>true</em> treatment effect. Thus, the covariate balance table offers a clearer view of each method’s performance.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./background.html" class="pagination-link" aria-label="Background: Causal Inference and Machine Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background: Causal Inference and Machine Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./metalearners.html" class="pagination-link" aria-label="Meta-Learners">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Meta-Learners</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb22" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="in">load(file = "my_environment.RData")</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- To-do:   --&gt;</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - Finish coffee data example --&gt;</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - rework the loss function theory stuff --&gt;</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - change code to quarto formatting --&gt;</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - perhaps remove the nsw example here --&gt;</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - change the tutorial code to be the replication study --&gt;</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - exposed vs treated (using treatment here). --&gt;</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make quarto and ggplot themes consistent. also changing colors of note callouts.  --&gt;</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make sure the link here says chapter not section --&gt;</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - better intro that explains a probability machine.  --&gt;</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - round off the structure and ensure header labels are consistent.  --&gt;</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - ml background to be transferred/written up to the background chapter. cite in this. --&gt;</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - state generally about how classification is binary and give examples of how ml is used for this. then transition it all a lot better.  --&gt;</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - more organised comparison of simulation results --&gt;</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - clarify the gini splitting vs accuracy loss function for rf/bag --&gt;</span></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- add rf and bagging to reduce words. replace all.  --&gt;</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ensure that covariate balance measures are noted and that there is a clear flow down to the simulation settting where balance is discussed.  --&gt;</span></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make some notes in the application that say why im not comparing balance across polynomicals. perhaps addd these to an appendix somewhere.   --&gt;</span></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- cite r packages --&gt;</span></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- add some comments about sdm thresholds in the intro --&gt;</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="fu"># Propensity Scores with Machine Learning {#sec-propensity}</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Conventional Approach: Propensity Scores and Balance</span></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. For similar groups, the average treatment effect (ATT) is a contrast of means from @eq-ate-estimate. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, @eq-ate-estimate leads to a biased estimate of the treatment effect.</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>For example, consider the causal question: *"How much does obtaining a bachelors degree increase lifetime earnings?"*. Individuals who complete a bachelor's degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the *cause* of higher incomes when the effect is confounded by the differences in covariates between groups. Recall that @fig-dag-confounder shows a confounding relationship. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of *covariate balance* which is a key concept behind underlying propensity score methods.</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>::: {#nte-balance-intution .callout-note title="What is Covariate Balance"}</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate are the same for each group, then those covariates are *balanced*. If covariates are similar across groups, then there should not be any confounding. Equally, similar covariates across groups implies exchangability between groups as the two groups should be similar (thus can be exchanged). There is a conceptual equivalence between covariate balance, unconfoundedness, and exchangeability meaning that @eq-independence is satisfied when covariates are balanced. </span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>In bachelor's degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parent income, geographic residence etc. Comparing the balanced matched pairs should result in a robust estimate of a bachelor's degree's impact on earnings because the individuals are exchangeable and satisfy @eq-conditional-independence. The covariates are said to be "conditioned on" by matching individuals on these covariates. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a *dimensionality* problem as the dimension of the number of covariates increases.</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>@Rosenbaum1983 offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as $e(X)$ and be expressed as:</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>e(X)=P(T=1|X).</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pscore}</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>A prediction of the probability of treatment based on the covariates is the best summary of each covariate. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects. </span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in @eq-conditional-independence. There are many sources that offer a comprehensive guide to propensity score methods such as <span class="co">[</span><span class="ot">@C5Mixtape2021, Chapter 4</span><span class="co">]</span> who provides applications and coded examples in R, Python, and Stata.</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>::: {#nte-balance-pscore .callout-note title="Balance and Propensity Scores"}</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>Note that an RCT will satisfy @eq-independence as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy @eq-conditional-independence as the potential outcomes are independent of the treatment status conditioned on some covariates. </span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a>The propensity score is conditioned on the covariates because the covariates are the predictors of the propensity score. Conditioning on the propensity score aims to replicate an RCT in observational data by balancing covariates between groups. When observations are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates.</span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>At a high level, the conditioned property of the propensity score is translated into a model by using PSM or IPW. @King2019 provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Propensity Score Modelling with Logistic Regression</span></span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a>A conventional propensity score model uses logistic regression to predict a probability between $0$ and $1$. Models may be specified to include interaction terms and polynomial terms so the model captures complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is a heuristically driven art rather than  science. <span class="co">[</span><span class="ot">@Brookhart2006; @Heinrich2010</span><span class="co">]</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are not statistically significant. Notably, many researchers do not discuss the specification of propensity models in their papers. @Austin2008 reviews $47$ papers that use propensity scores and find few perform adequate model selection, assess balance, or apply correct statistical tests.</span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>It is important to note that the true propensity score is never observable. A propensity score that is close to the theoretically true probability is well calibrated. Poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. Propensity scores may be poorly calibrated as covariates may be omitted by error, poorly measured, or be unobservable. Logistic regression may not predict calibrated scores if the true relationship is non-linear or involves complex interactions between covariates. Another important note is that the propensity model itself does not have an informative *causal* interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for each variable which is not informative of the desired estimand.</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probability Machines: Probability Theory and Machine Learning</span></span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a>Probability prediction is not a typical machine learning task. Supervised machine learning usually focuses on classifying observations into groups, or predicting continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability an observation belongs to a certain class.Machine learning methods that predict probabilities are sometimes called probability machines. </span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a>Probability machines are valuable in applications requiring calibrated probability predictions. For example, probability machines can predict loan defaults or other adverse events in finance. In marketing, they estimate the likelihood of customer response to a campaign. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a>Probability machines offer many advantages over parametric methods like logistic regression:</span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**Improved Calibration**: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-98"><a href="#cb22-98" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>**Flexible Modelling**: Unlike parametric methods like logistic regression, probability machines don't rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</span>
<span id="cb22-99"><a href="#cb22-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-100"><a href="#cb22-100" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>**Efficient Feature Selection**: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</span>
<span id="cb22-101"><a href="#cb22-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-102"><a href="#cb22-102" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>**Handling Missing Data**: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</span>
<span id="cb22-103"><a href="#cb22-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-104"><a href="#cb22-104" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>**Simplified Data Exploration**: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</span>
<span id="cb22-105"><a href="#cb22-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-106"><a href="#cb22-106" aria-hidden="true" tabindex="-1"></a>In causal inference, probability machines can predict better calibrated propensity scores and better estimate treatment effects. This discussion aims to clarify the use of probability machines in causal inference given the unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</span>
<span id="cb22-107"><a href="#cb22-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-108"><a href="#cb22-108" aria-hidden="true" tabindex="-1"></a>Please note that this chapter assumes a reader is familiar with *CART (Classification and Regression Tree)*, *Boosting*, *Bagging (Bootstrap Aggregation)*, *Random Forests*, *LASSO (Least Absolute Shrinkage and Selection Operator)*, and *Logistic Regression*. These methods are briefly discussed in @sec-background-ml.</span>
<span id="cb22-109"><a href="#cb22-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-110"><a href="#cb22-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choice of Loss Function and Probability Prediction</span></span>
<span id="cb22-111"><a href="#cb22-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-112"><a href="#cb22-112" aria-hidden="true" tabindex="-1"></a>The loss function measures the difference between a model's predictions and the actual target values, serving as a measure of a model's performance. The best model exists at the minimum of the loss function. In standard least squares regression from @sec-background-linear, the loss function is the residual sum of squares stated as $\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$. This loss function states the model must reduce the squared differences between the observed and predicted values. Different loss functions influence a model's behaviour so the choice of loss function is important.</span>
<span id="cb22-113"><a href="#cb22-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-114"><a href="#cb22-114" aria-hidden="true" tabindex="-1"></a>Classification models predict the category that each observation belongs to not the probability of each class. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Many classification loss functions minimize classification errors and improve accuracy as this results in the best classification. A loss function like the Gini index (introduced in @sec-background-cart) is effective for classification problems but it is unclear if this applies to probability problems. In other words, minimizing misclassification error may not lead to accurate probability predictions.</span>
<span id="cb22-115"><a href="#cb22-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-116"><a href="#cb22-116" aria-hidden="true" tabindex="-1"></a>At a high level, to classify an observation, $x_i$ as an $A$ or $B$, a model needs to determine if $\Pr(x_i=A)$ is less than or greater than $0.5$. If $\widehat{\Pr}(x_i=A) &gt; 0.5$, then it is more likely to be an $A$ and if $\widehat{\Pr}(x_i=A) &lt;0.5$ then it is more likely to be a $B$. Thus, if $x_i$ is an $A$ it is trivial if $\widehat{\Pr}(x_i=A)$ is $0.51$ or $0.99$ as this makes no difference to the classification as an $A$. But the difference between $\widehat{\Pr}(x_i=A) = 0.51$ and $0.99$ is extreme for a probability machine. It is important to understand that classification models are optimized for classification accuracy rather than probability prediction. This distinction affects the performance of ensemble methods like random forests or bagging ensembles that use classification trees for probability prediction.</span>
<span id="cb22-117"><a href="#cb22-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-118"><a href="#cb22-118" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bagging and Random Forest as Probability Machines {#sec-bagg-rf-probmachines}</span></span>
<span id="cb22-119"><a href="#cb22-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-120"><a href="#cb22-120" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- todo:  --&gt;</span></span>
<span id="cb22-121"><a href="#cb22-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-122"><a href="#cb22-122" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - reference to appendix --&gt;</span></span>
<span id="cb22-123"><a href="#cb22-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-124"><a href="#cb22-124" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - additional reserach for nsw inclding the cite ther.  --&gt;</span></span>
<span id="cb22-125"><a href="#cb22-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-126"><a href="#cb22-126" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - section reference --&gt;</span></span>
<span id="cb22-127"><a href="#cb22-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-128"><a href="#cb22-128" aria-hidden="true" tabindex="-1"></a>Across an entire bagging or random forest ensemble, class probabilities are determined through a *vote count* method. Within that ensemble, each tree makes a class prediction based on the majority class in a terminal node. For instance, if $x_i$ lies in a terminal node where $80\%$ of the observations are classified as an $A$, that *individual tree* will classify $x_i$ as $A$. The ensemble's overall prediction for $x_i$ is derived from the proportion of trees that classify $x_i$ as $A$ or $B$. Thus the ensemble *counts votes* for each class across the ensemble. Let $T$ be the total number of trees and $b_t$ be the $t$-th tree in the ensemble. Let $\mathbb{I}(b_t(x_i) = A)$ be the indicator function that returns $1$ when $b_t$ predicts that observation $x_i$ belongs to class $A$. The probability of class $A$ for observation $x_i$ is calculated as:</span>
<span id="cb22-129"><a href="#cb22-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-130"><a href="#cb22-130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-131"><a href="#cb22-131" aria-hidden="true" tabindex="-1"></a>\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A). </span>
<span id="cb22-132"><a href="#cb22-132" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ensemble-vote-method}</span>
<span id="cb22-133"><a href="#cb22-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-134"><a href="#cb22-134" aria-hidden="true" tabindex="-1"></a>@Olson2018 notes a potential bias towards predictions of $0$ or $1$ when trees in a bagged ensemble or random forest are highly correlated. Using a vote count method, an ensemble will bias predicted probabilities towards $\hat{P}(x_i=A) \in <span class="sc">\{</span>0,1<span class="sc">\}</span>$ when trees are highly correlated. Imagine that each tree in the ensemble is perfectly correlated implying that each tree will make the the same class prediction for each $x_i$. For such an ensemble, the predicted probabilities will will be exactly $0$ or $1$ using a vote count. Of course an ensemble of identical trees is unrealistic but the intuition still applies in the real world where ensembles may have some degree of correlation. The larger the correlation, the more the probability predictions will exhibit a *divergence bias* towards $0$ and $1$. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</span>
<span id="cb22-135"><a href="#cb22-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-136"><a href="#cb22-136" aria-hidden="true" tabindex="-1"></a>If $x_i$ has a known membership of $A$, and an unknown $\Pr_{\text{true}}(x_i=A) = 0.6$, the ensemble might classify $x_i$ correctly $90\%$ of the time leading to $\widehat{\Pr}(x_i=A) = 0.9$. As a probability machine, the ensemble has overestimated the probability by $0.3$ even though a $90\%$ classification accuracy is excellent. To predict $P_{\text{true}}(x_i=A) = 0.6$, an ensemble needs to incorrectly classify $x_i$ in $40\%$ of its trees. However, bagging ensembles and random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</span>
<span id="cb22-137"><a href="#cb22-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-138"><a href="#cb22-138" aria-hidden="true" tabindex="-1"></a>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected bootstrapped sample of the data. Random forests further reduce tree correlation by considering a random number of variables at each split, referred to as $mtry$. When $mtry$ is equal to to number of predictors, the model considers all variables at each split and the random forest is equal to a bagging ensemble. A lower $mtry$ should reduce the correlation between trees and decrease divergence bias as the structure of the tree is modified by the selected variables at each split. However, a lower $mtry$ also introduces other theoretical problems.</span>
<span id="cb22-139"><a href="#cb22-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-140"><a href="#cb22-140" aria-hidden="true" tabindex="-1"></a>Consider the scenario where the binary outcome (treatment and control) of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If $mtry$ is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. Each predictor has a chance of $\frac{mtry}{\text{number of predictors}}$ of selection at each split implying a lower $mtry$ decreases the chance of a splitting on the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of $x_i$ in each tree. Such an ensemble may have highly unstable probability predictions. </span>
<span id="cb22-141"><a href="#cb22-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-142"><a href="#cb22-142" aria-hidden="true" tabindex="-1"></a>Additionally, consider there is a class imbalance and the majority of observations are classified as $A$ not $B$. The terminal nodes of each tree within an ensemble are more likely to contain the majority class. Consequently, there is a *majority class bias* as each tree in the ensemble is more likely to misclassifying an observation as an $A$ because the terminal nodes have a higher proportion of $A$ due to the higher proportion of $A$'s in the data overall.</span>
<span id="cb22-143"><a href="#cb22-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-144"><a href="#cb22-144" aria-hidden="true" tabindex="-1"></a>::: {#nte-class-imbalance .callout-note title="Class Imbalance and Machine Learning"}</span>
<span id="cb22-145"><a href="#cb22-145" aria-hidden="true" tabindex="-1"></a>When there is a difference in the number of observations in each class, this is called class imbalance. It is important to note that majority class bias exists in conventional classification tasks with machine learning. Bagging ensembles and random forest are well known to be sensitive to class imbalance meaning that class predictions are biased towards the majority <span class="co">[</span><span class="ot">see @Bader2019</span><span class="co">]</span>.</span>
<span id="cb22-146"><a href="#cb22-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-147"><a href="#cb22-147" aria-hidden="true" tabindex="-1"></a>However, the class imbalance problem is particularly notable when predicting probabilities. The probability prediction from a vote count method is very sensitive to a change in the votes from each tree. Suppose that balanced data results in $80/100$ trees classifying $B$ as $B$ and imbalanced data (more $A$ than $B$) reduces correct classifications of $B$ to $60/100$. This results in a $20\%$ margin of error in probability estimates but the classification remains as $B$.</span>
<span id="cb22-148"><a href="#cb22-148" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-149"><a href="#cb22-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-150"><a href="#cb22-150" aria-hidden="true" tabindex="-1"></a>Individually, a low $mtry$ can lead to unstable probability predictions and class imbalance can create bias towards the majority class. But probability machines are particularly effected when there is both a low $mtry$ *and* class imbalance. Because successive noisy splits (relating to a low $mtry$) result in impure child nodes, the effects of majority class bias are exaggerated. Without the ability to separate the classes, the majority class will dominate terminal nodes. If the ensemble was able to split on informative covariates each time ($mtry$ is higher), then it should still be able to create pure splits even when there is some class imbalance. In other words, if there is a small class imbalance, reducing $mtry$ may reveal majority class bias not visible at higher $mtry$'s. Equally, if there is low $mtry$, then even a small class imbalance can lead to majority class bias.  </span>
<span id="cb22-151"><a href="#cb22-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-152"><a href="#cb22-152" aria-hidden="true" tabindex="-1"></a>To exemplify these theoretical points, the National Supported Work (NSW) programme is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with $445$ total participants, $185$ in the program group, and $260$ in the control group, so the true probability of treatment for each individual can be calculated as $185/445=0.42$ or $42\%$. Further information about this data is found in @sec-data-nsw-jobs.</span>
<span id="cb22-153"><a href="#cb22-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-154"><a href="#cb22-154" aria-hidden="true" tabindex="-1"></a>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although @fig-rf-varimp and @tbl-combined-btab suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by @Smith2005 who suggests that self-selection bias is prevalent in the NSW data.</span>
<span id="cb22-155"><a href="#cb22-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-156"><a href="#cb22-156" aria-hidden="true" tabindex="-1"></a>@fig-rf-theory-demo shows both divergence bias and majority class bias using <span class="in">`randomForest()`</span> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when $mtry$ is equal to the number of predictors and so specifying <span class="in">`mtry = 7`</span> in the <span class="in">`randomForest()`</span> function fits a bagging ensemble. Additionally, logistic regression using the <span class="in">`gbm()`</span> function provides a meaningful comparison.</span>
<span id="cb22-157"><a href="#cb22-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-160"><a href="#cb22-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-161"><a href="#cb22-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-162"><a href="#cb22-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-163"><a href="#cb22-163" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb22-164"><a href="#cb22-164" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-theory-demo</span></span>
<span id="cb22-165"><a href="#cb22-165" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-166"><a href="#cb22-166" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-167"><a href="#cb22-167" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](fig-rf-theory-demo)" </span></span>
<span id="cb22-168"><a href="#cb22-168" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Shows kernel density estimates of propensity scores for each controls and participants in the National Supported Work programme. `randomForest()` fits a random forest with `mtry = 1` and bagging ensemble with `mtry = 7`. Uses the default values of `ntree = 500` and `nodesize = 1`."</span></span>
<span id="cb22-169"><a href="#cb22-169" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb22-170"><a href="#cb22-170" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb22-171"><a href="#cb22-171" aria-hidden="true" tabindex="-1"></a>nsw_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">as.factor</span>(treat) <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> </span>
<span id="cb22-172"><a href="#cb22-172" aria-hidden="true" tabindex="-1"></a>                          black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> marr)</span>
<span id="cb22-173"><a href="#cb22-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-174"><a href="#cb22-174" aria-hidden="true" tabindex="-1"></a>logit_preds <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb22-175"><a href="#cb22-175" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="fu">binomial</span>())<span class="sc">$</span>fitted.values </span>
<span id="cb22-176"><a href="#cb22-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-177"><a href="#cb22-177" aria-hidden="true" tabindex="-1"></a>rf_mtry1_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">randomForest</span>(nsw_formula, </span>
<span id="cb22-178"><a href="#cb22-178" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">data =</span> nsw_data), </span>
<span id="cb22-179"><a href="#cb22-179" aria-hidden="true" tabindex="-1"></a>                          <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb22-180"><a href="#cb22-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-181"><a href="#cb22-181" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(nsw_formula, <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>, </span>
<span id="cb22-182"><a href="#cb22-182" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> nsw_data)</span>
<span id="cb22-183"><a href="#cb22-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-184"><a href="#cb22-184" aria-hidden="true" tabindex="-1"></a>bagged_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagging_model, <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb22-185"><a href="#cb22-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-186"><a href="#cb22-186" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-187"><a href="#cb22-187" aria-hidden="true" tabindex="-1"></a>plot_pmachines <span class="ot">&lt;-</span> <span class="cf">function</span>(pscores, plot_subtitle) {</span>
<span id="cb22-188"><a href="#cb22-188" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(nsw_data, <span class="fu">aes</span>(<span class="at">x =</span> pscores, <span class="at">fill =</span> <span class="fu">factor</span>(treat))) <span class="sc">+</span></span>
<span id="cb22-189"><a href="#cb22-189" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb22-190"><a href="#cb22-190" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb22-191"><a href="#cb22-191" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Participant"</span>)) <span class="sc">+</span></span>
<span id="cb22-192"><a href="#cb22-192" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">subtitle =</span> plot_subtitle, <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, </span>
<span id="cb22-193"><a href="#cb22-193" aria-hidden="true" tabindex="-1"></a>         <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb22-194"><a href="#cb22-194" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb22-195"><a href="#cb22-195" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb22-196"><a href="#cb22-196" aria-hidden="true" tabindex="-1"></a>    custom_ggplot_theme</span>
<span id="cb22-197"><a href="#cb22-197" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-198"><a href="#cb22-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-199"><a href="#cb22-199" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(logit_preds, <span class="st">"Logistic Regression"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb22-200"><a href="#cb22-200" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>) <span class="sc">+</span> </span>
<span id="cb22-201"><a href="#cb22-201" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb22-202"><a href="#cb22-202" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb22-203"><a href="#cb22-203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb22-204"><a href="#cb22-204" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb22-205"><a href="#cb22-205" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)</span>
<span id="cb22-206"><a href="#cb22-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-207"><a href="#cb22-207" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(rf_mtry1_preds, <span class="st">"Random Forest (mtry = 1)"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb22-208"><a href="#cb22-208" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb22-209"><a href="#cb22-209" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(bagged_preds, <span class="st">"Bagging (Bootstrap Aggregation)"</span>)</span>
<span id="cb22-210"><a href="#cb22-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-211"><a href="#cb22-211" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb22-212"><a href="#cb22-212" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">/</span> p2 <span class="sc">/</span> p3 <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb22-213"><a href="#cb22-213" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>)</span>
<span id="cb22-214"><a href="#cb22-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-215"><a href="#cb22-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-218"><a href="#cb22-218" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-219"><a href="#cb22-219" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-220"><a href="#cb22-220" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-221"><a href="#cb22-221" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-varimp</span></span>
<span id="cb22-222"><a href="#cb22-222" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-223"><a href="#cb22-223" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-224"><a href="#cb22-224" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](fig-rf-theory-demo)"</span></span>
<span id="cb22-225"><a href="#cb22-225" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The figure compares the variable importance assigned to each variable from a baggin ensemble. The data originates from the National Supported Work programme. The difference in relative important of some variables indicates that randomisation may not have created exchangability between the groups."</span></span>
<span id="cb22-226"><a href="#cb22-226" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-227"><a href="#cb22-227" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb22-228"><a href="#cb22-228" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb22-229"><a href="#cb22-229" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb22-230"><a href="#cb22-230" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb22-231"><a href="#cb22-231" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb22-232"><a href="#cb22-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-233"><a href="#cb22-233" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb22-234"><a href="#cb22-234" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb22-235"><a href="#cb22-235" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb22-236"><a href="#cb22-236" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb22-237"><a href="#cb22-237" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb22-238"><a href="#cb22-238" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb22-239"><a href="#cb22-239" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb22-240"><a href="#cb22-240" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb22-241"><a href="#cb22-241" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb22-242"><a href="#cb22-242" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb22-243"><a href="#cb22-243" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>,</span>
<span id="cb22-244"><a href="#cb22-244" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb22-245"><a href="#cb22-245" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb22-246"><a href="#cb22-246" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb22-247"><a href="#cb22-247" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"none"</span></span>
<span id="cb22-248"><a href="#cb22-248" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-249"><a href="#cb22-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-250"><a href="#cb22-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-251"><a href="#cb22-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-252"><a href="#cb22-252" aria-hidden="true" tabindex="-1"></a>@fig-rf-theory-demo shows the logistic regression model has identified a central tendency and most propensities are between $0.25$ and $0.75$ which roughly aligns with the true probability. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside $0.25$ and $0.75$ which is likely related to tree correlation. For the random forest with $mtry=1$, a significant number of the treatment and control observations are centred near the control area ($T=0$) with a wide range of other predictions. Recall that the control group is the majority class. Reducing $mtry$ from $7$ to $1$ reveals the majority class bias reinforcing the theoretical discussion that a combination of low $mtry$ and class imbalance is especially troubling. The models over predicts the majority class and has unstable predictions otherwise. Both random forest and bagging ensembles have performed poorly compared to the true probability of $0.42\%$. </span>
<span id="cb22-253"><a href="#cb22-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-254"><a href="#cb22-254" aria-hidden="true" tabindex="-1"></a>The tuning of $mtry$ faces double jeopardy and is another important area of discussion in probability machines. The selection of $mtry$ is typically completed in with a classification loss function such as accuracy or out-of-bag error. @Olson2018 compares tuning $mtry$ measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of $mtry$ for classification differs from probability prediction.<span class="ot">[^propensity-2]</span> In other words, if a grid search finds that $mtry=3$ is optimal for a classification task, this does not imply that $mtry=3$ is optimal for predicting probabilities so the tuning of $mtry$ is difficult. </span>
<span id="cb22-255"><a href="#cb22-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-256"><a href="#cb22-256" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-2]: </span>Note that tuning $mtry$ for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.</span>
<span id="cb22-257"><a href="#cb22-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-258"><a href="#cb22-258" aria-hidden="true" tabindex="-1"></a>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in @sec-mlps-sims. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</span>
<span id="cb22-259"><a href="#cb22-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-260"><a href="#cb22-260" aria-hidden="true" tabindex="-1"></a>Heuristically, divergence bias and majority class bias will most effect a probability machine when there is considerable overlap between groups. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common overlap may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low $mtry$, the terminal nodes of each tree may be relatively impure leading to a majority class bias. Alternatively, if true probabilities exist near $0$ or $1$ and there is a clear separation of class, divergence bias may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low $mtry$ may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</span>
<span id="cb22-261"><a href="#cb22-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-262"><a href="#cb22-262" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- maybe need a little chat about cross entropy here and why its not as good as in the gbm case.  --&gt;</span></span>
<span id="cb22-263"><a href="#cb22-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-264"><a href="#cb22-264" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Boosting Machines as Probability Machines {#sec-gbm}</span></span>
<span id="cb22-265"><a href="#cb22-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-266"><a href="#cb22-266" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- needs a little more comparison to rf and bagging. perhaps do some more reserach about why these are good. note and differentiate the different types of boosting. perhaps also clarify how the gradient descent works for my own learning (dont write it to be too technical).  --&gt;</span></span>
<span id="cb22-267"><a href="#cb22-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-268"><a href="#cb22-268" aria-hidden="true" tabindex="-1"></a>Moving beyond classification trees in random forests or bagging ensembles, @Friedman2001 introduced the *Gradient Boosting Machine* (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss function to reach a minimum. The update rule for the model after each iteration can be expressed as:</span>
<span id="cb22-269"><a href="#cb22-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-270"><a href="#cb22-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-271"><a href="#cb22-271" aria-hidden="true" tabindex="-1"></a>\hat{p}_i^{(t)} = \hat{p}_i^{(t-1)} + \lambda \cdot b_t(x_i), </span>
<span id="cb22-272"><a href="#cb22-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-273"><a href="#cb22-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-274"><a href="#cb22-274" aria-hidden="true" tabindex="-1"></a>where $\lambda$ is the learning rate, and $b_t(x_i)$ is the $b$-th regression tree fitted on the pseudo-residuals of the previous regression tree. In words, the current overall iteration $t$, is a combination of the previous model plus the current iteration scaled by a learning rate. </span>
<span id="cb22-275"><a href="#cb22-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-276"><a href="#cb22-276" aria-hidden="true" tabindex="-1"></a>A learning rate controls the contribution of each weak learner to the final model. By using a small learning rate, the machine learns slowly so that it can slowly descend the loss function. This allows for finer adjustments during the iterative process to better capture patterns in the data. </span>
<span id="cb22-277"><a href="#cb22-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-278"><a href="#cb22-278" aria-hidden="true" tabindex="-1"></a>GBMs can be generalized to many different applications by minimizing a different loss function which can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple regression trees and a logistic function to transform regression predictions into probabilities. Specifically, the logistic function used is:</span>
<span id="cb22-279"><a href="#cb22-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-280"><a href="#cb22-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-281"><a href="#cb22-281" aria-hidden="true" tabindex="-1"></a>\hat{p}_i = \frac{1}{1 + \exp(-\text{model}(x_i))}.</span>
<span id="cb22-282"><a href="#cb22-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-283"><a href="#cb22-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-284"><a href="#cb22-284" aria-hidden="true" tabindex="-1"></a>This logistic function is the same as in logistic regression, so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood with logistic regression. Maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The full GBM model, $f_T(x)$ after $T$ iterations can be written as:</span>
<span id="cb22-285"><a href="#cb22-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-286"><a href="#cb22-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-287"><a href="#cb22-287" aria-hidden="true" tabindex="-1"></a>f_T(x_i) = b_1(x_i) + \lambda \sum_{t=1}^{T} b_t(x_i).</span>
<span id="cb22-288"><a href="#cb22-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-289"><a href="#cb22-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-290"><a href="#cb22-290" aria-hidden="true" tabindex="-1"></a>Inside a base tree, each split considers all variables and makes the most informative split to descend the loss function using gradient descent. GBMs utilize many weak learners, such as a regression tree with a single split called a regression stump. However, additional splits enable the model to capture interactions between terms, which may increase probability calibration in complex or high-dimensional datasets.</span>
<span id="cb22-291"><a href="#cb22-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-292"><a href="#cb22-292" aria-hidden="true" tabindex="-1"></a>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques, as well as allowing a probability distribution-based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. Since GBMs predict probabilities from a logistic function, they avoid problems associated with a vote count method. Additionally, there are no difficult parameters to tune, such as $mtry$ in a random forest. The implementation and workflow to fit a GBM for propensity scores is discussed in @sec-gbm-tune-workflow.</span>
<span id="cb22-293"><a href="#cb22-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-294"><a href="#cb22-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-295"><a href="#cb22-295" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting</span></span>
<span id="cb22-296"><a href="#cb22-296" aria-hidden="true" tabindex="-1"></a>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting can prevent overfitting to ensure a model can generalise to unseen data.</span>
<span id="cb22-297"><a href="#cb22-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-298"><a href="#cb22-298" aria-hidden="true" tabindex="-1"></a>However, the model is not required to generalise a propensity score model as different datasets will have a different model. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</span>
<span id="cb22-299"><a href="#cb22-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-300"><a href="#cb22-300" aria-hidden="true" tabindex="-1"></a>::: {#nte-overfit-logistic .callout-note title="Overfitting in Logistic Regression"}</span>
<span id="cb22-301"><a href="#cb22-301" aria-hidden="true" tabindex="-1"></a>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is @Schuster2016, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards $0$ or $1$ because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to $0$ or $1$ receive artificially inflated weighting.</span>
<span id="cb22-302"><a href="#cb22-302" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-303"><a href="#cb22-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-304"><a href="#cb22-304" aria-hidden="true" tabindex="-1"></a>@Lee2010 simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</span>
<span id="cb22-305"><a href="#cb22-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-306"><a href="#cb22-306" aria-hidden="true" tabindex="-1"></a>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</span>
<span id="cb22-307"><a href="#cb22-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-308"><a href="#cb22-308" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparison of Machine Learning Algorithms: Simulation Results {#sec-mlps-sims}</span></span>
<span id="cb22-309"><a href="#cb22-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-310"><a href="#cb22-310" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- todo:  --&gt;</span></span>
<span id="cb22-311"><a href="#cb22-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-312"><a href="#cb22-312" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- fix early cite --&gt;</span></span>
<span id="cb22-313"><a href="#cb22-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-314"><a href="#cb22-314" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- clarify that sim studies are propensity score based. maybe look for general probability machine example sims.  --&gt;</span></span>
<span id="cb22-315"><a href="#cb22-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-316"><a href="#cb22-316" aria-hidden="true" tabindex="-1"></a>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="co">[</span><span class="ot">see @McCaffrey2004; @Setoguchi2008; @Lee2010; @Cannas2019; @Tu2019; @Goller2020; @Ferri2020</span><span class="co">]</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</span>
<span id="cb22-317"><a href="#cb22-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-318"><a href="#cb22-318" aria-hidden="true" tabindex="-1"></a>@Tu2019 compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</span>
<span id="cb22-319"><a href="#cb22-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-320"><a href="#cb22-320" aria-hidden="true" tabindex="-1"></a>Despite poor theoretical properties as a probability machine, @Lee2010 find that bagging results in the lowest standard error across many datasets.<span class="ot">[^propensity-3]</span> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</span>
<span id="cb22-321"><a href="#cb22-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-322"><a href="#cb22-322" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-3]: </span>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.</span>
<span id="cb22-323"><a href="#cb22-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-324"><a href="#cb22-324" aria-hidden="true" tabindex="-1"></a>Additionally, @Lee2010 finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best $95\%$ coverage in all simulation scenarios, with $98.6\%$ coverage.<span class="ot">[^propensity-4]</span></span>
<span id="cb22-325"><a href="#cb22-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-326"><a href="#cb22-326" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-4]: </span>In this context, the coverage is the proportion of times that the true treatment effect is within the $95\%$ confidence interval across the number of simulations. This author implements $1000$ simulations of each scenario.</span>
<span id="cb22-327"><a href="#cb22-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-328"><a href="#cb22-328" aria-hidden="true" tabindex="-1"></a>@Cannas2019 also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <span class="ot">[^propensity-5]</span> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</span>
<span id="cb22-329"><a href="#cb22-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-330"><a href="#cb22-330" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-5]: </span>@Cannas2019 provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <span class="in">`twang`</span> and <span class="in">`gbm`</span> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benefits discussed in @sec-gbm.</span>
<span id="cb22-331"><a href="#cb22-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-332"><a href="#cb22-332" aria-hidden="true" tabindex="-1"></a>@Goller2020 adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<span class="ot">[^propensity-6]</span> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</span>
<span id="cb22-333"><a href="#cb22-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-334"><a href="#cb22-334" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-6]: </span>@Goller2020 calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the *true* treatment effect. Thus, the covariate balance table offers a clearer view of each method's performance.</span>
<span id="cb22-335"><a href="#cb22-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-336"><a href="#cb22-336" aria-hidden="true" tabindex="-1"></a>Based on a review of the literature, the findings can be distilled into five important points:</span>
<span id="cb22-337"><a href="#cb22-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-338"><a href="#cb22-338" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</span>
<span id="cb22-339"><a href="#cb22-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-340"><a href="#cb22-340" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="co">[</span><span class="ot">@Rosenbaum1983</span><span class="co">]</span>.</span>
<span id="cb22-341"><a href="#cb22-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-342"><a href="#cb22-342" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Bagging ensembles perform poorly, a finding replicated across multiple studies.</span>
<span id="cb22-343"><a href="#cb22-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-344"><a href="#cb22-344" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</span>
<span id="cb22-345"><a href="#cb22-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-346"><a href="#cb22-346" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</span>
<span id="cb22-347"><a href="#cb22-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-348"><a href="#cb22-348" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</span>
<span id="cb22-349"><a href="#cb22-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-350"><a href="#cb22-350" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</span>
<span id="cb22-351"><a href="#cb22-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-352"><a href="#cb22-352" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implimentation and Hyperparameter Tuning with `WeightIt` and`gbm` in R</span></span>
<span id="cb22-353"><a href="#cb22-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-354"><a href="#cb22-354" aria-hidden="true" tabindex="-1"></a>Based on @Friedman2001, the <span class="in">`gbm`</span> package implements a *Generalized Boosting Machine*. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. Although this implementation very closely follows @Friedman2001 who introduced the gradient boosting machine. <span class="in">`gbm`</span> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <span class="in">`bag.fraction`</span> parameter.</span>
<span id="cb22-355"><a href="#cb22-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-356"><a href="#cb22-356" aria-hidden="true" tabindex="-1"></a>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <span class="in">`gbm`</span> package does not support this type of tuning, a wrapper package like <span class="in">`WeightIt`</span> is necessary. <span class="in">`WeightIt`</span> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <span class="in">`WeightIt`</span> supports hyperparameter turning of <span class="in">`shrinkage`</span>, <span class="in">`interaction.depth`</span>, and <span class="in">`n.trees`</span>. Once the best model is identified, propensity scores are predicted inside <span class="in">`WeightIt`</span>. These can be used inside <span class="in">`WeightIt`</span> to perform IPW or extracted for other implementations. <span class="in">`WeightIt`</span> also supports an offset meaning that logistic regression predictions are supplied to the <span class="in">`GBM`</span> package.</span>
<span id="cb22-357"><a href="#cb22-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-358"><a href="#cb22-358" aria-hidden="true" tabindex="-1"></a>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="co">[</span><span class="ot">see @McCaffrey2004; @Ridgeway2024</span><span class="co">]</span>. A very low learning rate, such as $0.01$ or $0.0005$, allows a smooth descent of the loss function. The model should include a high number of trees, with $10,000$ or $20,000$ being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for $3$ splits. Based on anecdotal experience, $1$ to $5$ splits per tree is optimal, consistent with recommendations by @McCaffrey2004.</span>
<span id="cb22-359"><a href="#cb22-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-360"><a href="#cb22-360" aria-hidden="true" tabindex="-1"></a>Another package, <span class="in">`twang`</span>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <span class="in">`caret`</span> must be used. Although <span class="in">`twang`</span> has other useful functionalities which users may wish to implement.</span>
<span id="cb22-361"><a href="#cb22-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-362"><a href="#cb22-362" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperparameter Tuning and Workflow {#sec-gbm-tune-workflow}</span></span>
<span id="cb22-363"><a href="#cb22-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-364"><a href="#cb22-364" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  --&gt;</span></span>
<span id="cb22-365"><a href="#cb22-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-366"><a href="#cb22-366" aria-hidden="true" tabindex="-1"></a>The <span class="in">`WeigthtIt`</span> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <span class="in">`cobalt`</span>. The best information for this package can be found on this <span class="co">[</span><span class="ot">website</span><span class="co">](https://ngreifer.github.io/WeightIt/index.html)</span> or accessed with <span class="in">`vignette("WeightIt")`</span> inside R after installation using <span class="in">`install.packages("WeightIt")`</span>.</span>
<span id="cb22-367"><a href="#cb22-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-368"><a href="#cb22-368" aria-hidden="true" tabindex="-1"></a>A workflow for hyperparameter tuning in <span class="in">`WeightIt`</span> may be completed as follows:</span>
<span id="cb22-369"><a href="#cb22-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-370"><a href="#cb22-370" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specify the <span class="in">`criterion`</span> option, which specifies the measure of the *”best model”*. The available options are the options that the <span class="in">`cobalt`</span> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <span class="in">`sdm.mean`</span> or the smallest maximum SDM across covariates called <span class="in">`sdm.max`</span>.</span>
<span id="cb22-371"><a href="#cb22-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-372"><a href="#cb22-372" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Set the number of trees high. The package default is <span class="in">`n.trees = 10000`</span> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <span class="in">`n.tree`</span> if optimal.</span>
<span id="cb22-373"><a href="#cb22-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-374"><a href="#cb22-374" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specify the grid search for the depth of the tree called <span class="in">`interaction.depth`</span> and the learning rate called <span class="in">`shrinkage`</span>. These values can be specified using <span class="in">`c()`</span> such as <span class="in">`shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)`</span> or as integers such as <span class="in">`interaction.depth = 1:5`</span>. These particular values are heuristically selected *suggestions* of good starting values. Additionally, an offset can be considered by performing a grid search across <span class="in">`offset=c(TRUE,FALSE)`</span>.</span>
<span id="cb22-375"><a href="#cb22-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-376"><a href="#cb22-376" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <span class="in">`my_weightit_object$info$best.tune`</span>.</span>
<span id="cb22-377"><a href="#cb22-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-378"><a href="#cb22-378" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <span class="in">`interaction.depth = 1:5`</span> and the best fit is $5$, then a new search can consider <span class="in">`interaction.depth = 3:7`</span> so that the local area around $5$ can be searched.</span>
<span id="cb22-379"><a href="#cb22-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-380"><a href="#cb22-380" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Experiment with <span class="in">`bag.fraction`</span>, which means each tree will consider a drawn proportion of observations equal to <span class="in">`bag.fraction`</span>. Iteratively changing <span class="in">`bag.fraction`</span> and assessing balance at each value should be practical. Consider $0.5$, $0.67$, and $1$.</span>
<span id="cb22-381"><a href="#cb22-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-382"><a href="#cb22-382" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <span class="in">`love.plot()`</span> such as @fig-coffee-replication-lplot.</span>
<span id="cb22-383"><a href="#cb22-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-384"><a href="#cb22-384" aria-hidden="true" tabindex="-1"></a><span class="ss">8.  </span>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</span>
<span id="cb22-385"><a href="#cb22-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-386"><a href="#cb22-386" aria-hidden="true" tabindex="-1"></a><span class="ss">9.  </span>Estimation and reporting of treatment effect.</span>
<span id="cb22-387"><a href="#cb22-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-388"><a href="#cb22-388" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: NSW Jobs Dataset Using R</span></span>
<span id="cb22-389"><a href="#cb22-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-390"><a href="#cb22-390" aria-hidden="true" tabindex="-1"></a>For demonstration, propensity scores are estimated following the workflow discussed in @sec-gbm-tune-workflow to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in @sec-data-nsw-jobs. Randomisation should eliminate structural differences between groups, but @Rosenbaum1983 notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <span class="in">`WeightIt`</span> and a logistic regression model using <span class="in">`glm()`</span>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</span>
<span id="cb22-391"><a href="#cb22-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-392"><a href="#cb22-392" aria-hidden="true" tabindex="-1"></a>::: {#nte-ipw .callout-note title="Inverse Probability of Treatment Weighting"}</span>
<span id="cb22-393"><a href="#cb22-393" aria-hidden="true" tabindex="-1"></a>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a *pseudo-population* where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</span>
<span id="cb22-394"><a href="#cb22-394" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-395"><a href="#cb22-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-396"><a href="#cb22-396" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 1-6: Model Fitting and Tuning</span></span>
<span id="cb22-397"><a href="#cb22-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-398"><a href="#cb22-398" aria-hidden="true" tabindex="-1"></a>The <span class="in">`glm()`</span> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <span class="in">`binomial()`</span>. Recall the <span class="in">`nsw_formula`</span> is specified in @sec-bagg-rf-probmachines</span>
<span id="cb22-399"><a href="#cb22-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-400"><a href="#cb22-400" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-401"><a href="#cb22-401" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_logit_pmodel</span></span>
<span id="cb22-402"><a href="#cb22-402" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb22-403"><a href="#cb22-403" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-404"><a href="#cb22-404" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false </span></span>
<span id="cb22-405"><a href="#cb22-405" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_pmodel &lt;- glm(nsw_formula, data = nsw_data,</span></span>
<span id="cb22-406"><a href="#cb22-406" aria-hidden="true" tabindex="-1"></a><span class="in">                        family=binomial()) #&lt;1&gt;</span></span>
<span id="cb22-407"><a href="#cb22-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-408"><a href="#cb22-408" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_pscores &lt;- nsw_logit_pmodel$fitted.values#&lt;2&gt;</span></span>
<span id="cb22-409"><a href="#cb22-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-410"><a href="#cb22-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-411"><a href="#cb22-411" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb22-412"><a href="#cb22-412" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<span id="cb22-413"><a href="#cb22-413" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>()) <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-414"><a href="#cb22-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-415"><a href="#cb22-415" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values<span class="co">#&lt;2&gt;</span></span>
<span id="cb22-416"><a href="#cb22-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-417"><a href="#cb22-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-418"><a href="#cb22-418" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Fits a logistic regression model using the <span class="in">`glm()`</span> function specified to be a logistic model with <span class="in">`family=binomial()`</span> using the previously created <span class="in">`nsw_formula`</span>.</span>
<span id="cb22-419"><a href="#cb22-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-420"><a href="#cb22-420" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Extracts the fitted values (propensity scores) from the model.</span>
<span id="cb22-421"><a href="#cb22-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-422"><a href="#cb22-422" aria-hidden="true" tabindex="-1"></a>Using the propensity score column of <span class="in">`nsw_data`</span>, the <span class="in">`WeightIt`</span> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <span class="in">`nsw_logit_weight`</span>.</span>
<span id="cb22-423"><a href="#cb22-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-426"><a href="#cb22-426" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-427"><a href="#cb22-427" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nsw_logit_weight</span></span>
<span id="cb22-428"><a href="#cb22-428" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb22-429"><a href="#cb22-429" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-430"><a href="#cb22-430" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-431"><a href="#cb22-431" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb22-432"><a href="#cb22-432" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-433"><a href="#cb22-433" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,<span class="co">#&lt;2&gt;</span></span>
<span id="cb22-434"><a href="#cb22-434" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)<span class="co">#&lt;3&gt;</span></span>
<span id="cb22-435"><a href="#cb22-435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-436"><a href="#cb22-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-437"><a href="#cb22-437" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb22-438"><a href="#cb22-438" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb22-439"><a href="#cb22-439" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-440"><a href="#cb22-440" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,<span class="co">#&lt;2&gt;</span></span>
<span id="cb22-441"><a href="#cb22-441" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)<span class="co">#&lt;3&gt;</span></span>
<span id="cb22-442"><a href="#cb22-442" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-443"><a href="#cb22-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-444"><a href="#cb22-444" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specifies the formula and data.</span>
<span id="cb22-445"><a href="#cb22-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-446"><a href="#cb22-446" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Provides <span class="in">`weightit()`</span> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <span class="in">`weightit()`</span> function with <span class="in">`method = "glm"`</span>. The separate estimation of the propensity scores is for illustrative purposes.</span>
<span id="cb22-447"><a href="#cb22-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-448"><a href="#cb22-448" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
<span id="cb22-449"><a href="#cb22-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-450"><a href="#cb22-450" aria-hidden="true" tabindex="-1"></a>A GBM model for propensity scores can be specified using <span class="in">`method = "gbm"`</span> inside the <span class="in">`weightit()`</span> function. To ensure consistent results, running <span class="in">`set.seed(88)`</span> will ensure each tree uses the same <span class="in">`seed`</span> if <span class="in">`bag.fraction`</span> less than $1$. The model is fit using the heuristically suggested starting values. Note that this model may take approximately $30$ second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</span>
<span id="cb22-451"><a href="#cb22-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-452"><a href="#cb22-452" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb22-453"><a href="#cb22-453" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb22-454"><a href="#cb22-454" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-455"><a href="#cb22-455" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>, <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-456"><a href="#cb22-456" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-457"><a href="#cb22-457" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>), <span class="co">#&lt;3&gt;</span></span>
<span id="cb22-458"><a href="#cb22-458" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="co">#&lt;3&gt;</span></span>
<span id="cb22-459"><a href="#cb22-459" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>, <span class="co">#&lt;4&gt;</span></span>
<span id="cb22-460"><a href="#cb22-460" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>), <span class="co">#&lt;4&gt;</span></span>
<span id="cb22-461"><a href="#cb22-461" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, <span class="co">#&lt;5&gt;</span></span>
<span id="cb22-462"><a href="#cb22-462" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>) <span class="co">#&lt;5&gt;</span></span>
<span id="cb22-463"><a href="#cb22-463" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune) <span class="co">#&lt;6&gt;</span></span>
<span id="cb22-464"><a href="#cb22-464" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-465"><a href="#cb22-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-466"><a href="#cb22-466" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specifies the formula and data.</span>
<span id="cb22-467"><a href="#cb22-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-468"><a href="#cb22-468" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
<span id="cb22-469"><a href="#cb22-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-470"><a href="#cb22-470" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Performs a grid search over these values of the learning rate and depth of tree.</span>
<span id="cb22-471"><a href="#cb22-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-472"><a href="#cb22-472" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
<span id="cb22-473"><a href="#cb22-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-474"><a href="#cb22-474" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be $10000$ which is the package default.</span>
<span id="cb22-475"><a href="#cb22-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-476"><a href="#cb22-476" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Prints the tune details of the model with the best covariate balance.</span>
<span id="cb22-477"><a href="#cb22-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-478"><a href="#cb22-478" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- clarify the meaning of learning rate/shrinkage --&gt;</span></span>
<span id="cb22-479"><a href="#cb22-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-480"><a href="#cb22-480" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- change all the instructions to active speech not passive.  --&gt;</span></span>
<span id="cb22-481"><a href="#cb22-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-482"><a href="#cb22-482" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-483"><a href="#cb22-483" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_boosted_weight</span></span>
<span id="cb22-484"><a href="#cb22-484" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb22-485"><a href="#cb22-485" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-486"><a href="#cb22-486" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-487"><a href="#cb22-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-488"><a href="#cb22-488" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb22-489"><a href="#cb22-489" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_weight &lt;- weightit(nsw_formula, data = nsw_data, </span></span>
<span id="cb22-490"><a href="#cb22-490" aria-hidden="true" tabindex="-1"></a><span class="in">                               method="gbm",</span></span>
<span id="cb22-491"><a href="#cb22-491" aria-hidden="true" tabindex="-1"></a><span class="in">                               estimand = "ATE", </span></span>
<span id="cb22-492"><a href="#cb22-492" aria-hidden="true" tabindex="-1"></a><span class="in">                               shrinkage= c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3),</span></span>
<span id="cb22-493"><a href="#cb22-493" aria-hidden="true" tabindex="-1"></a><span class="in">                               interaction.depth = 1:5,</span></span>
<span id="cb22-494"><a href="#cb22-494" aria-hidden="true" tabindex="-1"></a><span class="in">                               bag.fraction = 1,</span></span>
<span id="cb22-495"><a href="#cb22-495" aria-hidden="true" tabindex="-1"></a><span class="in">                               offset = c(TRUE, FALSE),</span></span>
<span id="cb22-496"><a href="#cb22-496" aria-hidden="true" tabindex="-1"></a><span class="in">                               criterion = "smd.mean", </span></span>
<span id="cb22-497"><a href="#cb22-497" aria-hidden="true" tabindex="-1"></a><span class="in">                               n.trees = 10000)</span></span>
<span id="cb22-498"><a href="#cb22-498" aria-hidden="true" tabindex="-1"></a><span class="in">print(nsw_boosted_weight$info$best.tune)</span></span>
<span id="cb22-499"><a href="#cb22-499" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-500"><a href="#cb22-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-501"><a href="#cb22-501" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- cite what the balance statistics should be in the intro when discussing propensity score and balance.  --&gt;</span></span>
<span id="cb22-502"><a href="#cb22-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-503"><a href="#cb22-503" aria-hidden="true" tabindex="-1"></a>The best balance across all tuning combinations yields an average SMD of $0.023$ showing strong balance. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in @sec-nsw-balance accompanying a discussion of balance.</span>
<span id="cb22-504"><a href="#cb22-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-505"><a href="#cb22-505" aria-hidden="true" tabindex="-1"></a>The best machine has a learning rate of $0.3$ and contains $2392$ decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to $0.3$. A reduction in the depth of tree and number of trees will reduce computation time.</span>
<span id="cb22-506"><a href="#cb22-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-507"><a href="#cb22-507" aria-hidden="true" tabindex="-1"></a>The new tune grid will consider <span class="in">`shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)`</span> as this allows the GBM to consider values between $0.2$ and $0.3$ and above $0.3$ which were missing in the previous grid.</span>
<span id="cb22-508"><a href="#cb22-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-509"><a href="#cb22-509" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-510"><a href="#cb22-510" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_boosted_weight2</span></span>
<span id="cb22-511"><a href="#cb22-511" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-512"><a href="#cb22-512" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-513"><a href="#cb22-513" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb22-514"><a href="#cb22-514" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "PALCEHOLDER"</span></span>
<span id="cb22-515"><a href="#cb22-515" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb22-516"><a href="#cb22-516" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_weight2 &lt;- weightit(nsw_formula, data = nsw_data, </span></span>
<span id="cb22-517"><a href="#cb22-517" aria-hidden="true" tabindex="-1"></a><span class="in">                               method="gbm",</span></span>
<span id="cb22-518"><a href="#cb22-518" aria-hidden="true" tabindex="-1"></a><span class="in">                               estimand = "ATE", </span></span>
<span id="cb22-519"><a href="#cb22-519" aria-hidden="true" tabindex="-1"></a><span class="in">                               shrinkage= c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5),</span></span>
<span id="cb22-520"><a href="#cb22-520" aria-hidden="true" tabindex="-1"></a><span class="in">                               interaction.depth = 1:3,</span></span>
<span id="cb22-521"><a href="#cb22-521" aria-hidden="true" tabindex="-1"></a><span class="in">                               bag.fraction = 1,</span></span>
<span id="cb22-522"><a href="#cb22-522" aria-hidden="true" tabindex="-1"></a><span class="in">                               offset = c(TRUE, FALSE),</span></span>
<span id="cb22-523"><a href="#cb22-523" aria-hidden="true" tabindex="-1"></a><span class="in">                               criterion = "smd.mean", </span></span>
<span id="cb22-524"><a href="#cb22-524" aria-hidden="true" tabindex="-1"></a><span class="in">                               n.trees = 5000)</span></span>
<span id="cb22-525"><a href="#cb22-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-526"><a href="#cb22-526" aria-hidden="true" tabindex="-1"></a><span class="in">print(nsw_boosted_weight2$info$best.tune)</span></span>
<span id="cb22-527"><a href="#cb22-527" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-528"><a href="#cb22-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-529"><a href="#cb22-529" aria-hidden="true" tabindex="-1"></a>Comparing the two iterations, there is a reduction from $0.022$ to $0.02$. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of $0.45$, a tree depth of $2$, and $95$ trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</span>
<span id="cb22-530"><a href="#cb22-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-531"><a href="#cb22-531" aria-hidden="true" tabindex="-1"></a>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, @fig-balance-iterations shows the optimal number of trees is highly variable. If the learning rate is set to <span class="in">`shrinkage = 0.05`</span>, then the best balance is not achieved until near to $20,000$ trees.</span>
<span id="cb22-532"><a href="#cb22-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-535"><a href="#cb22-535" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-536"><a href="#cb22-536" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-balance-iterations</span></span>
<span id="cb22-537"><a href="#cb22-537" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Average Standardised Mean Differernce (Covaraite Balance) and the number of interations. Please note the difference in horozontal scale between the two plots."</span></span>
<span id="cb22-538"><a href="#cb22-538" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-539"><a href="#cb22-539" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-540"><a href="#cb22-540" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Make the @Rosenbaum1983"</span></span>
<span id="cb22-541"><a href="#cb22-541" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-542"><a href="#cb22-542" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-543"><a href="#cb22-543" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb22-544"><a href="#cb22-544" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb22-545"><a href="#cb22-545" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb22-546"><a href="#cb22-546" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb22-547"><a href="#cb22-547" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb22-548"><a href="#cb22-548" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb22-549"><a href="#cb22-549" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb22-550"><a href="#cb22-550" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb22-551"><a href="#cb22-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-552"><a href="#cb22-552" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-553"><a href="#cb22-553" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb22-554"><a href="#cb22-554" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb22-555"><a href="#cb22-555" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb22-556"><a href="#cb22-556" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb22-557"><a href="#cb22-557" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb22-558"><a href="#cb22-558" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb22-559"><a href="#cb22-559" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb22-560"><a href="#cb22-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-561"><a href="#cb22-561" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb22-562"><a href="#cb22-562" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb22-563"><a href="#cb22-563" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb22-564"><a href="#cb22-564" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb22-565"><a href="#cb22-565" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb22-566"><a href="#cb22-566" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span> </span>
<span id="cb22-567"><a href="#cb22-567" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb22-568"><a href="#cb22-568" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb22-569"><a href="#cb22-569" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb22-570"><a href="#cb22-570" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb22-571"><a href="#cb22-571" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb22-572"><a href="#cb22-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-573"><a href="#cb22-573" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb22-574"><a href="#cb22-574" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>)</span>
<span id="cb22-575"><a href="#cb22-575" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-576"><a href="#cb22-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-577"><a href="#cb22-577" aria-hidden="true" tabindex="-1"></a>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</span>
<span id="cb22-578"><a href="#cb22-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-579"><a href="#cb22-579" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 7 and 8: Assessing Balance {#sec-nsw-balance}</span></span>
<span id="cb22-580"><a href="#cb22-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-581"><a href="#cb22-581" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title="The Importance of Discussing Balance"}</span>
<span id="cb22-582"><a href="#cb22-582" aria-hidden="true" tabindex="-1"></a>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study's conclusions. @King2019 notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</span>
<span id="cb22-583"><a href="#cb22-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-584"><a href="#cb22-584" aria-hidden="true" tabindex="-1"></a>A good resource of information for assessing balance is documentation from the <span class="in">`cobalt`</span> package, which can be viewed by running <span class="in">`vignette(“cobalt”, package = “cobalt”)`</span> in R.</span>
<span id="cb22-585"><a href="#cb22-585" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-586"><a href="#cb22-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-587"><a href="#cb22-587" aria-hidden="true" tabindex="-1"></a><span class="in">`cobalt`</span> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <span class="in">`WeightIt`</span> for IPW and <span class="in">`MatchIt`</span> for propensity score matching. Balance tables are created using <span class="in">`bal.tab()`</span>.</span>
<span id="cb22-588"><a href="#cb22-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-589"><a href="#cb22-589" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make sure this comment about integration is not repeditive  --&gt;</span></span>
<span id="cb22-590"><a href="#cb22-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-593"><a href="#cb22-593" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-594"><a href="#cb22-594" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nsw-btab-logit</span></span>
<span id="cb22-595"><a href="#cb22-595" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb22-596"><a href="#cb22-596" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-597"><a href="#cb22-597" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-598"><a href="#cb22-598" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt) <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-599"><a href="#cb22-599" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight, <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-600"><a href="#cb22-600" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data, <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-601"><a href="#cb22-601" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),<span class="co">#&lt;3&gt;</span></span>
<span id="cb22-602"><a href="#cb22-602" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,<span class="co">#&lt;3&gt;</span></span>
<span id="cb22-603"><a href="#cb22-603" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))<span class="co">#&lt;4&gt;</span></span>
<span id="cb22-604"><a href="#cb22-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-605"><a href="#cb22-605" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] <span class="co">#&lt;5&gt;</span></span>
<span id="cb22-606"><a href="#cb22-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-607"><a href="#cb22-607" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-608"><a href="#cb22-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-609"><a href="#cb22-609" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb22-610"><a href="#cb22-610" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt) <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-611"><a href="#cb22-611" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight, <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-612"><a href="#cb22-612" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data, <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-613"><a href="#cb22-613" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>), <span class="co">#&lt;3&gt;</span></span>
<span id="cb22-614"><a href="#cb22-614" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>, <span class="co">#&lt;3&gt;</span></span>
<span id="cb22-615"><a href="#cb22-615" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>)) <span class="co">#&lt;4&gt;</span></span>
<span id="cb22-616"><a href="#cb22-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-617"><a href="#cb22-617" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] <span class="co">#&lt;5&gt;</span></span>
<span id="cb22-618"><a href="#cb22-618" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-619"><a href="#cb22-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-620"><a href="#cb22-620" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Loads the <span class="in">`cobalt`</span> package. This assumes the package is already installed with <span class="in">`install.packages("cobalt")`</span></span>
<span id="cb22-621"><a href="#cb22-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-622"><a href="#cb22-622" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Uses the <span class="in">`bal.tab()`</span> fucntion to create balance statistics for the previously created <span class="in">`nsw_logit_weight`</span> model.</span>
<span id="cb22-623"><a href="#cb22-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-624"><a href="#cb22-624" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
<span id="cb22-625"><a href="#cb22-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-626"><a href="#cb22-626" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Sets a threshold of balance to be $0.1$ to determine if a covariate is balanced.</span>
<span id="cb22-627"><a href="#cb22-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-628"><a href="#cb22-628" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Extracts the balance table of the <span class="in">`nsw_logit_btab`</span> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
<span id="cb22-629"><a href="#cb22-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-630"><a href="#cb22-630" aria-hidden="true" tabindex="-1"></a>Additionally, <span class="in">`bal.tab()`</span> will create balance tables for the GBM method's IPWs and the raw data. For presentation, <span class="in">`dplyr`</span> combines each of the individual balance tables for presentation using <span class="in">`kable`</span> and <span class="in">`kableExtra`</span>.</span>
<span id="cb22-631"><a href="#cb22-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-632"><a href="#cb22-632" aria-hidden="true" tabindex="-1"></a><span class="in">```{r nsw_boosted_btab, cache=TRUE}</span></span>
<span id="cb22-633"><a href="#cb22-633" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw-btab-raw-boosted</span></span>
<span id="cb22-634"><a href="#cb22-634" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-635"><a href="#cb22-635" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-636"><a href="#cb22-636" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb22-637"><a href="#cb22-637" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show the Code to See Creation of Balance Tables"</span></span>
<span id="cb22-638"><a href="#cb22-638" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_btab &lt;- bal.tab(nsw_boosted_weight, </span></span>
<span id="cb22-639"><a href="#cb22-639" aria-hidden="true" tabindex="-1"></a><span class="in">                            data = nsw_data,</span></span>
<span id="cb22-640"><a href="#cb22-640" aria-hidden="true" tabindex="-1"></a><span class="in">                            stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb22-641"><a href="#cb22-641" aria-hidden="true" tabindex="-1"></a><span class="in">                            binary = "std", continuous = "std",</span></span>
<span id="cb22-642"><a href="#cb22-642" aria-hidden="true" tabindex="-1"></a><span class="in">                            thresholds = c(mean.diffs = 0.1))</span></span>
<span id="cb22-643"><a href="#cb22-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-644"><a href="#cb22-644" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_raw_btab &lt;- bal.tab(nsw_formula, </span></span>
<span id="cb22-645"><a href="#cb22-645" aria-hidden="true" tabindex="-1"></a><span class="in">                        data = nsw_data, </span></span>
<span id="cb22-646"><a href="#cb22-646" aria-hidden="true" tabindex="-1"></a><span class="in">                        stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb22-647"><a href="#cb22-647" aria-hidden="true" tabindex="-1"></a><span class="in">                        binary = "std", continuous = "std",</span></span>
<span id="cb22-648"><a href="#cb22-648" aria-hidden="true" tabindex="-1"></a><span class="in">                        thresholds = c(mean.diffs = 0.1),</span></span>
<span id="cb22-649"><a href="#cb22-649" aria-hidden="true" tabindex="-1"></a><span class="in">                        s.d.denom = "treated")</span></span>
<span id="cb22-650"><a href="#cb22-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-651"><a href="#cb22-651" aria-hidden="true" tabindex="-1"></a><span class="in"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb22-652"><a href="#cb22-652" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_btab &lt;- nsw_boosted_btab$Balance[-1,-c(2,3)]</span></span>
<span id="cb22-653"><a href="#cb22-653" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_raw_btab &lt;- nsw_raw_btab$Balance[-c(5,6)]</span></span>
<span id="cb22-654"><a href="#cb22-654" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-655"><a href="#cb22-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-656"><a href="#cb22-656" aria-hidden="true" tabindex="-1"></a>::: {#tbl-combined-btab}</span>
<span id="cb22-659"><a href="#cb22-659" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-660"><a href="#cb22-660" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-combined-btab</span></span>
<span id="cb22-661"><a href="#cb22-661" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-662"><a href="#cb22-662" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-663"><a href="#cb22-663" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-664"><a href="#cb22-664" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code"</span></span>
<span id="cb22-665"><a href="#cb22-665" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Placeholder"</span></span>
<span id="cb22-666"><a href="#cb22-666" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb22-667"><a href="#cb22-667" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb22-668"><a href="#cb22-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-669"><a href="#cb22-669" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb22-670"><a href="#cb22-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-671"><a href="#cb22-671" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb22-672"><a href="#cb22-672" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb22-673"><a href="#cb22-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-674"><a href="#cb22-674" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb22-675"><a href="#cb22-675" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb22-676"><a href="#cb22-676" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb22-677"><a href="#cb22-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-678"><a href="#cb22-678" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb22-679"><a href="#cb22-679" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb22-680"><a href="#cb22-680" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb22-681"><a href="#cb22-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-682"><a href="#cb22-682" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb22-683"><a href="#cb22-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-684"><a href="#cb22-684" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb22-685"><a href="#cb22-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-686"><a href="#cb22-686" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb22-687"><a href="#cb22-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-688"><a href="#cb22-688" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb22-689"><a href="#cb22-689" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb22-690"><a href="#cb22-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-691"><a href="#cb22-691" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T,<span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb22-692"><a href="#cb22-692" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-693"><a href="#cb22-693" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb22-694"><a href="#cb22-694" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-695"><a href="#cb22-695" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-696"><a href="#cb22-696" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width=</span><span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-697"><a href="#cb22-697" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span>
<span id="cb22-698"><a href="#cb22-698" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-699"><a href="#cb22-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-700"><a href="#cb22-700" aria-hidden="true" tabindex="-1"></a>Balance Table for NSW Data</span>
<span id="cb22-701"><a href="#cb22-701" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-702"><a href="#cb22-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-703"><a href="#cb22-703" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- double check that the variables are in the righ tpalces in the table  --&gt;</span></span>
<span id="cb22-704"><a href="#cb22-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-705"><a href="#cb22-705" aria-hidden="true" tabindex="-1"></a>@tbl-combined-btab shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</span>
<span id="cb22-706"><a href="#cb22-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-707"><a href="#cb22-707" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- perhaps find the threshold for variance ratios --&gt;</span></span>
<span id="cb22-708"><a href="#cb22-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-709"><a href="#cb22-709" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 9: Results {#sec-nsw-results}</span></span>
<span id="cb22-710"><a href="#cb22-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-711"><a href="#cb22-711" aria-hidden="true" tabindex="-1"></a>Finally, the treatment effect can be estimated using <span class="in">`lm_weightit()`</span> from the <span class="in">`WeightIt`</span> package and <span class="in">`avg_comparisons()`</span> from the <span class="in">`marginaleffects`</span> package. <span class="in">`lm_weightit()`</span> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <span class="in">`avg_comparisons()`</span> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</span>
<span id="cb22-712"><a href="#cb22-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-713"><a href="#cb22-713" aria-hidden="true" tabindex="-1"></a>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="co">[</span><span class="ot">@Naimi2017</span><span class="co">]</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <span class="in">`re78`</span> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</span>
<span id="cb22-714"><a href="#cb22-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-715"><a href="#cb22-715" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-716"><a href="#cb22-716" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: compute-ate-nsw-boosted</span></span>
<span id="cb22-717"><a href="#cb22-717" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-718"><a href="#cb22-718" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb22-719"><a href="#cb22-719" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-720"><a href="#cb22-720" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_lm &lt;- lm_weightit(re78 ~ treat * (age + educ + re75 + black + </span></span>
<span id="cb22-721"><a href="#cb22-721" aria-hidden="true" tabindex="-1"></a><span class="in">                              hisp + degree + marr), data = nsw_data, </span></span>
<span id="cb22-722"><a href="#cb22-722" aria-hidden="true" tabindex="-1"></a><span class="in">                              weights = nsw_boosted_weight$weights)</span></span>
<span id="cb22-723"><a href="#cb22-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-724"><a href="#cb22-724" aria-hidden="true" tabindex="-1"></a><span class="in">library(marginaleffects)</span></span>
<span id="cb22-725"><a href="#cb22-725" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_result &lt;- avg_comparisons(nsw_boosted_lm, variables = "treat")</span></span>
<span id="cb22-726"><a href="#cb22-726" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-727"><a href="#cb22-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-728"><a href="#cb22-728" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb22-729"><a href="#cb22-729" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span> <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-730"><a href="#cb22-730" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb22-731"><a href="#cb22-731" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights) <span class="co">#&lt;2&gt;</span></span>
<span id="cb22-732"><a href="#cb22-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-733"><a href="#cb22-733" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects) <span class="co">#&lt;3&gt;</span></span>
<span id="cb22-734"><a href="#cb22-734" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>) <span class="co">#&lt;3&gt;</span></span>
<span id="cb22-735"><a href="#cb22-735" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-736"><a href="#cb22-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-737"><a href="#cb22-737" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Uses <span class="in">`lm_weightit()`</span> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <span class="in">`*`</span> indicates multiplication in R.</span>
<span id="cb22-738"><a href="#cb22-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-739"><a href="#cb22-739" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Specifies the <span class="in">`weights`</span> from the <span class="in">`nsw_boosted_weight`</span> object created earlier by the <span class="in">`weightit()`</span> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <span class="in">`nsw_boosted_weight$weights`</span>.</span>
<span id="cb22-740"><a href="#cb22-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-741"><a href="#cb22-741" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
<span id="cb22-742"><a href="#cb22-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-743"><a href="#cb22-743" aria-hidden="true" tabindex="-1"></a>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</span>
<span id="cb22-744"><a href="#cb22-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-745"><a href="#cb22-745" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-746"><a href="#cb22-746" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw-comparisons</span></span>
<span id="cb22-747"><a href="#cb22-747" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-748"><a href="#cb22-748" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: true</span></span>
<span id="cb22-749"><a href="#cb22-749" aria-hidden="true" tabindex="-1"></a><span class="in">#| eval: false</span></span>
<span id="cb22-750"><a href="#cb22-750" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-751"><a href="#cb22-751" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb22-752"><a href="#cb22-752" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show the Code to Create the Table"</span></span>
<span id="cb22-753"><a href="#cb22-753" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_lm &lt;- lm_weightit(re78~treat*(age + educ + </span></span>
<span id="cb22-754"><a href="#cb22-754" aria-hidden="true" tabindex="-1"></a><span class="in">                             re75 + black + hisp + </span></span>
<span id="cb22-755"><a href="#cb22-755" aria-hidden="true" tabindex="-1"></a><span class="in">                             degree + marr), data = nsw_data, </span></span>
<span id="cb22-756"><a href="#cb22-756" aria-hidden="true" tabindex="-1"></a><span class="in">                             weights = nsw_logit_weight$weights)</span></span>
<span id="cb22-757"><a href="#cb22-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-758"><a href="#cb22-758" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_result &lt;- avg_comparisons(nsw_logit_lm, variables = "treat")</span></span>
<span id="cb22-759"><a href="#cb22-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-760"><a href="#cb22-760" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_comparisons_tab &lt;- rbind(extract_comparison_results(nsw_logit_result),</span></span>
<span id="cb22-761"><a href="#cb22-761" aria-hidden="true" tabindex="-1"></a><span class="in">                             extract_comparison_results(nsw_boosted_result))</span></span>
<span id="cb22-762"><a href="#cb22-762" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(nsw_comparisons_tab) &lt;- c("Logistic Regression", "GBM")</span></span>
<span id="cb22-763"><a href="#cb22-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-764"><a href="#cb22-764" aria-hidden="true" tabindex="-1"></a><span class="in">kbl(nsw_comparisons_tab, digits=2,booktabs= T, align = "c", </span></span>
<span id="cb22-765"><a href="#cb22-765" aria-hidden="true" tabindex="-1"></a><span class="in">      font_size=10) %&gt;%</span></span>
<span id="cb22-766"><a href="#cb22-766" aria-hidden="true" tabindex="-1"></a><span class="in">  kable_styling(full_width = T)</span></span>
<span id="cb22-767"><a href="#cb22-767" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-768"><a href="#cb22-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-769"><a href="#cb22-769" aria-hidden="true" tabindex="-1"></a>::: {#tbl-nsw-comparisons}</span>
<span id="cb22-770"><a href="#cb22-770" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-771"><a href="#cb22-771" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-772"><a href="#cb22-772" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb22-773"><a href="#cb22-773" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-774"><a href="#cb22-774" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_lm &lt;- lm_weightit(re78~treat*(age + educ + </span></span>
<span id="cb22-775"><a href="#cb22-775" aria-hidden="true" tabindex="-1"></a><span class="in">                             re75 + black + hisp + </span></span>
<span id="cb22-776"><a href="#cb22-776" aria-hidden="true" tabindex="-1"></a><span class="in">                             degree + marr), data = nsw_data, </span></span>
<span id="cb22-777"><a href="#cb22-777" aria-hidden="true" tabindex="-1"></a><span class="in">                             weights = nsw_logit_weight$weights)</span></span>
<span id="cb22-778"><a href="#cb22-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-779"><a href="#cb22-779" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_result &lt;- avg_comparisons(nsw_logit_lm, variables = "treat")</span></span>
<span id="cb22-780"><a href="#cb22-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-781"><a href="#cb22-781" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_comparisons_tab &lt;- rbind(extract_comparison_results(nsw_logit_result),</span></span>
<span id="cb22-782"><a href="#cb22-782" aria-hidden="true" tabindex="-1"></a><span class="in">                             extract_comparison_results(nsw_boosted_result))</span></span>
<span id="cb22-783"><a href="#cb22-783" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(nsw_comparisons_tab) &lt;- c("Logistic Regression", "Generalized Boosting Machine ")</span></span>
<span id="cb22-784"><a href="#cb22-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-785"><a href="#cb22-785" aria-hidden="true" tabindex="-1"></a><span class="in">kbl(nsw_comparisons_tab, digits=2,booktabs= T, align = "c", </span></span>
<span id="cb22-786"><a href="#cb22-786" aria-hidden="true" tabindex="-1"></a><span class="in">      font_size=10) %&gt;%</span></span>
<span id="cb22-787"><a href="#cb22-787" aria-hidden="true" tabindex="-1"></a><span class="in">  kable_styling(full_width = T)</span></span>
<span id="cb22-788"><a href="#cb22-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-789"><a href="#cb22-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-790"><a href="#cb22-790" aria-hidden="true" tabindex="-1"></a>Comparison of ATE Estimates</span>
<span id="cb22-791"><a href="#cb22-791" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-792"><a href="#cb22-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-793"><a href="#cb22-793" aria-hidden="true" tabindex="-1"></a>@tbl-nsw-comparisons shows that both estimates of the treatment effect are nearly identical at $\$1610$ with logistic regression inferring a $\$0.86$ larger treatment effect. Additionally, these results are statistically significant at the $5\%$ level with nearly identical standard errors.</span>
<span id="cb22-794"><a href="#cb22-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-795"><a href="#cb22-795" aria-hidden="true" tabindex="-1"></a><span class="fu">## Replication Study</span></span>
<span id="cb22-796"><a href="#cb22-796" aria-hidden="true" tabindex="-1"></a>The replication study focuses on a paper titled "The Impact of Coffee Certification on Small-Scale Producers' Livelihoods: A Case Study from the Jimma Zone, Ethiopia," published in Agricultural Economics (2012) by Pradyot Ranjan Jena, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. This paper explores the effects of coffee certification schemes on the economic wellbeing of small-scale coffee farmers in Ethiopia, particularly examining whether these schemes contribute to poverty reduction and improved livelihoods among smallholders.</span>
<span id="cb22-797"><a href="#cb22-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-798"><a href="#cb22-798" aria-hidden="true" tabindex="-1"></a>The central theme of the paper is the evaluation of certification schemes, such as Fairtrade and organic certification, as tools for enhancing the income stability and economic resilience of small-scale coffee producers. Certification is seen as a potential tool for economic growth and and environmental sustainability and so it is important to understand the impact on small-scale farmers. @tbl-coffee-var-summary summarises the variables used in the propensity score model. </span>
<span id="cb22-799"><a href="#cb22-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-800"><a href="#cb22-800" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-801"><a href="#cb22-801" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-coffee-var-summary</span></span>
<span id="cb22-802"><a href="#cb22-802" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb22-803"><a href="#cb22-803" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-804"><a href="#cb22-804" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: true</span></span>
<span id="cb22-805"><a href="#cb22-805" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: "bla"</span></span>
<span id="cb22-806"><a href="#cb22-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-807"><a href="#cb22-807" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_variable_summary &lt;- data.frame(</span></span>
<span id="cb22-808"><a href="#cb22-808" aria-hidden="true" tabindex="-1"></a><span class="in">  Variable_name = c("Certification (Treatment/ Control)", "Household Age", "Squared Household Age", "Gender", </span></span>
<span id="cb22-809"><a href="#cb22-809" aria-hidden="true" tabindex="-1"></a><span class="in">                    "Dependency Ratio", "Education Level", "Years of Coffee Production", </span></span>
<span id="cb22-810"><a href="#cb22-810" aria-hidden="true" tabindex="-1"></a><span class="in">                    "Log Total Land", "Access to Credit", </span></span>
<span id="cb22-811"><a href="#cb22-811" aria-hidden="true" tabindex="-1"></a><span class="in">                    "Bad Weather", "Non-farm Income Access"),</span></span>
<span id="cb22-812"><a href="#cb22-812" aria-hidden="true" tabindex="-1"></a><span class="in">  Description_of_variable = c("If the farming household is certified (=1) or </span></span>
<span id="cb22-813"><a href="#cb22-813" aria-hidden="true" tabindex="-1"></a><span class="in">                              otherwise (=0)", </span></span>
<span id="cb22-814"><a href="#cb22-814" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Age of the head of the household in years", </span></span>
<span id="cb22-815"><a href="#cb22-815" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Age of the head of the household squared", </span></span>
<span id="cb22-816"><a href="#cb22-816" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Gender of the head of household (male = 1 and </span></span>
<span id="cb22-817"><a href="#cb22-817" aria-hidden="true" tabindex="-1"></a><span class="in">                              female = 0)", </span></span>
<span id="cb22-818"><a href="#cb22-818" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Household members below 14 and above 65 years </span></span>
<span id="cb22-819"><a href="#cb22-819" aria-hidden="true" tabindex="-1"></a><span class="in">                              divided by rest of the household member", </span></span>
<span id="cb22-820"><a href="#cb22-820" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Education of the head of household in years", </span></span>
<span id="cb22-821"><a href="#cb22-821" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Years of experience in coffee farming", </span></span>
<span id="cb22-822"><a href="#cb22-822" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Logarithm of total land size in hectares", </span></span>
<span id="cb22-823"><a href="#cb22-823" aria-hidden="true" tabindex="-1"></a><span class="in">                              "Household has access to credit (yes = 1, </span></span>
<span id="cb22-824"><a href="#cb22-824" aria-hidden="true" tabindex="-1"></a><span class="in">                              otherwise = 0)",</span></span>
<span id="cb22-825"><a href="#cb22-825" aria-hidden="true" tabindex="-1"></a><span class="in">                              "If the household was affected by floods/droughts </span></span>
<span id="cb22-826"><a href="#cb22-826" aria-hidden="true" tabindex="-1"></a><span class="in">                              during the last year (2008–2009)", </span></span>
<span id="cb22-827"><a href="#cb22-827" aria-hidden="true" tabindex="-1"></a><span class="in">                              "If the household has access to nonfarm income"))</span></span>
<span id="cb22-828"><a href="#cb22-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-829"><a href="#cb22-829" aria-hidden="true" tabindex="-1"></a><span class="in">kbl(coffee_variable_summary, booktabs= T, align = "l", </span></span>
<span id="cb22-830"><a href="#cb22-830" aria-hidden="true" tabindex="-1"></a><span class="in">      font_size=10, col.names = c("Variable name", "Description of variable")) %&gt;%</span></span>
<span id="cb22-831"><a href="#cb22-831" aria-hidden="true" tabindex="-1"></a><span class="in">  row_spec(1, bold = TRUE) %&gt;%</span></span>
<span id="cb22-832"><a href="#cb22-832" aria-hidden="true" tabindex="-1"></a><span class="in">  kable_styling(full_width = T)</span></span>
<span id="cb22-833"><a href="#cb22-833" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-834"><a href="#cb22-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-835"><a href="#cb22-835" aria-hidden="true" tabindex="-1"></a>@Jana2012 define livelihood as a combination of per capita income, total income, per capita consumption and yield per hectare. For simplicity, this replication will only use per capita income as a dependent variable. This measure is selected as per capita income best quantifies the individual income which most strongly impacts overall livelihood <span class="co">[</span><span class="ot">see @cite</span><span class="co">]</span>. </span>
<span id="cb22-836"><a href="#cb22-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-837"><a href="#cb22-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-838"><a href="#cb22-838" aria-hidden="true" tabindex="-1"></a>Randomisation into certified and uncertified is not possible and it is likely that farmers who seek certification are different than farmers who don't. Thus, there is selection bias leading to structural differences between groups so a contrast in means between the certified (treated) and uncertified (control) farmers would be biased. Propensity scores are used to create covariate balance and reduce bias of the estimated treatment effect. The paper did not assess the balance of covariates. However, this provides a good opportunity to assess covariate balance in the initial paper and the repeat the analysis using a machine learning propensity model. </span>
<span id="cb22-839"><a href="#cb22-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-840"><a href="#cb22-840" aria-hidden="true" tabindex="-1"></a><span class="fu">### Replication of Original Results</span></span>
<span id="cb22-841"><a href="#cb22-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-842"><a href="#cb22-842" aria-hidden="true" tabindex="-1"></a>@Jena2012 provides a replication package including Stata code that uses Stata's psmatch2 package to perform nearest neighbour matching with replacement and common support trimming. Common support trimming means that any observations outside the commonly overlapping are are discarded. The results of the paper are be fully replicated using the <span class="in">`MatchIt`</span> package inside R.</span>
<span id="cb22-843"><a href="#cb22-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-844"><a href="#cb22-844" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-845"><a href="#cb22-845" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-846"><a href="#cb22-846" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-847"><a href="#cb22-847" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb22-848"><a href="#cb22-848" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show Code to Replicate Results of @Jena2012"</span></span>
<span id="cb22-849"><a href="#cb22-849" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: coffee_rep_code</span></span>
<span id="cb22-850"><a href="#cb22-850" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_formula &lt;- as.formula(certified ~ age_hh + </span></span>
<span id="cb22-851"><a href="#cb22-851" aria-hidden="true" tabindex="-1"></a><span class="in">                  agesq + nonfarmincome_access + depratio +</span></span>
<span id="cb22-852"><a href="#cb22-852" aria-hidden="true" tabindex="-1"></a><span class="in">                  logtotal_land + badweat + edu + gender + </span></span>
<span id="cb22-853"><a href="#cb22-853" aria-hidden="true" tabindex="-1"></a><span class="in">                  years_cofeproduction + access_credit)</span></span>
<span id="cb22-854"><a href="#cb22-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-855"><a href="#cb22-855" aria-hidden="true" tabindex="-1"></a><span class="in">library(MatchIt)</span></span>
<span id="cb22-856"><a href="#cb22-856" aria-hidden="true" tabindex="-1"></a><span class="in">library(marginaleffects)</span></span>
<span id="cb22-857"><a href="#cb22-857" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_rep_pmodel &lt;- matchit(coffee_formula, data=coffee_data, distance="glm", </span></span>
<span id="cb22-858"><a href="#cb22-858" aria-hidden="true" tabindex="-1"></a><span class="in">                              method="nearest", replace = T, estimand="ATT", </span></span>
<span id="cb22-859"><a href="#cb22-859" aria-hidden="true" tabindex="-1"></a><span class="in">                              discard="both") </span></span>
<span id="cb22-860"><a href="#cb22-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-861"><a href="#cb22-861" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_md &lt;- match.data(coffee_rep_pmodel)</span></span>
<span id="cb22-862"><a href="#cb22-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-863"><a href="#cb22-863" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_rep_fit&lt;- lm(percapitaincome_day_maleeq ~ certified, data = coffee_logit_md, weights=weights)</span></span>
<span id="cb22-864"><a href="#cb22-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-865"><a href="#cb22-865" aria-hidden="true" tabindex="-1"></a><span class="in">replicated_result &lt;- avg_comparisons(coffee_rep_fit, variables = "certified",</span></span>
<span id="cb22-866"><a href="#cb22-866" aria-hidden="true" tabindex="-1"></a><span class="in">                vcov = TRUE,</span></span>
<span id="cb22-867"><a href="#cb22-867" aria-hidden="true" tabindex="-1"></a><span class="in">                newdata = subset(coffee_logit_md, certified == 1),</span></span>
<span id="cb22-868"><a href="#cb22-868" aria-hidden="true" tabindex="-1"></a><span class="in">                wts = "weights")</span></span>
<span id="cb22-869"><a href="#cb22-869" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-870"><a href="#cb22-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-871"><a href="#cb22-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-874"><a href="#cb22-874" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-875"><a href="#cb22-875" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-876"><a href="#cb22-876" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-replicated-result</span></span>
<span id="cb22-877"><a href="#cb22-877" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-878"><a href="#cb22-878" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-879"><a href="#cb22-879" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show to Code to Make [Figure #](#tbl-replicated-result)"</span></span>
<span id="cb22-880"><a href="#cb22-880" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Placeholder"</span></span>
<span id="cb22-881"><a href="#cb22-881" aria-hidden="true" tabindex="-1"></a>replicated_result_tbl <span class="ot">&lt;-</span> <span class="fu">extract_comparison_results</span>(replicated_result)</span>
<span id="cb22-882"><a href="#cb22-882" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(replicated_result_tbl) <span class="ot">&lt;-</span> <span class="st">"Replicated Result"</span></span>
<span id="cb22-883"><a href="#cb22-883" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(replicated_result_tbl, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb22-884"><a href="#cb22-884" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-885"><a href="#cb22-885" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span>
<span id="cb22-886"><a href="#cb22-886" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-887"><a href="#cb22-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-888"><a href="#cb22-888" aria-hidden="true" tabindex="-1"></a>@tbl-replicated-result shows the replicated result obtained by @Jena2012. The intriguing finding of the paper is that the average treatment effect on the treated (ATT) is negative. That is, of the farmers that become certified, their per capita income is expected to decrease by $\$0.15$ per day. Intuition and proponents of certification schemes suggest that certification leads to an increase of income. If certification negatively impacts income, it would call into question a significant effort to engage in certification and fair trade practices.</span>
<span id="cb22-889"><a href="#cb22-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-890"><a href="#cb22-890" aria-hidden="true" tabindex="-1"></a>@Jena2012 does not perform any discussion or consideration of balance in their paper and so it is unclear if propensity score matching results in covariate balance. The <span class="in">`cobalt`</span> package creates balance tables using <span class="in">`bal.tab()`</span> and a visualisation using <span class="in">`love.plot()`</span>. </span>
<span id="cb22-891"><a href="#cb22-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-894"><a href="#cb22-894" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-895"><a href="#cb22-895" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-896"><a href="#cb22-896" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-897"><a href="#cb22-897" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-coffee-rep-kbl</span></span>
<span id="cb22-898"><a href="#cb22-898" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-899"><a href="#cb22-899" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Table #](#tbl-coffee-rep-kbl)"</span></span>
<span id="cb22-900"><a href="#cb22-900" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "PALCHOLDER"</span></span>
<span id="cb22-901"><a href="#cb22-901" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb22-902"><a href="#cb22-902" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_rep_pmodel, </span>
<span id="cb22-903"><a href="#cb22-903" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb22-904"><a href="#cb22-904" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb22-905"><a href="#cb22-905" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb22-906"><a href="#cb22-906" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb22-907"><a href="#cb22-907" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb22-908"><a href="#cb22-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-909"><a href="#cb22-909" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab_ss <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Observations</span>
<span id="cb22-910"><a href="#cb22-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-911"><a href="#cb22-911" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb22-912"><a href="#cb22-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-913"><a href="#cb22-913" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb22-914"><a href="#cb22-914" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Household Age"</span>, <span class="st">"Squared Household Age"</span>, <span class="st">"Non-farm Income Access"</span>, </span>
<span id="cb22-915"><a href="#cb22-915" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Log Total Land"</span>, <span class="st">"Dependency Ratio"</span>, <span class="st">"Bad Weather"</span>,</span>
<span id="cb22-916"><a href="#cb22-916" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Education Level"</span>, <span class="st">"Gender"</span>, <span class="st">"Years of Coffee Production"</span>, </span>
<span id="cb22-917"><a href="#cb22-917" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Access to Credit"</span>)</span>
<span id="cb22-918"><a href="#cb22-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-919"><a href="#cb22-919" aria-hidden="true" tabindex="-1"></a>colnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Variable"</span>,<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balance Threshold"</span>, <span class="st">"Variance Ratio"</span>)</span>
<span id="cb22-920"><a href="#cb22-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-921"><a href="#cb22-921" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_rep_btab) <span class="ot">&lt;-</span> rowlabels</span>
<span id="cb22-922"><a href="#cb22-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-923"><a href="#cb22-923" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb22-924"><a href="#cb22-924" aria-hidden="true" tabindex="-1"></a>          coffee_rep_btab[,<span class="dv">3</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb22-925"><a href="#cb22-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-926"><a href="#cb22-926" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb22-927"><a href="#cb22-927" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb22-928"><a href="#cb22-928" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>)</span>
<span id="cb22-929"><a href="#cb22-929" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-930"><a href="#cb22-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-933"><a href="#cb22-933" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-934"><a href="#cb22-934" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb22-935"><a href="#cb22-935" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-936"><a href="#cb22-936" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: getndropped</span></span>
<span id="cb22-937"><a href="#cb22-937" aria-hidden="true" tabindex="-1"></a>nobs_coffee_dropped <span class="ot">&lt;-</span> <span class="fu">sum</span>(coffee_rep_pmodel<span class="sc">$</span>discarded, <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span>
<span id="cb22-938"><a href="#cb22-938" aria-hidden="true" tabindex="-1"></a>coffee_data<span class="sc">$</span>discarded <span class="ot">&lt;-</span> coffee_rep_pmodel<span class="sc">$</span>discarded</span>
<span id="cb22-939"><a href="#cb22-939" aria-hidden="true" tabindex="-1"></a>nobs_coffee_Tdropped <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(coffee_data,discarded<span class="sc">==</span><span class="cn">TRUE</span><span class="sc">&amp;</span>certified<span class="sc">==</span><span class="dv">1</span>))</span>
<span id="cb22-940"><a href="#cb22-940" aria-hidden="true" tabindex="-1"></a>nobs_coffee_Cdropped <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(coffee_data,discarded<span class="sc">==</span><span class="cn">TRUE</span><span class="sc">&amp;</span>certified<span class="sc">==</span><span class="dv">0</span>))</span>
<span id="cb22-941"><a href="#cb22-941" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-942"><a href="#cb22-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-945"><a href="#cb22-945" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-946"><a href="#cb22-946" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-coffee-replication-lplot</span></span>
<span id="cb22-947"><a href="#cb22-947" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-948"><a href="#cb22-948" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-949"><a href="#cb22-949" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-950"><a href="#cb22-950" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#fig-coffee-replication-lplot)"</span></span>
<span id="cb22-951"><a href="#cb22-951" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-952"><a href="#cb22-952" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "PLACEHOLDER"</span></span>
<span id="cb22-953"><a href="#cb22-953" aria-hidden="true" tabindex="-1"></a><span class="co"># add render info for showtext to yaml. also chang legend to be more informative.  </span></span>
<span id="cb22-954"><a href="#cb22-954" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb22-955"><a href="#cb22-955" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb22-956"><a href="#cb22-956" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb22-957"><a href="#cb22-957" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel),</span>
<span id="cb22-958"><a href="#cb22-958" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,</span>
<span id="cb22-959"><a href="#cb22-959" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb22-960"><a href="#cb22-960" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>),</span>
<span id="cb22-961"><a href="#cb22-961" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>, <span class="at">thresholds=</span><span class="fl">0.1</span>, <span class="at">s.d.denom=</span><span class="st">"treated"</span>) <span class="sc">+</span></span>
<span id="cb22-962"><a href="#cb22-962" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance"</span>,</span>
<span id="cb22-963"><a href="#cb22-963" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb22-964"><a href="#cb22-964" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb22-965"><a href="#cb22-965" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb22-966"><a href="#cb22-966" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>)))</span>
<span id="cb22-967"><a href="#cb22-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-968"><a href="#cb22-968" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-969"><a href="#cb22-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-970"><a href="#cb22-970" aria-hidden="true" tabindex="-1"></a>@tbl-coffee-rep-kbl and @fig-coffee-replication-lplot show that propensity score matching has obtained very poor balance. Based on the $10\%$ rule discussed in @sec-, not a single variable is balanced and so the estimate of the treatment effect is likely to be biased by structural differences between control and certified.</span>
<span id="cb22-971"><a href="#cb22-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-972"><a href="#cb22-972" aria-hidden="true" tabindex="-1"></a>Four key variables: *age*, *gender*, *education*, and *access to credit* all exhibit poor balance. These variables are strong confounders in theory and so emphasising balance in these variables is critical to making a robust causal inference. Perhaps there is gender or age discrimination in the certification process. Perhaps, those with lesser education may struggle to obtain certification. Perhaps those who have less access to credit are unable to afford to become certified. Moving forward, these variables must exhibit better covariate balance to make a robust conclusion.</span>
<span id="cb22-973"><a href="#cb22-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-974"><a href="#cb22-974" aria-hidden="true" tabindex="-1"></a>@fig-replication-pscore shows the effect of common support trimming. @tbl-coffee-replication-ss shows <span class="in">`{r} nobs_coffee_dropped`</span> total observations are dropped of which <span class="in">`{r} nobs_coffee_Tdropped`</span> are treated and <span class="in">`{r} nobs_coffee_Cdropped`</span> are control. By dropping these observations, PSM avoids making poor matching which should lead to better covariate balance. When observations are discarded, the estimand is no longer the ATT. Instead, it is refereed to as the average treatment effect on the matched or ATM. There is a significant reduction in the effective sample size in the control group from $82$ to $21$ individuals.  </span>
<span id="cb22-975"><a href="#cb22-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-978"><a href="#cb22-978" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-979"><a href="#cb22-979" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-980"><a href="#cb22-980" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-981"><a href="#cb22-981" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb22-982"><a href="#cb22-982" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-replication-pscore</span></span>
<span id="cb22-983"><a href="#cb22-983" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-984"><a href="#cb22-984" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-985"><a href="#cb22-985" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#fig-replication-pscore)"</span></span>
<span id="cb22-986"><a href="#cb22-986" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "PLACEHOLDER"</span></span>
<span id="cb22-987"><a href="#cb22-987" aria-hidden="true" tabindex="-1"></a>discarded_scores <span class="ot">&lt;-</span> coffee_rep_pmodel<span class="sc">$</span>distance[coffee_rep_pmodel<span class="sc">$</span>discarded]</span>
<span id="cb22-988"><a href="#cb22-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-989"><a href="#cb22-989" aria-hidden="true" tabindex="-1"></a>discard_min <span class="ot">&lt;-</span> <span class="fu">min</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb22-990"><a href="#cb22-990" aria-hidden="true" tabindex="-1"></a>discard_max <span class="ot">&lt;-</span> <span class="fu">max</span>(discarded_scores, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb22-991"><a href="#cb22-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-992"><a href="#cb22-992" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(coffee_data, <span class="fu">aes</span>(<span class="at">x =</span> coffee_rep_pmodel<span class="sc">$</span>distance, <span class="at">fill =</span> <span class="fu">factor</span>(certified))) <span class="sc">+</span></span>
<span id="cb22-993"><a href="#cb22-993" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb22-994"><a href="#cb22-994" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb22-995"><a href="#cb22-995" aria-hidden="true" tabindex="-1"></a>                    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Certified"</span>)) <span class="sc">+</span></span>
<span id="cb22-996"><a href="#cb22-996" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Distribution of Propensity Scores in @Jena2012"</span>, </span>
<span id="cb22-997"><a href="#cb22-997" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb22-998"><a href="#cb22-998" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb22-999"><a href="#cb22-999" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb22-1000"><a href="#cb22-1000" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_min, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb22-1001"><a href="#cb22-1001" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> discard_max, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb22-1002"><a href="#cb22-1002" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> <span class="dv">0</span>, <span class="at">xmax =</span> discard_min, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb22-1003"><a href="#cb22-1003" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"rect"</span>, <span class="at">xmin =</span> discard_max, <span class="at">xmax =</span> <span class="dv">1</span>, <span class="at">ymin =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">ymax =</span> <span class="cn">Inf</span>, <span class="at">fill =</span> <span class="st">"#333333"</span>, <span class="at">alpha=</span><span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb22-1004"><a href="#cb22-1004" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.02</span>, <span class="at">y =</span> <span class="fl">2.5</span>, </span>
<span id="cb22-1005"><a href="#cb22-1005" aria-hidden="true" tabindex="-1"></a>           <span class="at">label =</span> <span class="st">"Discarded Range"</span>, <span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">1.5</span>, <span class="at">size=</span><span class="dv">4</span>,<span class="at">fontface=</span><span class="st">"bold"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>) <span class="sc">+</span></span>
<span id="cb22-1006"><a href="#cb22-1006" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme</span>
<span id="cb22-1007"><a href="#cb22-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1008"><a href="#cb22-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1011"><a href="#cb22-1011" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1012"><a href="#cb22-1012" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-coffee-replication-ss</span></span>
<span id="cb22-1013"><a href="#cb22-1013" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-1014"><a href="#cb22-1014" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-1015"><a href="#cb22-1015" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-1016"><a href="#cb22-1016" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-1017"><a href="#cb22-1017" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Table #](#tbl-coffee-replication-ss)"</span></span>
<span id="cb22-1018"><a href="#cb22-1018" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Placeholder"</span></span>
<span id="cb22-1019"><a href="#cb22-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1020"><a href="#cb22-1020" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_rep_btab_ss, <span class="at">digits=</span><span class="dv">0</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb22-1021"><a href="#cb22-1021" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1022"><a href="#cb22-1022" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span>F)</span>
<span id="cb22-1023"><a href="#cb22-1023" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1024"><a href="#cb22-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1025"><a href="#cb22-1025" aria-hidden="true" tabindex="-1"></a>Overall, the propensity score matching in @Jena2012 is poor and results in unbalanced covariates and a loss of estimand.</span>
<span id="cb22-1026"><a href="#cb22-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1027"><a href="#cb22-1027" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further Modelling </span></span>
<span id="cb22-1028"><a href="#cb22-1028" aria-hidden="true" tabindex="-1"></a>To improve the poor balance achieved by the @Jana2012, there are two strategies to obtain better balance. First, the propensity scores can be re-estimated using machine learning to obtain better calibrated propensity scores. Second, inverse propensity weighting (IPW) can be used instead of propensity score matching (PSM). IPW should ensure that the sample size remains the same as no observations are lost through a matching process. IPW should retain all observations and preserve the estimand as the ATT. Additionally, IPW is generally more efficient as a pseudo-population is based on precise weights compared to matched observations that are based on approximate similarity.</span>
<span id="cb22-1029"><a href="#cb22-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1030"><a href="#cb22-1030" aria-hidden="true" tabindex="-1"></a>The machine learning propensity scores will be estimated using the <span class="in">`WeightIt`</span> package in the same process as @sec-demo. To select the tuning criteria, consider that @fig-coffee-replication-lplot that shows a significant range of balance levels in the raw data. Knowing this, the model is tuned using <span class="in">`criterion = “smd.max”`</span> as reducing the priority is to reduce the extremely unbalanced covariates even if this leads to a higher average SMD. </span>
<span id="cb22-1031"><a href="#cb22-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1034"><a href="#cb22-1034" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1035"><a href="#cb22-1035" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: coffee-boost-btab</span></span>
<span id="cb22-1036"><a href="#cb22-1036" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-1037"><a href="#cb22-1037" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-1038"><a href="#cb22-1038" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-1039"><a href="#cb22-1039" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show to Code to Fit the GBM model using `WeightIt` and `cobalt`"</span></span>
<span id="cb22-1040"><a href="#cb22-1040" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb22-1041"><a href="#cb22-1041" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<span id="cb22-1042"><a href="#cb22-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1043"><a href="#cb22-1043" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb22-1044"><a href="#cb22-1044" aria-hidden="true" tabindex="-1"></a>coffee_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data=</span>coffee_data, </span>
<span id="cb22-1045"><a href="#cb22-1045" aria-hidden="true" tabindex="-1"></a>                                <span class="at">method=</span><span class="st">"gbm"</span>, <span class="at">distribution=</span><span class="st">"bernoulli"</span>,</span>
<span id="cb22-1046"><a href="#cb22-1046" aria-hidden="true" tabindex="-1"></a>                                <span class="at">use.offset=</span><span class="fu">c</span>(T),</span>
<span id="cb22-1047"><a href="#cb22-1047" aria-hidden="true" tabindex="-1"></a>                                <span class="at">shrinkage=</span><span class="fu">seq</span>(<span class="fl">0.15</span>, <span class="fl">0.4</span>,<span class="at">length.out=</span><span class="dv">5</span>),</span>
<span id="cb22-1048"><a href="#cb22-1048" aria-hidden="true" tabindex="-1"></a>                                <span class="at">bag.fraction=</span><span class="fl">0.67</span>, </span>
<span id="cb22-1049"><a href="#cb22-1049" aria-hidden="true" tabindex="-1"></a>                                <span class="at">interaction.depth=</span><span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb22-1050"><a href="#cb22-1050" aria-hidden="true" tabindex="-1"></a>                                <span class="at">n.trees=</span><span class="dv">500</span>,</span>
<span id="cb22-1051"><a href="#cb22-1051" aria-hidden="true" tabindex="-1"></a>                                <span class="at">criterion=</span><span class="st">"smd.mean"</span>, </span>
<span id="cb22-1052"><a href="#cb22-1052" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand=</span><span class="st">"ATT"</span>)</span>
<span id="cb22-1053"><a href="#cb22-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1054"><a href="#cb22-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1055"><a href="#cb22-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1056"><a href="#cb22-1056" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_boosted_weight, </span>
<span id="cb22-1057"><a href="#cb22-1057" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb22-1058"><a href="#cb22-1058" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb22-1059"><a href="#cb22-1059" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb22-1060"><a href="#cb22-1060" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb22-1061"><a href="#cb22-1061" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb22-1062"><a href="#cb22-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1063"><a href="#cb22-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance tabltune# Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb22-1064"><a href="#cb22-1064" aria-hidden="true" tabindex="-1"></a>coffee_boosted_btab <span class="ot">&lt;-</span> coffee_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb22-1065"><a href="#cb22-1065" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1066"><a href="#cb22-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1067"><a href="#cb22-1067" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Discussion of Tuning"}</span>
<span id="cb22-1068"><a href="#cb22-1068" aria-hidden="true" tabindex="-1"></a>Initially, a tuning grid considering shrinkage values of $0.001,0.005,.01,0.05,0.1,\text{ and }0.2$ were considered using $10000$ trees with a depth between $1$ and $5$. The best tuning performance was found with shrinkage of $0.2$ and $9$ trees which were three splits $3$ deep. As such, the tuning grid was redefined in a second iteration to use $0.1, 0.15, 0.2, 0.25, 0.3,0.35,\text{ and } 0.4$ with only $1000$ trees with between $2$ and $5$ depth. The second fit, suggested a learning rate of $0.35$ so the local area of $0.3, 0.325, 0.350, 0.375, \text{ and }0.4$ is searched in the final fit.</span>
<span id="cb22-1069"><a href="#cb22-1069" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-1070"><a href="#cb22-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1071"><a href="#cb22-1071" aria-hidden="true" tabindex="-1"></a>Of course there is no guarantee that the GBM model will perform the best and so a logistic model is also fitted. An interesting comparison is between the SMDs in the matched data and in the weighted sample. Any differences between the two samples relates to the difference between PSM and IPW as the propensity scores are identical.</span>
<span id="cb22-1072"><a href="#cb22-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1073"><a href="#cb22-1073" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb22-1074"><a href="#cb22-1074" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb22-1075"><a href="#cb22-1075" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb22-1076"><a href="#cb22-1076" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: coffee_logit_weight</span></span>
<span id="cb22-1077"><a href="#cb22-1077" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb22-1078"><a href="#cb22-1078" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show to Code to Perform IPW with Logistic Regression."</span></span>
<span id="cb22-1079"><a href="#cb22-1079" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_weight &lt;- weightit(coffee_formula, data = coffee_data, method= "glm",</span></span>
<span id="cb22-1080"><a href="#cb22-1080" aria-hidden="true" tabindex="-1"></a><span class="in">                                estimand = "ATT")</span></span>
<span id="cb22-1081"><a href="#cb22-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1082"><a href="#cb22-1082" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_btab &lt;- bal.tab(coffee_logit_weight, </span></span>
<span id="cb22-1083"><a href="#cb22-1083" aria-hidden="true" tabindex="-1"></a><span class="in">                             formula = coffee_formula,</span></span>
<span id="cb22-1084"><a href="#cb22-1084" aria-hidden="true" tabindex="-1"></a><span class="in">                             data = coffee_data, </span></span>
<span id="cb22-1085"><a href="#cb22-1085" aria-hidden="true" tabindex="-1"></a><span class="in">                             stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb22-1086"><a href="#cb22-1086" aria-hidden="true" tabindex="-1"></a><span class="in">                             binary = "std", continuous = "std",</span></span>
<span id="cb22-1087"><a href="#cb22-1087" aria-hidden="true" tabindex="-1"></a><span class="in">                             thresholds = c(mean.diffs = 0.1),</span></span>
<span id="cb22-1088"><a href="#cb22-1088" aria-hidden="true" tabindex="-1"></a><span class="in">                             s.d.denom = "treated")</span></span>
<span id="cb22-1089"><a href="#cb22-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1090"><a href="#cb22-1090" aria-hidden="true" tabindex="-1"></a><span class="in">coffee_logit_btab &lt;- coffee_logit_btab$Balance[-1, -c(2,3)]</span></span>
<span id="cb22-1091"><a href="#cb22-1091" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1092"><a href="#cb22-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1093"><a href="#cb22-1093" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparison of Methods </span></span>
<span id="cb22-1094"><a href="#cb22-1094" aria-hidden="true" tabindex="-1"></a>In some of the earlier code chucnks, the <span class="in">`cobalt`</span> package's <span class="in">`bal.tab()`</span> computed balance tables which are combined together in @tbl-coffee-comparison to provide a comparison between methods. For a visual interpretation, <span class="in">`love.plot()`</span> creates @fig-coffee-love-all. </span>
<span id="cb22-1095"><a href="#cb22-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1098"><a href="#cb22-1098" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1099"><a href="#cb22-1099" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: manipulate-btab</span></span>
<span id="cb22-1100"><a href="#cb22-1100" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-1101"><a href="#cb22-1101" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-1102"><a href="#cb22-1102" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-1103"><a href="#cb22-1103" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code Prepairing the Balance Table for Presentation"</span></span>
<span id="cb22-1104"><a href="#cb22-1104" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"data.table"</span>)</span>
<span id="cb22-1105"><a href="#cb22-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1106"><a href="#cb22-1106" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_formula, </span>
<span id="cb22-1107"><a href="#cb22-1107" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb22-1108"><a href="#cb22-1108" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb22-1109"><a href="#cb22-1109" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb22-1110"><a href="#cb22-1110" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb22-1111"><a href="#cb22-1111" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb22-1112"><a href="#cb22-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1113"><a href="#cb22-1113" aria-hidden="true" tabindex="-1"></a>coffee_raw_btab <span class="ot">&lt;-</span> coffee_raw_btab<span class="sc">$</span>Balance[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span>
<span id="cb22-1114"><a href="#cb22-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1115"><a href="#cb22-1115" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> <span class="fu">rbindlist</span>(<span class="fu">list</span>(coffee_raw_btab,</span>
<span id="cb22-1116"><a href="#cb22-1116" aria-hidden="true" tabindex="-1"></a>                                       coffee_logit_btab,</span>
<span id="cb22-1117"><a href="#cb22-1117" aria-hidden="true" tabindex="-1"></a>                                       coffee_boosted_btab), <span class="at">use.names=</span><span class="cn">FALSE</span>)</span>
<span id="cb22-1118"><a href="#cb22-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1119"><a href="#cb22-1119" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb22-1120"><a href="#cb22-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1121"><a href="#cb22-1121" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> coffee_combined_btab[,<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)]</span>
<span id="cb22-1122"><a href="#cb22-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1123"><a href="#cb22-1123" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb22-1124"><a href="#cb22-1124" aria-hidden="true" tabindex="-1"></a>          coffee_combined_btab[,<span class="dv">4</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb22-1125"><a href="#cb22-1125" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1126"><a href="#cb22-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1127"><a href="#cb22-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1130"><a href="#cb22-1130" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1131"><a href="#cb22-1131" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: true</span></span>
<span id="cb22-1132"><a href="#cb22-1132" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb22-1133"><a href="#cb22-1133" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-1134"><a href="#cb22-1134" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Table #](#tbl-coffee-comparison)."</span></span>
<span id="cb22-1135"><a href="#cb22-1135" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-coffee-comparison</span></span>
<span id="cb22-1136"><a href="#cb22-1136" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Comparison of Balance for Coffee Data Using Different Propensity Models"</span></span>
<span id="cb22-1137"><a href="#cb22-1137" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb22-1138"><a href="#cb22-1138" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_combined_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb22-1139"><a href="#cb22-1139" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb22-1140"><a href="#cb22-1140" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1141"><a href="#cb22-1141" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold=</span><span class="cn">TRUE</span>, <span class="at">width=</span><span class="st">"5cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1142"><a href="#cb22-1142" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold=</span><span class="cn">FALSE</span>, <span class="at">width=</span><span class="st">"1cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1143"><a href="#cb22-1143" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Raw Data"</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1144"><a href="#cb22-1144" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Logistic Regression and IPTW"</span>, <span class="dv">11</span>, <span class="dv">20</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1145"><a href="#cb22-1145" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Boosted Machine with IPTW"</span>, <span class="dv">21</span>, <span class="dv">30</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>)</span>
<span id="cb22-1146"><a href="#cb22-1146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1147"><a href="#cb22-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1148"><a href="#cb22-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1151"><a href="#cb22-1151" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1152"><a href="#cb22-1152" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-coffee-love-all</span></span>
<span id="cb22-1153"><a href="#cb22-1153" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-1154"><a href="#cb22-1154" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb22-1155"><a href="#cb22-1155" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-1156"><a href="#cb22-1156" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#fig-coffee-love-all)"</span></span>
<span id="cb22-1157"><a href="#cb22-1157" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of Balance for Coffee Data Using Different Methods"</span></span>
<span id="cb22-1158"><a href="#cb22-1158" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb22-1159"><a href="#cb22-1159" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb22-1160"><a href="#cb22-1160" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel,</span>
<span id="cb22-1161"><a href="#cb22-1161" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Logit =</span> coffee_logit_weight,</span>
<span id="cb22-1162"><a href="#cb22-1162" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Boosting=</span> coffee_boosted_weight),</span>
<span id="cb22-1163"><a href="#cb22-1163" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,<span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb22-1164"><a href="#cb22-1164" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#333333"</span>, <span class="st">"#2780e3"</span>, <span class="st">"darkblue"</span>,<span class="st">"darkred"</span>), </span>
<span id="cb22-1165"><a href="#cb22-1165" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>, <span class="st">"triangle"</span>, <span class="st">"diamond"</span>),</span>
<span id="cb22-1166"><a href="#cb22-1166" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>,<span class="at">thresholds=</span><span class="fl">0.1</span>,<span class="at">s.d.denom=</span><span class="st">"treated"</span>,<span class="at">use.grid=</span>F)<span class="sc">+</span></span>
<span id="cb22-1167"><a href="#cb22-1167" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance Using Different Balance Methods"</span>,</span>
<span id="cb22-1168"><a href="#cb22-1168" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb22-1169"><a href="#cb22-1169" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb22-1170"><a href="#cb22-1170" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="at">length.out=</span><span class="dv">7</span>),<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="sc">+</span> custom_ggplot_theme</span>
<span id="cb22-1171"><a href="#cb22-1171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1172"><a href="#cb22-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1173"><a href="#cb22-1173" aria-hidden="true" tabindex="-1"></a>There are three notable findings: </span>
<span id="cb22-1174"><a href="#cb22-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1175"><a href="#cb22-1175" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>PSM has performed very poorly relative to IPW even when matching dropps a significant number of observations.  </span>
<span id="cb22-1176"><a href="#cb22-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1177"><a href="#cb22-1177" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A GBM model has resulted in better covariate balance than logistic regression for most covariates. Using a $10\%$ threshold for determining balance, logistic regression leaves $5$ variables unbalanced and the GBM leaves $3$ variables unbalanced. Additionally, the degree of unbalance is larger for logistic regression. </span>
<span id="cb22-1178"><a href="#cb22-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1179"><a href="#cb22-1179" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Logistic regress has a satisfactory average SMD of <span class="in">`{r} mean(coffee_logit_btab$Diff.Adj)`</span>. Boosting has an average SMD of <span class="in">`{r} mean(coffee_boosted_btab$Diff.Adj)`</span> which is excellent and meets a rigorous threshold of $5\%$. </span>
<span id="cb22-1180"><a href="#cb22-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1181"><a href="#cb22-1181" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>The covariate with the highest SMD is *household age* ($0.245\%$) in logistic regression and *bad weather* ($0.191$) in the GBM.</span>
<span id="cb22-1182"><a href="#cb22-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1183"><a href="#cb22-1183" aria-hidden="true" tabindex="-1"></a><span class="fu">### Results</span></span>
<span id="cb22-1184"><a href="#cb22-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1185"><a href="#cb22-1185" aria-hidden="true" tabindex="-1"></a>Now that satisfactory covariate balance is achieved, the treatment effect can be estimated under logistic regression, the GBM, and then compared to the result in the paper. Note that the estimand in the paper is intended to be the average treatment effect (ATT) but dropped observations mean the actual treatment effect is the average treatment effect on matched (ATM) individuals. In theory, better covariate balance should lead to a better estimate of the ATT so a comparison of the estimates is interesting. As in @sec-nsw-results, the results will be completed using G-computation with the <span class="in">`lm_weightit()`</span> and <span class="in">`avg_comparisons()`</span> functions.</span>
<span id="cb22-1186"><a href="#cb22-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1189"><a href="#cb22-1189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb22-1190"><a href="#cb22-1190" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb22-1191"><a href="#cb22-1191" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb22-1192"><a href="#cb22-1192" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb22-1193"><a href="#cb22-1193" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure #](#tbl-comparison-coffee-results)"</span></span>
<span id="cb22-1194"><a href="#cb22-1194" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-comparison-coffee-results</span></span>
<span id="cb22-1195"><a href="#cb22-1195" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "PLACEHOLDER"</span></span>
<span id="cb22-1196"><a href="#cb22-1196" aria-hidden="true" tabindex="-1"></a>coffee_att_formula <span class="ot">&lt;-</span> <span class="fu">update.formula</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, <span class="fu">paste</span>(<span class="fu">attr</span>(<span class="fu">terms</span>(coffee_formula), <span class="st">"term.labels"</span>), <span class="at">collapse =</span> <span class="st">" + "</span>))), percapitaincome_day_maleeq <span class="sc">~</span> certified <span class="sc">*</span> .)</span>
<span id="cb22-1197"><a href="#cb22-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1198"><a href="#cb22-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1199"><a href="#cb22-1199" aria-hidden="true" tabindex="-1"></a>coffee_logit_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb22-1200"><a href="#cb22-1200" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_logit_weight)</span>
<span id="cb22-1201"><a href="#cb22-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1202"><a href="#cb22-1202" aria-hidden="true" tabindex="-1"></a>coffee_boosted_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb22-1203"><a href="#cb22-1203" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_boosted_weight)</span>
<span id="cb22-1204"><a href="#cb22-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1205"><a href="#cb22-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1206"><a href="#cb22-1206" aria-hidden="true" tabindex="-1"></a>coffee_logit_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_logit_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb22-1207"><a href="#cb22-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1208"><a href="#cb22-1208" aria-hidden="true" tabindex="-1"></a>coffee_boosted_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_boosted_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb22-1209"><a href="#cb22-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1210"><a href="#cb22-1210" aria-hidden="true" tabindex="-1"></a>coffee_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(replicated_result_tbl, <span class="fu">extract_comparison_results</span>(coffee_logit_att),</span>
<span id="cb22-1211"><a href="#cb22-1211" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(coffee_boosted_att))</span>
<span id="cb22-1212"><a href="#cb22-1212" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coffee_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Rep. Result (Logistic with PSM)"</span>,<span class="st">"Logistic Regression and IPW"</span>, <span class="st">"Generalized Boosting Machine and IPW"</span>)</span>
<span id="cb22-1213"><a href="#cb22-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1214"><a href="#cb22-1214" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_comparisons_tab, <span class="at">digits =</span> <span class="dv">4</span>,<span class="at">booktabs =</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb22-1215"><a href="#cb22-1215" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb22-1216"><a href="#cb22-1216" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span>
<span id="cb22-1217"><a href="#cb22-1217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1218"><a href="#cb22-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1219"><a href="#cb22-1219" aria-hidden="true" tabindex="-1"></a>@tbl-comparison-coffee-results shows the estimates of the treatment effect across different methods. Recall that @Jena2012 estimate a an effect of $-0.15$ implying that daily income reduces by $0.15$ if a farmer becomes certified. This result is not statistically significant. </span>
<span id="cb22-1220"><a href="#cb22-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1221"><a href="#cb22-1221" aria-hidden="true" tabindex="-1"></a>The IPW estimate is $-1.58$ implying that certification leads to a $\$1.58$ decrease in daily income. This coefficient is much larger than than the original paper by a magnitude of $10$. Additionally, this estimate is statistically significant at the $1\%$ level. The GBM estimate is $-1.02$ which predicts a decrease in daily income by $\$1.02$ when a farmer becomes certified. This finding is statistically significant at the $5\%$ level. </span>
<span id="cb22-1222"><a href="#cb22-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1223"><a href="#cb22-1223" aria-hidden="true" tabindex="-1"></a>The reason for a large difference is threefold. First, better covariate balance by using a GBM and IPW and should result in a more robust estimate. Of course better covariate balance alone does not guarantee robust results but it is a step in the right direction. Theoretically, weighting on the inverse of the propensity scores from a GBM results in the best estimate so the paper may significantly underestimate the coefficient. Second, a different estimand will often lead to a different estimate of the treatment effect. It is not clear or directly estimable how much of the difference in estimate results from switching from the ATM to the ATT. Related to this, the data used to estimate the treatment effect is different as there are no dropped observations either of the IPW estimates. Third...</span>
<span id="cb22-1224"><a href="#cb22-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1225"><a href="#cb22-1225" aria-hidden="true" tabindex="-1"></a>The most interesting result is that the estimates become even more negative. One may expect that the result from a better balanced sample would become positive to align with theoretical motivations for cerification policies. A possible answer for why the estimand is negative is reverse causality which is suggested by @</span>
<span id="cb22-1226"><a href="#cb22-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1227"><a href="#cb22-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1228"><a href="#cb22-1228" aria-hidden="true" tabindex="-1"></a>@Jena2012 presented two explanations for why certification shows no positive impact. Firstly, the authors note that the prices offered by certified cooperatives are not significantly different from those provided by non-certified cooperatives. Secondly, a substantial portion of coffee—about 75%—is sold to private traders, who often pay higher prices to non-certified farmers. Additionally, from qualitative interviews with farmers, the authors note that policies and arrangements within different cooperatives exhibit heterogeneity so the impact of certification may relate more to the structure of the cooperatives. </span>
<span id="cb22-1229"><a href="#cb22-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1230"><a href="#cb22-1230" aria-hidden="true" tabindex="-1"></a>An additional answer is the impact of reverse causality. A general problem is causal inference is that the direction of causality is not always known. While it is most intuitive that coffee certification would impact income, it is also possible that a farmers daily income might determine their certification. Suppose that proponents of fairtrade and certification are correct that it will increase income and benefit livelihood. If farmers are are of this, then perhaps the lowest income farmers are most likely to attempt to become certified to increase their income. Additionally, income likely has a reverse causal relationship with many of the explanatory variables. For example, a higher income may lead to better access to credit and the accumulation of land. </span>
<span id="cb22-1231"><a href="#cb22-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1232"><a href="#cb22-1232" aria-hidden="true" tabindex="-1"></a>a final concluding remark. </span>
<span id="cb22-1233"><a href="#cb22-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1234"><a href="#cb22-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1235"><a href="#cb22-1235" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb22-1236"><a href="#cb22-1236" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Provided for PDF Output</span></span>
<span id="cb22-1237"><a href="#cb22-1237" aria-hidden="true" tabindex="-1"></a><span class="in">```{r get-labels, echo = FALSE}</span></span>
<span id="cb22-1238"><a href="#cb22-1238" aria-hidden="true" tabindex="-1"></a><span class="in">labs = knitr::all_labels()</span></span>
<span id="cb22-1239"><a href="#cb22-1239" aria-hidden="true" tabindex="-1"></a><span class="in">labs = setdiff(labs, c("get-labels", "nsw_logit_pmodel", "nsw_boosted_weight", "nsw-btab-logit", "compute-ate-nsw-boosted" ))</span></span>
<span id="cb22-1240"><a href="#cb22-1240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1241"><a href="#cb22-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1242"><a href="#cb22-1242" aria-hidden="true" tabindex="-1"></a><span class="in">```{r all-code, ref.label=labs, eval=FALSE}</span></span>
<span id="cb22-1243"><a href="#cb22-1243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-1244"><a href="#cb22-1244" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-1245"><a href="#cb22-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1246"><a href="#cb22-1246" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb22-1247"><a href="#cb22-1247" aria-hidden="true" tabindex="-1"></a><span class="in">save.image(file = "my_environment.RData")</span></span>
<span id="cb22-1248"><a href="#cb22-1248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>