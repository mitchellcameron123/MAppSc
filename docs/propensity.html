<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.52">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>propensity</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">




<!-- To-do:   -->
<!-- - Finish coffee data example -->
<!-- - rework the loss function theory stuff -->
<!-- - change code to quarto formatting -->
<!-- - perhaps remove the nsw example here -->
<!-- - change the tutorial code to be the replication study -->
<!-- - exposed vs treated (using treatment here). -->
<!-- make quarto and ggplot themes consistent. also changing colors of note callouts.  -->
<!-- make sure the link here says chapter not section -->
<!-- - better intro that explains a probability machine.  -->
<!-- - round off the structure and ensure header labels are consistent.  -->
<!-- - ml background to be transferred/written up to the background chapter. cite in this. -->
<!-- - state generally about how classification is binary and give examples of how ml is used for this. then transition it all a lot better.  -->
<!-- - more organised comparison of simulation results -->
<!-- - clarify the gini splitting vs accuracy loss function for rf/bag -->
<!-- add rf and bagging to reduce words. replace all.  -->
<!-- ensure that covariate balance measures are noted and that there is a clear flow down to the simulation settting where balance is discussed.  -->
<section id="sec-propensity" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Propensity Scores with Machine Learning</h1>
<section id="a-conventional-approach-propensity-scores-and-balance" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">1.1</span> A Conventional Approach: Propensity Scores and Balance</h2>
<p>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation and so the average treatment effect is a contrast of means from <strong>?@eq-ate-estimate</strong>. In observational data, the exposure to a given treatment is not random, implying there may be systematic differences between groups. As groups are not comparable, <strong>?@eq-ate-estimate</strong> will lead to a biased estimate of the treatment effect. For example, consider the causal question: <em>“How much does attending university increase lifetime earnings?”</em>. Individuals who complete a bachelor’s degree are not selected at random for university programs (treatment) and may have different observable characteristics than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents who have professional jobs. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the <em>cause</em> of higher incomes when the effect is confounded by the differences in covariates between groups. This discussion introduces the idea of <em>covariate balance</em> which is a key concept behind underlying propensity score methods.</p>
<div id="nte-balance-intution" class="callout callout-style-default callout-note callout-titled" title="What is Covariate Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;1: What is Covariate Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups and implies exchangeability between observations. Recall that defining unconfoundedness as <strong>?@eq-independence</strong> also implies exchangeability, implying that covariate balance should eliminate the confounding effect of variables associated with both the treatment and the outcome. If the distributions of the covariates are the same for each group, then those covariates are balanced, implying exchangeability with respect to those covariates.</p>
</div>
</div>
<p>From our bachelor’s degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. All pairs have the same academic ability, motivation, parent income, geographic residence etc. Comparing our matched pairs should result in a robust estimate of the effect of a bachelor’s degree on earnings, as the individuals are exchangeable by definition. However, there is a serious dimensionality problem and exact matches cannot be made as the number of matched covariates increases.</p>
<p><span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> offers a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditional on observed covariates and reduces the dimension of the number of covariates to a single dimension. The propensity score, denoted as <span class="math inline">\(e(X)\)</span>, it is expressed as:</p>
<p><span id="eq-pscore"><span class="math display">\[
e(X)=P(T=1|X)
\tag{1}\]</span></span></p>
<p>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in <strong>?@eq-conditional-independence</strong>. There are many sources that offer a comprehensive guide to propensity score methods.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div id="nte-balance-pscore" class="callout callout-style-default callout-note callout-titled" title="Balance and Propensity Scores">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2: Balance and Propensity Scores
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that an RCT will satisfy <strong>?@eq-independence</strong> as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy <strong>?@eq-conditional-independence</strong> as the potential outcomes are independent of the treatment status conditioned on some covariates. Conditioning on the propensity score aims to replicate an RCT in the observational data by balancing covariates between groups. We desire that, when units are balanced on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates. The variables used to predict the propensity model are said to be conditioned on.</p>
</div>
</div>
<!-- there is repetition here of eqs in the boxes and writing.  -->
<p>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched pairs with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</p>
<p><span class="citation" data-cites="King2019">King and Nielsen (<a href="#ref-King2019" role="doc-biblioref">2019</a>)</span> provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</p>
<section id="propensity-score-modelling-with-logistic-regression" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="propensity-score-modelling-with-logistic-regression"><span class="header-section-number">1.1.1</span> Propensity Score Modelling with Logistic Regression</h3>
<p>A conventional propensity score model most commonly uses logistic regression to predict a probability between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Model specification may include interaction terms and polynomial terms to best achieve covariate balance. There are a range of approaches for specifying a propensity score model, but the process is driven by heuristics <span class="citation" data-cites="Brookhart2006 Heinrich2010">(<a href="#ref-Brookhart2006" role="doc-biblioref">Brookhart et al. 2006</a>; <a href="#ref-Heinrich2010" role="doc-biblioref">Heinrich 2010</a>)</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are statistically significant. Many researchers do not discuss the specification of their propensity model in papers. <span class="citation" data-cites="Austin2008">Austin (<a href="#ref-Austin2008" role="doc-biblioref">2008</a>)</span> review 47 papers that use propensity scores and few assess balance, perform adequate model selection and diagnosis, or apply correct statistical tests.</p>
<p>It’s important to note that the true value of a propensity score is never observable. A propensity score that is close to the theoretical probability is well calibrated. Using poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. The calibration of propensity scores depends on correctly specifying the model used to estimate them. Covariates may be omitted by error, poorly measured, or be unobservable. If the true relationship is non-linear or involves complex interactions between covariates, logistic regression may not predict calibrated scores. Another important note is that the propensity model itself does not have an informative causal interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for a variable which is not informative of the desired estimand.</p>
<p>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</p>
</section>
</section>
<section id="probability-machines-probability-theory-and-machine-learning" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">1.2</span> Probability Machines: Probability Theory and Machine Learning</h2>
<p>Predicting probabilities is not a typical machine learning task. Supervised machine learning usually focuses on classifying observations into groups, or regression to predict continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability that an observation will belong to a certain class. In this context, these applications are sometimes called probability machines.</p>
<p>Probability machines are valuable in applications requiring calibrated probability predictions. Probability machines can predict loan defaults or other adverse events in finance. They estimate the likelihood of customer response to a campaign in marketing. In criminal justice, they help forecast recidivism or future arrests, informing parole decisions. Weather forecasting uses probability machines to predict events like the chance of rain. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</p>
<p>Probability machines offer many advantages over parametric methods like logistic regression:</p>
<ol type="1">
<li><p><strong>Improved Calibration</strong>: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</p></li>
<li><p><strong>Flexible Modelling</strong>: Unlike parametric methods like logistic regression, probability machines don’t rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</p></li>
<li><p><strong>Efficient Feature Selection</strong>: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</p></li>
<li><p><strong>Handling Missing Data</strong>: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</p></li>
<li><p><strong>Simplified Data Exploration</strong>: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</p></li>
</ol>
<p>In causal inference, probability machines can predict propensity scores to maximize covariate balance and better estimate treatment effects. The first use of machine learning in economics and social sciences was for predicting propensity scores, driven by strong theoretical and practical motivations. This discussion aims to clarify the use of probability machines in causal inference given the sometimes unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</p>
<p>Please note that this chapter assumes a reader is familiar with <em>CART (Classification and Regression Tree)</em>, <em>Boosting</em>, <em>Bagging (Bootstrap Aggregation)</em>, <em>Random Forests</em>, <em>LASSO (Least Absolute Shrinkage and Selection Operator</em>, and <em>Logistic Regression</em>. These methods are briefly discussed in <strong>?@sec-background-ml</strong>.</p>
<section id="choice-of-loss-function-and-probability-prediction" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="choice-of-loss-function-and-probability-prediction"><span class="header-section-number">1.2.1</span> Choice of Loss Function and Probability Prediction</h3>
<p>The loss function measures the difference between a model’s predictions and the actual target values, serving as an measure of the model’s performance. The model with the lowest error is found when the loss function is minimised. In standard least squares regression, the loss function is the residual sum of squares that can be stated as: <span class="math inline">\(\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span>. This loss function says that the model must reduce the squared differences between the observed and predicted values. Different loss functions influence the model’s behaviour and so the choice of loss function is important.</p>
<p>Classification models determine the category to which each observation belongs. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Another example is in email filtering, where classifiers are used to predict whether or not an email is spam. Given these binary classification objectives, many loss functions minimize classification errors and improve accuracy.</p>
<p>A probability machine might employ a classification approach suitable for binary outcomes. While a loss function like the Gini (introduced in <strong>?@sec-background-cart</strong>) index is effective for classification problems, its effectiveness in calculating class probabilities is uncertain. In other words, minimizing misclassification error may not lead to accurate probability predictions.</p>
<p>To classify an observation as either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, a model needs to determine if <span class="math inline">\(P(A)\)</span> is less than or greater than <span class="math inline">\(0.5\)</span>. Thus, it is trivial if the probability of that classification is <span class="math inline">\(0.51\)</span> or <span class="math inline">\(0.99\)</span> as this makes no difference to the classification. For a probability machine, the difference between <span class="math inline">\(\hat{P}(A) = 0.51\)</span> and <span class="math inline">\(\hat{P}(A) = 0.99\)</span> is extreme. Understanding that classification models are optimized for classification accuracy rather than probability prediction is important. This distinction affects the performance of ensemble methods like random forests or bagging ensembles which use classification trees.</p>
</section>
<section id="sec-bagg-rf-probmachines" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="sec-bagg-rf-probmachines"><span class="header-section-number">1.2.2</span> Bagging and Random Forest as Probability Machines</h3>
<!-- todo:  -->
<!-- - reference to appendix -->
<!-- - additional reserach for nsw inclding the cite ther.  -->
<!-- - section reference -->
<p>In a bagging or random forest ensemble, class probabilities are determined through a <em>vote count</em> method. Each tree in the ensemble makes a class prediction based on the majority class in a terminal node. For instance, if <span class="math inline">\(x_i\)</span> lies in a terminal node where <span class="math inline">\(80\%\)</span> of the observations are classified as <span class="math inline">\(A\)</span>, that <em>individual tree</em> will classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span>. The ensemble’s overall prediction for <span class="math inline">\(x_i\)</span> is derived from the proportion of trees that classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. Let <span class="math inline">\(T\)</span> be the total number of trees and <span class="math inline">\(b_t\)</span> be the <span class="math inline">\(t\)</span>-th tree in the ensemble. Let <span class="math inline">\(\mathbb{I}(b_t(x_i) = A)\)</span> be the indicator function that returns <span class="math inline">\(1\)</span> when <span class="math inline">\(b_t\)</span> predicts that observation <span class="math inline">\(x_i\)</span> belongs to class <span class="math inline">\(A\)</span>. The probability of class <span class="math inline">\(A\)</span> for observation <span class="math inline">\(x_i\)</span> is calculated as:</p>
<p><span id="eq-ensemble-vote-method"><span class="math display">\[
\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A).
\tag{2}\]</span></span></p>
<p>In discussing the theoretical properties of random forests and bagging ensembles for probability predictions, <span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="#ref-Olson2018" role="doc-biblioref">2018</a>)</span> notes a potential bias towards predictions of <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> when trees in an ensemble are highly correlated and a voting mechanism is used. When trees in an ensemble are highly correlated, a vote count method can bias predicted probabilities towards <span class="math inline">\(\hat{P}(x_i=A) \in \{0,1\}\)</span> because each individual tree gives an identical prediction for each <span class="math inline">\(x_i\)</span>. Across the whole ensemble, probability predictions will bias towards <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Although having an ensemble of identical trees is unrealistic, the notion illustrates that tree correlation can introduce a <em>divergence bias</em>. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</p>
<p>However, divergence bias is problematic in probability applications. If <span class="math inline">\(x_i\)</span> has a known membership of <span class="math inline">\(A\)</span>, and an unknown <span class="math inline">\(P_{\text{true}}(x_i=A) = 0.6\)</span>, the ensemble might classify <span class="math inline">\(x_i\)</span> correctly <span class="math inline">\(90\%\)</span> of the time leading to <span class="math inline">\(\hat{P}(x_i=A) = 0.9\)</span>. As a probability machine, the ensemble has overestimated the probability by <span class="math inline">\(0.3\)</span> even though <span class="math inline">\(90\%\)</span> accuracy is excellent. To predict <span class="math inline">\(P_{\text{true}}(x_i=A) = 0.6\)</span>, an ensemble would need to incorrectly classify <span class="math inline">\(x_i\)</span> in <span class="math inline">\(40\%\)</span> of its trees. However, random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</p>
<p>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected subset of the data. Random forests further reduce tree correlation by considering only a random number of variables at each split, referred to as <span class="math inline">\(mtry\)</span>. When <span class="math inline">\(mtry\)</span> is near to to number of predictors, the model considers more variables at each split, making the random forest closer to a bagging ensemble. A lower <span class="math inline">\(mtry\)</span> should reduce the correlation between trees and decrease divergence bias, but a lower <span class="math inline">\(mtry\)</span> also introduces other theoretical problems.</p>
<p>Consider the scenario where the binary outcome of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If <span class="math inline">\(mtry\)</span> is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. For example, each predictor has a chance of <span class="math inline">\(\frac{mtry}{\text{number of predictors}}\)</span> of selection at each split implying a lower <span class="math inline">\(mtry\)</span> decreases the chance of a split considering the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of <span class="math inline">\(x_i\)</span> in each tree. Additionally, consider there is a class imbalance and the majority of obvervations are classified as <span class="math inline">\(A\)</span> not <span class="math inline">\(B\)</span>. If sucessive noisy splits result in impure terminal nodes, then terminal nodes may be dominated by the majority class <span class="math inline">\(A\)</span>. Consequently, there is a <em>majority class</em> effect as each tree in the ensemble is more likely to misclassify an observation as an <span class="math inline">\(A\)</span> because the terminal nodes have a higher proportion of <span class="math inline">\(A\)</span> due to the higher proportion of <span class="math inline">\(A\)</span>’s in the data overall.</p>
<p>To exemplify this theoretical discussion, consider the National Supported Work (NSW) programme, which is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with <span class="math inline">\(445\)</span> total participants, <span class="math inline">\(185\)</span> in the program group, and <span class="math inline">\(260\)</span> in the control group, so the true probability of treatment for each individual can be calculated as <span class="math inline">\(185/445=0.42\)</span> or <span class="math inline">\(42\)</span>%. Further information about this data is found in <strong>?@sec-data-nsw-jobs</strong>.</p>
<p>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although <a href="#fig-rf-varimp" class="quarto-xref">Figure&nbsp;2</a> and <a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;1</a> do suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by <span class="citation" data-cites="Smith2005">Smith and Todd (<a href="#ref-Smith2005" role="doc-biblioref">2005</a>)</span> who suggests that self-selection bias is prevalent in the NSW data.</p>
<p><a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;1</a> shows both divergence bias and majority class effect using <code>randomForest</code> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when <span class="math inline">\(mtry\)</span> is equal to the number of predictors and so specifying <code>mtry = 7</code> in the <code>randomForest</code> function will fit a bagging ensemble. Logistic regression using the <code>gbm()</code> function provides a meaningful comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create the Plot</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nsw_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">as.factor</span>(treat) <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                          black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> marr)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>logit_preds <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="fu">binomial</span>())<span class="sc">$</span>fitted.values </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>rf_mtry1_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">randomForest</span>(nsw_formula, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">data =</span> nsw_data), </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                          <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(nsw_formula, <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> nsw_data)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>bagged_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagging_model, <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plot_pmachines <span class="ot">&lt;-</span> <span class="cf">function</span>(preds, title) {</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(nsw_data, <span class="fu">aes</span>(<span class="at">x =</span> preds, <span class="at">fill =</span> <span class="fu">factor</span>(treat))) <span class="sc">+</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Participants"</span>)) <span class="sc">+</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">subtitle =</span> title, <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>         <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    custom_ggplot_theme</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(logit_preds, <span class="st">"Logistic Regression"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span> </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(rf_mtry1_preds, <span class="st">"Random Forest (mtry = 1)"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(bagged_preds, <span class="st">"Bagging (Bootstrap Aggregation)"</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">/</span> p2 <span class="sc">/</span> p3 <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-theory-demo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-rf-theory-demo-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: This figure compares the kernel density estimates of propensity score of each observation in the National Supported Work programme. The random forest and bagging ensemle are fitted with the randomForest default value of 500 trees.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create the Plot</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"none"</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-varimp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-rf-varimp-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The figure compares the variable importance assigned to each variable from a baggin ensemble. The data originates from the National Supported Work programme. The difference in relative important of some variables indicates that randomisation may not have created exchangability between the groups.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The logistic regression model has identified a central tendency and most probabilities are between <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span> which roughly aligns with the true probability. For the random forest with <span class="math inline">\(mtry=1\)</span>, a significant number of the treatment and control observations are centred near <span class="math inline">\(0\)</span> with a wide range of other predictions. Such behaviour is consistent with a model overly predicting the majority class and having unstable predictions otherwise. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span>. Compared to the theoretically true probability, both random forest and bagging ensembles have performed poorly.</p>
<p>The tuning of <span class="math inline">\(mtry\)</span> faces double jeopardy and is another important area of discussion in probability machines. The selection of <span class="math inline">\(mtry\)</span> is typically completed in with a classification loss function such as accuracy or out-of-bag error. <span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="#ref-Olson2018" role="doc-biblioref">2018</a>)</span> compares tuning <span class="math inline">\(mtry\)</span> measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of <span class="math inline">\(mtry\)</span> for classification differs greatly from for probability prediction.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> In other words, if a grid search finds that <span class="math inline">\(mtry=3\)</span> is optimal for a classification task, this does not imply that <span class="math inline">\(mtry=3\)</span> is optimal for predicting probabilities.</p>
<p>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in <a href="#sec-mlps-sims" class="quarto-xref">Section&nbsp;1.2.5</a>. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</p>
<p>Anecdotally, divergence bias and majority class effects will most effect a probability machine when there is considerable overlap between groups. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common overlap may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low <span class="math inline">\(mtry\)</span>, the terminal nodes of each tree may be relatively impure leading to a majority class effect. Alternatively, if true probabilities exist near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and there is a clear separation of class, divergence effects may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low <span class="math inline">\(mtry\)</span> may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</p>
<!-- maybe need a little chat about cross entropy here and why its not as good as in the gbm case.  -->
</section>
<section id="gradient-boosting-machines-as-probability-machines" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="gradient-boosting-machines-as-probability-machines"><span class="header-section-number">1.2.3</span> Gradient Boosting Machines as Probability Machines</h3>
<!-- needs a little more comparison to rf and bagging. perhaps do some more reserach about why these are good. note and differentiate the different types of boosting. perhaps also clarify how the gradient descent works for my own learning (dont write it to be too technical).  -->
<p>Moving beyond classification trees in random forests or bagging ensembles, <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> introduced the <em>Gradient Boosting Machine</em> (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss until it reaches its minimum.</p>
<p>GBM’s can be be <em>generalised</em> to many different applications by providing different loss functions that can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple <em>boosted</em> regression trees and a logistic function to transform regression predictions into probabilities. This logistic function is the same as in logistic regression, and so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood function. The model is expected to be well-calibrated, as maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The GBM outputs probability predictions, avoiding the issues associated with vote count methods used by random forests and bagging ensembles.</p>
<p>Additionally, each split considers all variables and makes the most informative splits that descend the loss function most effectively. GBMs utilize many weak learners, where each learner is only slightly better than random guessing. These weak learners are often regression stumps, which are CART models with only a single split. However, additional splits enable the model to capture interactions between terms which may increase performance in complex or high-dimensional datasets.</p>
<p>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques as well as allowing a probability distribution based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. The implementation and workflow to fit a GBM for propensity scores, is discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref">Section&nbsp;1.3.1</a>.</p>
</section>
<section id="overfitting" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">1.2.4</span> Overfitting</h3>
<p>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting is used to prevent overfitting to ensure a model can generalize to unseen data.</p>
<p>However, the model is not required to generalise when predicting propensity scores, as a different propensity score model is fit for a other datasets. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</p>
<div id="nte-overfit-logistic" class="callout callout-style-default callout-note callout-titled" title="Overfitting in Logistic Regression">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3: Overfitting in Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is <span class="citation" data-cites="Schuster2016">Schuster, Lowe, and Platt (<a href="#ref-Schuster2016" role="doc-biblioref">2016</a>)</span>, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> receive artificially inflated weighting.</p>
</div>
</div>
<p><span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</p>
<p>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</p>
</section>
<section id="sec-mlps-sims" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="sec-mlps-sims"><span class="header-section-number">1.2.5</span> Comparison of Machine Learning Algorithms: Simulation Results</h3>
<!-- todo:  -->
<!-- fix early cite -->
<!-- clarify that sim studies are propensity score based. maybe look for general probability machine example sims.  -->
<p>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="citation" data-cites="McCaffrey2004 Setoguchi2008 Lee2010 Cannas2019 Tu2019 Goller2020 Ferri2020">(see <a href="#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="#ref-Setoguchi2008" role="doc-biblioref">Setoguchi et al. 2008</a>; <a href="#ref-Lee2010" role="doc-biblioref">Lee, Lessler, and Stuart 2010</a>; <a href="#ref-Cannas2019" role="doc-biblioref">Cannas and Arpino 2019</a>; <a href="#ref-Tu2019" role="doc-biblioref">Tu 2019</a>; <a href="#ref-Goller2020" role="doc-biblioref">Goller et al. 2020</a>; <a href="#ref-Ferri2020" role="doc-biblioref">Ferri-García and Del Mar Rueda 2020</a>)</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</p>
<p><span class="citation" data-cites="Tu2019">Tu (<a href="#ref-Tu2019" role="doc-biblioref">2019</a>)</span> compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</p>
<p>Despite poor theoretical properties as a probability machine, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> find that bagging results in the lowest standard error across many datasets.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</p>
<p>Additionally, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="#ref-Lee2010" role="doc-biblioref">2010</a>)</span> finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best <span class="math inline">\(95\%\)</span> coverage in all simulation scenarios, with <span class="math inline">\(98.6\%\)</span> coverage.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</p>
<p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="#ref-Goller2020" role="doc-biblioref">2020</a>)</span> adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</p>
<p>Based on a review of the literature, the findings can be distilled into five important points:</p>
<ol type="1">
<li><p>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</p></li>
<li><p>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="citation" data-cites="Rosenbaum1983">(<a href="#ref-Rosenbaum1983" role="doc-biblioref">Rosenbaum and Rubin 1983</a>)</span>.</p></li>
<li><p>Bagging ensembles perform poorly, a finding replicated across multiple studies.</p></li>
<li><p>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</p></li>
<li><p>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</p></li>
<li><p>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</p></li>
<li><p>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</p></li>
</ol>
</section>
</section>
<section id="implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="implimentation-and-hyperparameter-tuning-with-weightit-andgbm-in-r"><span class="header-section-number">1.3</span> Implimentation and Hyperparameter Tuning with <code>WeightIt</code> and<code>gbm</code> in R</h2>
<p>Based on <span class="citation" data-cites="Friedman2001">Friedman (<a href="#ref-Friedman2001" role="doc-biblioref">2001</a>)</span>, the <code>gbm</code> package implements a <em>Generalized Boosting Machine</em>. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. <code>gbm</code> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <code>bag.fraction</code> parameter.</p>
<p>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <code>gbm</code> package does not support this type of tuning, a wrapper package like <code>WeightIt</code> is necessary. <code>WeightIt</code> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <code>WeightIt</code> supports hyperparameter turning of <code>shrinkage</code>, <code>interaction.depth</code>, and <code>n.trees</code>. Once the best model is identified, propensity scores are predicted inside <code>WeightIt</code>. These can be used inside <code>WeightIt</code> to perform IPW or extracted for other implementations. <code>WeightIt</code> also supports an offset meaning that logistic regression predictions are supplied to the <code>GBM</code> package.</p>
<p>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="citation" data-cites="McCaffrey2004 Ridgeway2024">(see <a href="#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="#ref-Ridgeway2024" role="doc-biblioref">Ridgeway et al. 2024</a>)</span>. A very low learning rate, such as <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.0005\)</span>, allows a smooth descent of the loss function. The model should include a high number of trees, with <span class="math inline">\(10,000\)</span> or <span class="math inline">\(20,000\)</span> being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for <span class="math inline">\(3\)</span> splits. Based on anecdotal experience, <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span> splits per tree is optimal, consistent with recommendations by <span class="citation" data-cites="McCaffrey2004">McCaffrey, Ridgeway, and Morral (<a href="#ref-McCaffrey2004" role="doc-biblioref">2004</a>)</span>.</p>
<p>Another package, <code>twang</code>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <code>caret</code> must be used. Although <code>twang</code> has other useful functionalities which users may wish to implement.</p>
<section id="sec-gbm-tune-workflow" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="sec-gbm-tune-workflow"><span class="header-section-number">1.3.1</span> Hyperparameter Tuning and Workflow</h3>
<!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  -->
<p>The <code>WeigthtIt</code> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <code>cobalt</code>. The best information for this package can be found on this <a href="https://ngreifer.github.io/WeightIt/index.html">website</a> or accessed with <code>vignette("WeightIt")</code> inside R after installation using <code>install.packages("WeightIt")</code>.</p>
<p>A workflow for hyperparameter tuning in <code>WeightIt</code> may be completed as follows:</p>
<ol type="1">
<li><p>Specify the <code>criterion</code> option, which specifies the measure of the <em>”best model”</em>. The available options are the options that the <code>cobalt</code> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <code>sdm.mean</code> or the smallest maximum SDM across covariates called <code>sdm.max</code>.</p></li>
<li><p>Set the number of trees high. The package default is <code>n.trees = 10000</code> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <code>n.tree</code> if optimal.</p></li>
<li><p>Specify the grid search for the depth of the tree called <code>interaction.depth</code> and the learning rate called <code>shrinkage</code>. These values can be specified using <code>c()</code> such as <code>shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)</code> or as integers such as <code>interaction.depth = 1:5</code>. These particular values are heuristically selected <em>suggestions</em> of good starting values. Additionally, an offset can be considered by performing a grid search across <code>offset=c(TRUE,FALSE)</code>.</p></li>
<li><p>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <code>my_weightit_object$info$best.tune</code>.</p></li>
<li><p>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <code>interaction.depth = 1:5</code> and the best fit is <span class="math inline">\(5\)</span>, then a new search can consider <code>interaction.depth = 3:7</code> so that the local area around <span class="math inline">\(5\)</span> can be searched.</p></li>
<li><p>Experiment with <code>bag.fraction</code>, which means each tree will consider a drawn proportion of observations equal to <code>bag.fraction</code>. Iteratively changing <code>bag.fraction</code> and assessing balance at each value should be practical. Consider <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.67\)</span>, and <span class="math inline">\(1\)</span>.</p></li>
<li><p>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <code>love.plot()</code> such as <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;4</a>.</p></li>
<li><p>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</p></li>
<li><p>Estimation and reporting of treatment effect.</p></li>
</ol>
</section>
</section>
<section id="example-nsw-jobs-dataset-using-r" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="example-nsw-jobs-dataset-using-r"><span class="header-section-number">1.4</span> Example: NSW Jobs Dataset Using R</h2>
<p>For demonstration, propensity scores are estimated following the workflow discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref">Section&nbsp;1.3.1</a> to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in <strong>?@sec-data-nsw-jobs</strong>. Randomisation should eliminate structural differences between groups, but <span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <code>WeightIt</code> and a logistic regression model using <code>glm()</code>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</p>
<div id="nte-ipw" class="callout callout-style-default callout-note callout-titled" title="Inverse Probability of Treatment Weighting">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;4: Inverse Probability of Treatment Weighting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a <em>pseudo-population</em> where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</p>
</div>
</div>
<section id="step-1-6-model-fitting-and-tuning" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="step-1-6-model-fitting-and-tuning"><span class="header-section-number">1.4.1</span> Step 1-6: Model Fitting and Tuning</h3>
<p>The <code>glm()</code> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <code>binomial()</code>. Recall the <code>nsw_formula</code> is specified in <a href="#sec-bagg-rf-probmachines" class="quarto-xref">Section&nbsp;1.2.2</a></p>
<div class="sourceCode" id="annotated-cell-3"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-3-2" class="code-annotation-target"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>())</span>
<span id="annotated-cell-3-3"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-3-4" class="code-annotation-target"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="2" data-code-annotation="1">Fits a logistic regression model using the <code>glm()</code> function specified to be a logistic model with <code>family=binomial()</code> using the previously created <code>nsw_formula</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="4" data-code-annotation="2">Extracts the fitted values (propensity scores) from the model.</span>
</dd>
</dl>
<p>Using the propensity score column of <code>nsw_data</code>, the <code>WeightIt</code> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <code>nsw_logit_weight</code>.</p>
<div class="sourceCode" id="annotated-cell-4"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-4-1"><a href="#annotated-cell-4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-4-2" class="code-annotation-target"><a href="#annotated-cell-4-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-4-3" class="code-annotation-target"><a href="#annotated-cell-4-3" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-4-4" class="code-annotation-target"><a href="#annotated-cell-4-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-4" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="3" data-code-annotation="2">Provides <code>weightit()</code> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <code>weightit()</code> function with <code>method = "glm"</code>. The separate estimation of the propensity scores is for illustrative purposes.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="4" data-code-annotation="3">Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
</dd>
</dl>
<p>A GBM model for propensity scores can be specified using <code>method = "gbm"</code> inside the <code>weightit()</code> function. To ensure consistent results, running <code>set.seed(88)</code> will ensure each tree uses the same <code>seed</code> if <code>bag.fraction</code> less than <span class="math inline">\(1\)</span>. The model is fit using the heuristically suggested starting values. Note that this model may take approximately <span class="math inline">\(30\)</span> second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</p>
<div class="sourceCode" id="annotated-cell-5"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-5-1"><a href="#annotated-cell-5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-5-2" class="code-annotation-target"><a href="#annotated-cell-5-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-5-3" class="code-annotation-target"><a href="#annotated-cell-5-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="annotated-cell-5-4"><a href="#annotated-cell-5-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-5-5" class="code-annotation-target"><a href="#annotated-cell-5-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>),</span>
<span id="annotated-cell-5-6"><a href="#annotated-cell-5-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-5-7" class="code-annotation-target"><a href="#annotated-cell-5-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="annotated-cell-5-8"><a href="#annotated-cell-5-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-5-9" class="code-annotation-target"><a href="#annotated-cell-5-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>,</span>
<span id="annotated-cell-5-10"><a href="#annotated-cell-5-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="6" onclick="event.preventDefault();">6</a><span id="annotated-cell-5-11" class="code-annotation-target"><a href="#annotated-cell-5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-5" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="3,4" data-code-annotation="2">Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="5,6" data-code-annotation="3">Performs a grid search over these values of the learning rate and depth of tree.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="7,8" data-code-annotation="4">Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="9,10" data-code-annotation="5">Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be <span class="math inline">\(10000\)</span> which is the package default.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="11" data-code-annotation="6">Prints the tune details of the model with the best covariate balance.</span>
</dd>
</dl>
<!-- clarify the meaning of learning rate/shrinkage -->
<!-- change all the instructions to active speech not passive.  -->
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
6       0.3                 1    bernoulli      FALSE    0.02253485      2392</code></pre>
</div>
</div>
<!-- cite what the balance statistics should be in the intro when discussing propensity score and balance.  -->
<p>The best balance across all tuning combinations yields an average SMD of <span class="math inline">\(0.023\)</span> showing strong balance. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in <a href="#sec-nsw-balance" class="quarto-xref">Section&nbsp;1.4.2</a> accompanying a discussion of balance.</p>
<p>The best machine has a learning rate of <span class="math inline">\(0.3\)</span> and contains <span class="math inline">\(2392\)</span> decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to <span class="math inline">\(0.3\)</span>. A reduction in the depth of tree and number of trees will reduce computation time.</p>
<p>The new tune grid will consider <code>shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)</code> as this allows the GBM to consider values between <span class="math inline">\(0.2\)</span> and <span class="math inline">\(0.3\)</span> and above <span class="math inline">\(0.3\)</span> which were missing in the previous grid.</p>
<div class="cell">
<details class="code-fold">
<summary>PALCEHOLDER</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight2 <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method=</span><span class="st">"gbm"</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage=</span> <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.3</span>, <span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.45</span>, <span class="fl">0.5</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
11      0.45                 2    bernoulli      FALSE    0.01965492        95</code></pre>
</div>
</div>
<p>Comparing the two iterations, there is a reduction from <span class="math inline">\(0.022\)</span> to <span class="math inline">\(0.02\)</span>. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of <span class="math inline">\(0.45\)</span>, a tree depth of <span class="math inline">\(2\)</span>, and <span class="math inline">\(95\)</span> trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</p>
<p>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, <a href="#fig-balance-iterations" class="quarto-xref">Figure&nbsp;3</a> shows the optimal number of trees is highly variable. If the learning rate is set to <code>shrinkage = 0.05</code>, then the best balance is not achieved until near to <span class="math inline">\(20,000\)</span> trees.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Make the Plot</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span> </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-balance-iterations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-balance-iterations-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Average Standardised Mean Differernce (Covaraite Balance) and the number of interations. Please note the difference in horozontal scale between the two plots.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- showtext doesnt seem to be working here. Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font family 'Source Sans Pro' not found, will use 'sans' instead. perhaps need to define the font in side the plot_annotation()-->
<p>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</p>
</section>
<section id="sec-nsw-balance" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="sec-nsw-balance"><span class="header-section-number">1.4.2</span> Step 7 and 8: Assessing Balance</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="The Importance of Discussing Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Importance of Discussing Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study’s conclusions. <span class="citation" data-cites="King2019">King and Nielsen (<a href="#ref-King2019" role="doc-biblioref">2019</a>)</span> notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</p>
<p>A good resource of information for assessing balance is documentation from the <code>cobalt</code> package, which can be viewed by running <code>vignette(“cobalt”, package = “cobalt”)</code> in R.</p>
</div>
</div>
<p><code>cobalt</code> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <code>WeightIt</code> for IPW and <code>MatchIt</code> for propensity score matching. Balance tables are created using <code>bal.tab()</code>.</p>
<!-- make sure this comment about integration is not repeditive  -->
<div class="sourceCode" id="annotated-cell-8"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-8-1" class="code-annotation-target"><a href="#annotated-cell-8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-8-2" class="code-annotation-target"><a href="#annotated-cell-8-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight,</span>
<span id="annotated-cell-8-3"><a href="#annotated-cell-8-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-8-4" class="code-annotation-target"><a href="#annotated-cell-8-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="annotated-cell-8-5"><a href="#annotated-cell-8-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-8-6" class="code-annotation-target"><a href="#annotated-cell-8-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="annotated-cell-8-7"><a href="#annotated-cell-8-7" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-8-8" class="code-annotation-target"><a href="#annotated-cell-8-8" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="1" data-code-annotation="1">Loads the <code>cobalt</code> package. This assumes the package is already installed with <code>install.packages("cobalt")</code></span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="2,3" data-code-annotation="2">Uses the <code>bal.tab()</code> fucntion to create balance statistics for the previously created <code>nsw_logit_weight</code> model.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="4,5" data-code-annotation="3">Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="6" data-code-annotation="4">Sets a threshold of balance to be <span class="math inline">\(0.1\)</span> to determine if a covariate is balanced.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="8" data-code-annotation="5">Extracts the balance table of the <code>nsw_logit_btab</code> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
</dd>
</dl>
<p>Additionally, <code>bal.tab()</code> will create balance tables for the GBM method’s IPWs and the raw data. For presentation, <code>dplyr</code> combines each of the individual balance tables for presentation using <code>kable</code> and <code>kableExtra</code>.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to See Creation of Balance Tables</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_boosted_weight, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> nsw_data,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_formula, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> nsw_data, </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> nsw_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> nsw_raw_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<details class="code-fold">
<summary>Show the Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T,<span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width=</span><span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-combined-btab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Balance Table for NSW Data
</figcaption>
<div aria-describedby="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="tbl-combined-btab" class="cell anchored">
<div class="cell-output-display">
<table class="table caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variable</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Type</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">SMD</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Balanced</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.11</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.03</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.13</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.55</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.08</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.08</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.04</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.20</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.28</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.09</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Logistic Regression</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.98</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.27</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.80</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Boosting</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.91</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.02</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.14</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.02</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.01</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.00</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.05</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.05</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.01</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</div>
</figure>
</div>
<!-- double check that the variables are in the righ tpalces in the table  -->
<p><a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;1</a> shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</p>
<!-- perhaps find the threshold for variance ratios -->
</section>
<section id="step-9-results" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="step-9-results"><span class="header-section-number">1.4.3</span> Step 9: Results</h3>
<p>Finally, the treatment effect can be estimated using <code>lm_weightit()</code> from the <code>WeightIt</code> package and <code>avg_comparisons()</code> from the <code>marginaleffects</code> package. <code>lm_weightit()</code> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <code>avg_comparisons()</code> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</p>
<p>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="citation" data-cites="Naimi2017">(<a href="#ref-Naimi2017" role="doc-biblioref">Naimi, Cole, and Kennedy 2017</a>)</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <code>re78</code> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</p>
<div class="sourceCode" id="annotated-cell-10"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-10-1" class="code-annotation-target"><a href="#annotated-cell-10-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span></span>
<span id="annotated-cell-10-2"><a href="#annotated-cell-10-2" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-10-3" class="code-annotation-target"><a href="#annotated-cell-10-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights)</span>
<span id="annotated-cell-10-4"><a href="#annotated-cell-10-4" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-10-5" class="code-annotation-target"><a href="#annotated-cell-10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="annotated-cell-10-6"><a href="#annotated-cell-10-6" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-10" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="1,2" data-code-annotation="1">Uses <code>lm_weightit()</code> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <code>*</code> indicates multiplication in R.</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="3" data-code-annotation="2">Specifies the <code>weights</code> from the <code>nsw_boosted_weight</code> object created earlier by the <code>weightit()</code> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <code>nsw_boosted_weight$weights</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-10" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="5,6" data-code-annotation="3">Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
</dd>
</dl>
<p>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create the Table</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78<span class="sc">~</span>treat<span class="sc">*</span>(age <span class="sc">+</span> educ <span class="sc">+</span> </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                             re75 <span class="sc">+</span> black <span class="sc">+</span> hisp <span class="sc">+</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                             degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">weights =</span> nsw_logit_weight<span class="sc">$</span>weights)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>nsw_logit_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_logit_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>nsw_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">extract_comparison_results</span>(nsw_logit_result),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(nsw_boosted_result))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(nsw_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Logistic Regression"</span>, <span class="st">"GBM"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(nsw_comparisons_tab, <span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span> T, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="tbl-nsw-comparisons" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Comparison of ATE Estimates
</figcaption>
<div aria-describedby="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell">
<div class="cell-output-display">
<table class="table caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Estimate</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">SE</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">P.Value</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Lower.CI</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: center;">1610.79</td>
<td style="text-align: center;">668.49</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">300.58</td>
<td style="text-align: center;">2921.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">Generalized Boosting Machine</td>
<td style="text-align: center;">1609.95</td>
<td style="text-align: center;">669.42</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">297.91</td>
<td style="text-align: center;">2921.99</td>
</tr>
</tbody>
</table>


</div>
</div>
</div>
</figure>
</div>
<p><a href="#tbl-nsw-comparisons" class="quarto-xref">Table&nbsp;2</a> shows that both estimates of the treatment effect are nearly identical at <span class="math inline">\(\$1610\)</span> with logistic regression inferring a <span class="math inline">\(\$0.86\)</span> larger treatment effect. Additionally, these results are statistically significant at the <span class="math inline">\(5\%\)</span> level with nearly identical standard errors.</p>
</section>
</section>
<section id="replication-study-dont-read-this.-needs-an-honest-days-work" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="replication-study-dont-read-this.-needs-an-honest-days-work"><span class="header-section-number">1.5</span> Replication Study (Don’t read this. needs an honest days work)</h2>
<p><span class="citation" data-cites="coffecite">(<a href="#ref-coffecite" role="doc-biblioref"><strong>coffecite?</strong></a>)</span> aims to estimate the impact of the certification of coffee cooperatives on small-scale Ethiopian farmers’ livelihoods. Certification is seen as a potential tool for socioeconomic change and environmental sustainability and so it is important to understand the impact on small-scale farmers. Propensity scores are used to balance covariates between certified and non-certified farmers, isolating the certification’s effect on income.The paper did not assess the balance of propensity scores and it is difficult to replicate the results in the paper using best practice. However, this provides a good oppourtunity to assess covarate balance in the inital paper and the repeat the analysis using a machine learning propensity model.</p>
<section id="replication-of-original-results" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="replication-of-original-results"><span class="header-section-number">1.5.1</span> Replication of Original Results</h3>
<p>The paper is written inside Stata using psmatch2. I use R’s <code>MatchIt</code> package to replicate results. I have been able to reproduce the results of the paper using the below code including the estimate of the treatment effect. I have not been able to replicate the standard errors due to lack of support for <span class="citation" data-cites="abade">(<a href="#ref-abade" role="doc-biblioref"><strong>abade?</strong></a>)</span> SE’s. Although these are reproducable using Stata and the code provided in the original replication pacakge.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code Specifying the Function</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>coffee_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(certified <span class="sc">~</span> age_hh <span class="sc">+</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                  agesq <span class="sc">+</span> nonfarmincome_access <span class="sc">+</span> depratio <span class="sc">+</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                  logtotal_land <span class="sc">+</span> badweat <span class="sc">+</span> edu <span class="sc">+</span> gender <span class="sc">+</span> </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                  years_cofeproduction <span class="sc">+</span> access_credit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-fold="true">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MatchIt)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>coffee_rep_pmodel <span class="ot">&lt;-</span> <span class="fu">matchit</span>(coffee_formula, <span class="at">data=</span>coffee_data, <span class="at">distance=</span><span class="st">"glm"</span>, </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                              <span class="at">method=</span><span class="st">"nearest"</span>, <span class="at">replace =</span> T, <span class="at">estimand=</span><span class="st">"ATT"</span>, </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                              <span class="at">discard=</span><span class="st">"both"</span>) <span class="co">#Enforces Common Support trimming </span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>coffee_logit_md <span class="ot">&lt;-</span> <span class="fu">match.data</span>(coffee_rep_pmodel)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>coffee_rep_fit<span class="ot">&lt;-</span> <span class="fu">lm</span>(percapitaincome_day_maleeq <span class="sc">~</span> certified, <span class="at">data =</span> coffee_logit_md, <span class="at">weights=</span>weights)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="fu">avg_comparisons</span>(coffee_rep_fit, <span class="at">variables =</span> <span class="st">"certified"</span>,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">vcov =</span>F,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>                <span class="at">newdata =</span> <span class="fu">subset</span>(coffee_logit_md, certified <span class="sc">==</span> <span class="dv">1</span>),</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                <span class="at">wts =</span> <span class="st">"weights"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
      Term          Contrast Estimate
 certified mean(1) - mean(0)   -0.154

Columns: term, contrast, estimate, predicted_lo, predicted_hi, predicted 
Type:  response </code></pre>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code Creating the Balance Table</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_rep_pmodel, </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> coffee_data, </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>coffee_rep_btab <span class="ot">&lt;-</span> coffee_rep_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The paper does not discuss covariate balance and so I believe it is important to discuss this now. A good method of visualisation for comparison provided by <code>cobalt</code> is called a <code>love.plot()</code>.</p>
<div class="cell" data-fold="true">
<div class="cell-output-display">
<div id="fig-coffee-replication-lplot" class="quarto-float quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coffee-replication-lplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="propensity_files/figure-html/fig-coffee-replication-lplot-1.png" id="fig-coffee-replication-lplot" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-coffee-replication-lplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
</figcaption>
</figure>
</div>
</div>
</div>
<p>Viewing <a href="#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;4</a>, we can see that balance in the dataset is very poor for making inferences. Not a single variable is balanced using the <span class="math inline">\(10\%\)</span> cutoff and key variables that are likely strong predictors of both treatment and income are not balanced either. Balance in the study is not assessed for any variable which is a “red flag” for propensity score inference. For key variables such as Age, Gender, or Education, this balance is especially important. On a theoretical level, we expect that people who are more educated are more likely to become certified as they are better able to engage with the application process and also are expected to earn more as increased education should lead to greater productivity. There likely exists gender discrimination given the time period and geographic area which suggests woman are less likely to be certified than men while also earning less due to a wide gender pay gap. These variables are strong confounders in theory and so emphasising balance in these variables is critical to making a robust causal inference.</p>
<p>It is also worth noting that this result has partially achieved balance as 34 observations of which 33 are treated and 1 are control are dropped as they do not meet a common support requirement. This increases balance as the trimmed observations are extreme data points. When observations are discarded, the ATT, ATC, or ATE cannot be claimed. Instead, this is refereed to as the average treatment effect on the matched or ATM.</p>
<p>Overall, this model fit using logistic regression and propensity score matching has resulted in a poor model due to covariate imbalance and unidentifiable estimands. It is likely that improvement can be made using</p>
</section>
<section id="further-modelling" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="further-modelling"><span class="header-section-number">1.5.2</span> Further Modelling</h3>
<p>In the following model fitting process, I aim to obtain better results while preserving the estimand. To assess how much imbalance is caused by propensity score matching compared to weighting, I fit a logistic regression model and use IPTW instead of PSM. Additionally, I fit a generalised boosting model also using inverse probability of treatment weighting. My implementation will be using the <code>WeightIt</code> package as above. There are two interesting comparisons to be mad in this process. Firstly, what is the difference in balance between PSM and IPTW. Second, the relative performance of logistic regression and generalised boosting machines.</p>
<p>A logistic regression fit is simple to estimate.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>coffee_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(coffee_formula, <span class="at">data=</span>coffee_data, <span class="at">method=</span><span class="st">"glm"</span>,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">estimand=</span><span class="st">"ATT"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>coffee_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(coffee_logit_weight, </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                             <span class="at">formula =</span> coffee_formula,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                             <span class="at">data =</span> coffee_data, </span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                             <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                             <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                             <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>                             <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>coffee_logit_btab <span class="ot">&lt;-</span> coffee_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead, they are high only with a few variables which are also the most important to balance. Thus when tuning my boosting model, I have decided use the <code>criterion = "smd.max"</code> as obtaining better balance in these variables is more important than the average level of balance in my opinion.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Discussion of Tuning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion of Tuning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initially, a tuning grid considering shrinkage values of <span class="math inline">\(0.001,0.005,.01,0.05,0.1,\text{ and }0.2\)</span> were considered using <span class="math inline">\(10000\)</span> trees with a depth between <span class="math inline">\(1\)</span> and <span class="math inline">\(5\)</span>. The best tuning performance was found with shrinkage of <span class="math inline">\(0.2\)</span> and <span class="math inline">\(9\)</span> trees which were three splits <span class="math inline">\(3\)</span> deep. As such, the tuning grid was redefined in a second iteration to use <span class="math inline">\(0.1, 0.15, 0.2, 0.25, 0.3,0.35,\text{ and } 0.4\)</span> with only <span class="math inline">\(1000\)</span> trees with between <span class="math inline">\(2\)</span> and <span class="math inline">\(5\)</span> depth. The second fit, suggested a learning rate of <span class="math inline">\(0.35\)</span> so the local area of <span class="math inline">\(0.3, 0.325, 0.350, 0.375, \text{ and }0.4\)</span> is searched in the final fit.</p>
</div>
</div>
<div id="tbl-raw-btab" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl quarto-uncaptioned" id="tbl-raw-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3
</figcaption>
<div aria-describedby="tbl-raw-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>

</div>
</div>
</figure>
</div>
</section>
<section id="comparison-of-methodologies" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="comparison-of-methodologies"><span class="header-section-number">1.5.3</span> Comparison of Methodologies</h3>
<div class="cell">
<details class="code-fold">
<summary>Show the Code Prepairing the Balance Table for Presentation</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"data.table"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Household Age"</span>, <span class="st">"Squared Household Age"</span>, <span class="st">"Non-farm Income Access"</span>, </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Log Total Land"</span>, <span class="st">"Dependency Ratio"</span>, <span class="st">"Bad Weather"</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Education Level"</span>, <span class="st">"Gender"</span>, <span class="st">"Years of Coffee Production"</span>, </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Access to Credit"</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>colnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Variable"</span>,<span class="st">"Type"</span>, <span class="st">"Std. Mean Diff"</span>, <span class="st">"Balance Threshold"</span>, <span class="st">"Variance Ratio"</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> <span class="fu">rbindlist</span>(<span class="fu">list</span>(coffee_raw_btab,coffee_rep_btab,</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>                        coffee_logit_btab,coffee_boosted_btab),<span class="at">use.names=</span><span class="cn">FALSE</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">4</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab <span class="ot">&lt;-</span> coffee_combined_btab[,<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)]</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>coffee_combined_btab[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>          coffee_combined_btab[,<span class="dv">4</span>] <span class="sc">&gt;=</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to the Creation of Table 3.1.</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(coffee_combined_btab, <span class="at">digits=</span><span class="dv">3</span>, <span class="at">booktabs=</span><span class="cn">TRUE</span>, <span class="at">align=</span><span class="st">"c"</span>, </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size=</span><span class="dv">10</span>, <span class="at">col.names=</span>colnames) <span class="sc">%&gt;%</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold=</span><span class="cn">FALSE</span>, <span class="at">width=</span><span class="st">"1cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Raw Data"</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Replication Using PSM with Common Support Trimming"</span>, <span class="dv">11</span>, <span class="dv">20</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Logistic Regression and IPTW"</span>, <span class="dv">21</span>, <span class="dv">30</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="st">"Boosted Machine with IPTW"</span>, <span class="dv">31</span>, <span class="dv">40</span>, <span class="at">label_row_css =</span> <span class="st">"text-align: center;"</span>)<span class="sc">%&gt;%</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">footnote</span>(<span class="at">number=</span><span class="fu">c</span>(<span class="st">"SDM: Standardised Mean Difference"</span>,<span class="st">"Balanced: </span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="st">                    A treshold of |0.1| is used to determine variable balance."</span>,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"IPTW: Inverse probability of treatment weighting"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-coffee-comparison" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Comparison of Balance for Coffee Data Using Different Propensity Models
</figcaption>
<div aria-describedby="tbl-coffee-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variable</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Type</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Std. Mean Diff</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Balance Threshold</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.563</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.865</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.491</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.007</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.393</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.405</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.551</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.049</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.237</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.250</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.002</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.727</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.275</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.456</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.362</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.597</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Replication Using PSM with Common Support Trimming</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.272</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.073</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.255</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.143</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.301</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.260</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.297</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.400</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.979</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.202</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.244</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.034</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.132</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.340</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.911</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.195</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Logistic Regression and IPTW</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.245</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.927</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.228</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.072</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.170</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.092</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.856</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.114</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.388</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.194</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.047</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.922</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.046</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.061</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.112</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.029</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even" data-grouplength="10">
<td colspan="5" style="text-align: center;"><strong>Boosted Machine with IPTW</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.109</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">0.946</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Squared Household Age</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.103</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.073</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Non-farm Income Access</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.012</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Log Total Land</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">-0.059</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.660</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Dependency Ratio</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.034</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">0.971</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Bad Weather</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.110</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education Level</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.107</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">1.009</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Gender</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">-0.020</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Years of Coffee Production</td>
<td style="text-align: center; width: 1cm;">Contin.</td>
<td style="text-align: center; width: 1cm;">0.014</td>
<td style="text-align: center; width: 1cm;">Yes</td>
<td style="text-align: center; width: 1cm;">1.129</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Access to Credit</td>
<td style="text-align: center; width: 1cm;">Binary</td>
<td style="text-align: center; width: 1cm;">0.187</td>
<td style="text-align: center; width: 1cm;">No</td>
<td style="text-align: center; width: 1cm;">NA</td>
</tr>
</tbody><tfoot>
<tr class="odd">
<td style="text-align: center; padding: 0;"><sup>1</sup> SDM: Standardised Mean Difference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center; padding: 0;"><sup>2</sup> Balanced:<br>
A treshold of |0.1| is used to determine variable balance.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center; padding: 0;"><sup>3</sup> IPTW: Inverse probability of treatment weighting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tfoot>

</table>


</div>
</div>
</figure>
</div>
</div>
<div id="tbl-coffee-love-all" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coffee-love-all-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Comparison of Balance for Coffee Data Using Different Methods
</figcaption>
<div aria-describedby="tbl-coffee-love-all-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Show the Code to the Creation of Love Plot</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">love.plot</span>(coffee_formula,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> coffee_data, </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">weights =</span> <span class="fu">list</span>(<span class="at">Replication =</span> coffee_rep_pmodel,</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Logit =</span> coffee_logit_weight,</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Boosting=</span> coffee_boosted_weight),</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">var.order =</span> <span class="st">"unadjusted"</span>, <span class="at">binary =</span> <span class="st">"std"</span>,<span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">abs =</span> <span class="cn">TRUE</span>, <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"#e3e3e3"</span>, <span class="st">"#2780e3"</span>, <span class="st">"darkblue"</span>,<span class="st">"#333333"</span>), </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">shapes =</span> <span class="fu">c</span>(<span class="st">"circle"</span>, <span class="st">"square"</span>, <span class="st">"triangle"</span>, <span class="st">"diamond"</span>),</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">line =</span> <span class="cn">TRUE</span>,<span class="at">thresholds=</span><span class="fl">0.1</span>,<span class="at">s.d.denom=</span><span class="st">"treated"</span>,<span class="at">use.grid=</span>F)<span class="sc">+</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Balance Using Different Balance Methods"</span>,</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Absolute Standardised Mean Differences"</span>,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill=</span><span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme <span class="sc">+</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> scales<span class="sc">::</span><span class="fu">pretty_breaks</span>(<span class="at">n =</span> <span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<p>Viewing results of our balance shows three notable findings: 1. Propensity score matching has performed very poorly relative to weighting while still being able to retain all observations. Of course there are differences between datasets but this result is common. 2. For the logistic regression model, the balance statistics are marginally balanced. Using a <span class="math inline">\(10\%\)</span> threshold, half of the variables are balanced. Using a relaxed <span class="math inline">\(20\%\)</span> threshold, only Age and Age Squared are unbalanced but balance with threshold should be interpreted with caution. The average balance across all variables is 0.0768603 which is satisfactory. 3. GBM has been highly effective and resulted in the best balance on almost all variables. At a <span class="math inline">\(10\%\)</span> rule, half the variables are balanced however many of the unbalanced variables are trivially close to the threshold and could be considered balanced in effect. With minuscule variations in the tuning parameters or different seeds, these boundary variables may sink below the threshold but this is due to random noise not an increase in balance. All variables except Access to Credit are below <span class="math inline">\(0.11\)</span> which has an effect size of <span class="math inline">\(0.18\)</span> which would be balanced using the relaxed <span class="math inline">\(20\%\)</span> threshold. The average standardised mean is 0.0498114 which is quite impressive compared to the methodology used in the paper.</p>
<p>Now that variables have been balanced, it is quite interesting to estimate the treatment effect. In this case, all observations have been obtained and so the estimand is the ATT.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>coffee_att_formula <span class="ot">&lt;-</span> <span class="fu">update.formula</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"~"</span>, <span class="fu">paste</span>(<span class="fu">attr</span>(<span class="fu">terms</span>(coffee_formula), <span class="st">"term.labels"</span>), <span class="at">collapse =</span> <span class="st">" + "</span>))), percapitaincome_day_maleeq <span class="sc">~</span> certified <span class="sc">*</span> .)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>coffee_logit_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_logit_weight)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>coffee_boosted_fit <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(coffee_att_formula,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">data =</span> coffee_data, <span class="at">weightit =</span> coffee_boosted_weight)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>coffee_logit_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_logit_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>coffee_boosted_att <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(coffee_boosted_fit, <span class="at">variables =</span> <span class="st">"certified"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Austin2008" class="csl-entry" role="listitem">
Austin, Peter C. 2008. <span>“<span class="nocase">A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003</span>.”</span> <em>Statistics in Medicine</em> 27 (April): 2037–49. <a href="https://doi.org/10.1002/sim.3150">https://doi.org/10.1002/sim.3150</a>.
</div>
<div id="ref-Brookhart2006" class="csl-entry" role="listitem">
Brookhart, M. Alan, Sebastian Schneeweiss, Kenneth J. Rothman, Robert J. Glynn, Jerry Avorn, and Til Stürmer. 2006. <span>“<span class="nocase">Variable selection for propensity score models</span>.”</span> <em>American Journal of Epidemiology</em> 163 (12): 1149–56. <a href="https://doi.org/10.1093/aje/kwj149">https://doi.org/10.1093/aje/kwj149</a>.
</div>
<div id="ref-Cannas2019" class="csl-entry" role="listitem">
Cannas, Massimo, and Bruno Arpino. 2019. <span>“<span class="nocase">A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting</span>.”</span> <em>Biometrical Journal</em> 61 (4): 1049–72. <a href="https://doi.org/10.1002/bimj.201800132">https://doi.org/10.1002/bimj.201800132</a>.
</div>
<div id="ref-C5Mixtape2021" class="csl-entry" role="listitem">
Cunningham, Scott. 2021. <span>“<span class="nocase">Matching and Subclassification</span>.”</span> In <em>Causal Inference: The Mixtape</em>, 175–240. Yale University Press. <a href="https://doi.org/10.2307/j.ctv1c29t27.8">https://doi.org/10.2307/j.ctv1c29t27.8</a>.
</div>
<div id="ref-Ferri2020" class="csl-entry" role="listitem">
Ferri-García, Ramón, and María Del Mar Rueda. 2020. <span>“<span class="nocase">Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys</span>.”</span> <em>PLoS ONE</em> 15 (4): 1–19. <a href="https://doi.org/10.1371/journal.pone.0231500">https://doi.org/10.1371/journal.pone.0231500</a>.
</div>
<div id="ref-Friedman2001" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“<span>Greedy Function Approximation: A Gradient Boosting Machine</span>.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232. <a href="https://www.jstor.org/stable/2699986">https://www.jstor.org/stable/2699986</a>.
</div>
<div id="ref-Goller2020" class="csl-entry" role="listitem">
Goller, Daniel, Michael Lechner, Andreas Moczall, and Joachim Wolff. 2020. <span>“<span class="nocase">Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed</span>.”</span> <em>Labour Economics</em> 65 (March). <a href="https://doi.org/10.1016/j.labeco.2020.101855">https://doi.org/10.1016/j.labeco.2020.101855</a>.
</div>
<div id="ref-Heinrich2010" class="csl-entry" role="listitem">
Heinrich, Carolyn. 2010. <span>“<span class="nocase">A Primer for Applying Propensity-Score Matching</span>.”</span> <em>Development</em>, no. August: 59. <a href="http://www.iadb.org/document.cfm?id=35320229">http://www.iadb.org/document.cfm?id=35320229</a>.
</div>
<div id="ref-King2019" class="csl-entry" role="listitem">
King, Gary, and Richard Nielsen. 2019. <span>“<span class="nocase">Why Propensity Scores Should Not Be Used for Matching</span>.”</span> <em>Political Analysis</em> 27 (4): 435–54. <a href="https://doi.org/10.1017/pan.2019.11">https://doi.org/10.1017/pan.2019.11</a>.
</div>
<div id="ref-Lee2010" class="csl-entry" role="listitem">
Lee, Brian K., Justin Lessler, and Elizabeth A. Stuart. 2010. <span>“<span class="nocase">Improving propensity score weighting using machine learning</span>.”</span> <em>Statistics in Medicine</em> 29: 337–46. <a href="https://doi.org/10.1002/sim.3782">https://doi.org/10.1002/sim.3782</a>.
</div>
<div id="ref-McCaffrey2004" class="csl-entry" role="listitem">
McCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. <span>“<span class="nocase">Propensity score estimation with boosted regression for evaluating causal effects in observational studies</span>.”</span> <em>Psychological Methods</em> 9 (4): 403–25. <a href="https://doi.org/10.1037/1082-989X.9.4.403">https://doi.org/10.1037/1082-989X.9.4.403</a>.
</div>
<div id="ref-Naimi2017" class="csl-entry" role="listitem">
Naimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. 2017. <span>“<span class="nocase">An introduction to g methods</span>.”</span> <em>International Journal of Epidemiology</em> 46 (2): 756–62. <a href="https://doi.org/10.1093/ije/dyw323">https://doi.org/10.1093/ije/dyw323</a>.
</div>
<div id="ref-Olson2018" class="csl-entry" role="listitem">
Olson, Matthew A., and Abraham J. Wyner. 2018. <span>“<span class="nocase">Making Sense of Random Forest Probabilities: a Kernel Perspective</span>,”</span> 1–35. <a href="http://arxiv.org/abs/1812.05792">http://arxiv.org/abs/1812.05792</a>.
</div>
<div id="ref-Ridgeway2024" class="csl-entry" role="listitem">
Ridgeway, Greg, Dan Mccaffrey, Andrew Morral, Matthew Cefalu, Lane Burgette, and Beth Ann Griffin. 2024. <span>“<span class="nocase">Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package</span>.”</span> <a href="https://doi.org/10.7249/tl136.1">https://doi.org/10.7249/tl136.1</a>.
</div>
<div id="ref-Rosenbaum1983" class="csl-entry" role="listitem">
Rosenbaum, Paul R., and Donald B. Rubin. 1983. <span>“<span class="nocase">The central role of the propensity score in observational studies for causal effects</span>.”</span> <em>Biometrika</em> 70 (1): 41–55. <a href="https://doi.org/10.1017/CBO9780511810725.016">https://doi.org/10.1017/CBO9780511810725.016</a>.
</div>
<div id="ref-Schuster2016" class="csl-entry" role="listitem">
Schuster, Tibor, Wilfrid Kouokam Lowe, and Robert W. Platt. 2016. <span>“<span class="nocase">Propensity score model overfitting led to inflated variance of estimated odds ratios</span>.”</span> <em>Journal of Clinical Epidemiology</em> 80: 97–106. <a href="https://doi.org/10.1016/j.jclinepi.2016.05.017">https://doi.org/10.1016/j.jclinepi.2016.05.017</a>.
</div>
<div id="ref-Setoguchi2008" class="csl-entry" role="listitem">
Setoguchi, Soko, Sebastian Schneeweiss, Alan M. Brookhart, Robert J. Glynn, and Francis E. Cook. 2008. <span>“<span class="nocase">Evaluating uses of data mining techniques in propensity score estimation: a simulation study</span>.”</span> <em>Pharmacoepidemiology and Drug Safety</em> 17 (March): 546–55. <a href="https://doi.org/10.1002/pds">https://doi.org/10.1002/pds</a>.
</div>
<div id="ref-Smith2005" class="csl-entry" role="listitem">
Smith, Jeffrey A., and Petra E. Todd. 2005. <em><span class="nocase">Does matching overcome LaLonde’s critique of nonexperimental estimators?</span></em> Vol. 125. 1-2 SPEC. ISS. <a href="https://doi.org/10.1016/j.jeconom.2004.04.011">https://doi.org/10.1016/j.jeconom.2004.04.011</a>.
</div>
<div id="ref-Tu2019" class="csl-entry" role="listitem">
Tu, Chunhao. 2019. <span>“<span class="nocase">Comparison of various machine learning algorithms for estimating generalized propensity score</span>.”</span> <em>Journal of Statistical Computation and Simulation</em> 89 (4): 708–19. <a href="https://doi.org/10.1080/00949655.2019.1571059">https://doi.org/10.1080/00949655.2019.1571059</a>.
</div>
</div>
</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>An outstanding text on this is <span class="citation" data-cites="C5Mixtape2021">(<a href="#ref-C5Mixtape2021" role="doc-biblioref">Cunningham 2021, chap. 4</a>)</span> which provides applications and code examples in R, Python, and Stata.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Note that tuning <span class="math inline">\(mtry\)</span> for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>In this context, the coverage is the proportion of times that the true treatment effect is within the <span class="math inline">\(95\%\)</span> confidence interval across the number of simulations. This author implements <span class="math inline">\(1000\)</span> simulations of each scenario.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <code>twang</code> and <code>gbm</code> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benifits discussed in <span class="citation" data-cites="cross-reference">(<a href="#ref-cross-reference" role="doc-biblioref"><strong>cross-reference?</strong></a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="#ref-Goller2020" role="doc-biblioref">2020</a>)</span> calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the <em>true</em> treatment effect. Thus, the covariate balance table offers a clearer view of each method’s performance.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>